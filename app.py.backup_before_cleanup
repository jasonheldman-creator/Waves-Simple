import streamlit as st
import logging
import subprocess
import os
import traceback
import time
import itertools
import html
from datetime import datetime, timedelta

import pandas as pd
    
"""
Institutional Console v2 - Executive Layer v2
Full implementation with advanced analytics and visualization

Modular structure:
1. Configuration and styling
2. Utility functions  
3. Safe data-loading helpers
4. Data processing and calculation functions
5. Visualization components
6. Reusable UI components
7. Render functions for tabs and analytics
8. Main entry point

FULL MULTI-TAB CONSOLE UI - Post PR #336
Complete implementation with 18 tab render functions (16-17 visible tabs depending on configuration).
Includes all analytics, monitoring, and governance features.
"""

# ================================
import logging
import subprocess
import os
import traceback
import time
import itertools
import html
from datetime import datetime, timedelta, timezone

import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

logger = logging.getLogger("waves_app")
if not logger.handlers:
    logging.basicConfig(level=logging.INFO)



# Import alpha attribution module
try:
    from alpha_attribution import (
        compute_alpha_attribution_series,
        format_attribution_summary_table,
        format_daily_attribution_sample
    )
    ALPHA_ATTRIBUTION_AVAILABLE = True
except ImportError:
    ALPHA_ATTRIBUTION_AVAILABLE = False

# Import VIX overlay diagnostics module
try:
    from vix_overlay_diagnostics import get_wave_diagnostics
    VIX_DIAGNOSTICS_AVAILABLE = True
except ImportError:
    VIX_DIAGNOSTICS_AVAILABLE = False

# Import VIX overlay configuration
try:
    from config.vix_overlay_config import (
        get_vix_overlay_config,
        is_vix_overlay_live,
        get_vix_overlay_status
    )
    from waves_engine import (
        get_vix_overlay_status_for_wave,
        is_vix_overlay_active_for_wave
    )
    VIX_CONFIG_AVAILABLE = True
except ImportError:
    VIX_CONFIG_AVAILABLE = False

# Import waves engine for wave definitions (single source of truth)
try:
    from waves_engine import (
        get_all_waves as engine_get_all_waves, 
        WAVE_WEIGHTS,
        get_wave_id_from_display_name,
        get_display_name_from_wave_id
    )
    WAVES_ENGINE_AVAILABLE = True
except ImportError:
    WAVES_ENGINE_AVAILABLE = False
    engine_get_all_waves = None
    WAVE_WEIGHTS = {}
    get_wave_id_from_display_name = None
    get_display_name_from_wave_id = None

# Import wave registry manager
try:
    from wave_registry_manager import get_active_wave_registry
    WAVE_REGISTRY_MANAGER_AVAILABLE = True
except ImportError:
    WAVE_REGISTRY_MANAGER_AVAILABLE = False
    get_active_wave_registry = None

# V3 ADD-ON: Bottom Ticker (Institutional Rail) - Import V3 ticker module
try:
    from helpers.ticker_rail import render_bottom_ticker_v3
    TICKER_V3_AVAILABLE = True
except ImportError:
    TICKER_V3_AVAILABLE = False

# Import auto-refresh configuration
try:
    from auto_refresh_config import (
        DEFAULT_AUTO_REFRESH_ENABLED,
        DEFAULT_REFRESH_INTERVAL_MS,
        REFRESH_INTERVAL_OPTIONS,
        get_default_settings,
        validate_refresh_interval,
        get_interval_display_name,
        should_refresh_component,
        AUTO_PAUSE_ON_ERROR,
        MAX_CONSECUTIVE_ERRORS,
        STATUS_FORMAT
    )
    AUTO_REFRESH_CONFIG_AVAILABLE = True
except ImportError:
    AUTO_REFRESH_CONFIG_AVAILABLE = False
    # Fallback defaults if config module is unavailable
    # Auto-refresh OFF by default to prevent infinite reruns
    DEFAULT_AUTO_REFRESH_ENABLED = False
    DEFAULT_REFRESH_INTERVAL_MS = 60000
    REFRESH_INTERVAL_OPTIONS = {"1 minute": 60000, "2 minutes": 120000}
    AUTO_PAUSE_ON_ERROR = True
    MAX_CONSECUTIVE_ERRORS = 3
    STATUS_FORMAT = {"enabled": "üü¢ ON", "disabled": "üî¥ OFF", "paused": "üü° PAUSED", "error": "‚ö†Ô∏è ERROR"}

# Import wave registry validator
try:
    from helpers.wave_registry_validator import validate_wave_registry, load_wave_registry, get_enabled_waves
    WAVE_REGISTRY_VALIDATOR_AVAILABLE = True
except ImportError:
    WAVE_REGISTRY_VALIDATOR_AVAILABLE = False

# Import executive summary generator
try:
    from helpers.executive_summary import generate_executive_summary
    EXECUTIVE_SUMMARY_AVAILABLE = True
except ImportError:
    EXECUTIVE_SUMMARY_AVAILABLE = False

# Import diagnostics artifact loader
try:
    from helpers.diagnostics_artifact import load_diagnostics_artifact
    DIAGNOSTICS_ARTIFACT_AVAILABLE = True
except ImportError:
    DIAGNOSTICS_ARTIFACT_AVAILABLE = False

# Import operator toolbox helper functions
try:
    from helpers.operator_toolbox import (
        get_data_health_metadata,
        rebuild_price_cache as rebuild_price_cache_toolbox,
        rebuild_wave_history,
        run_self_test,
        force_ledger_recompute
    )
    OPERATOR_TOOLBOX_AVAILABLE = True
except ImportError:
    OPERATOR_TOOLBOX_AVAILABLE = False

# Import price_book constants for Mission Control
# UI uses price_book as source of truth to prevent divergence
try:
    from helpers.price_book import (
        PRICE_CACHE_OK_DAYS, 
        PRICE_CACHE_DEGRADED_DAYS,
        CANONICAL_CACHE_PATH,
        compute_system_health,
        get_price_book
    )
    PRICE_BOOK_CONSTANTS_AVAILABLE = True
    # Legacy aliases for backward compatibility - mapping to canonical names:
    # - DEGRADED_DAYS_THRESHOLD = threshold for transitioning FROM OK TO DEGRADED (14 days)
    # - STALE_DAYS_THRESHOLD = threshold for transitioning FROM DEGRADED TO STALE (30 days)
    # These maintain backward compatibility with code that uses the old threshold names.
    DEGRADED_DAYS_THRESHOLD = PRICE_CACHE_OK_DAYS
    STALE_DAYS_THRESHOLD = PRICE_CACHE_DEGRADED_DAYS
except ImportError:
    PRICE_BOOK_CONSTANTS_AVAILABLE = False
    # Fallback defaults if price_book is unavailable
    PRICE_CACHE_OK_DAYS = 14
    PRICE_CACHE_DEGRADED_DAYS = 30
    STALE_DAYS_THRESHOLD = 30
    DEGRADED_DAYS_THRESHOLD = 14
    CANONICAL_CACHE_PATH = "data/cache/prices_cache.parquet"  # Canonical path
    compute_system_health = None
    get_price_book = None

# Import wave performance functions for portfolio metrics
try:
    from helpers.wave_performance import compute_portfolio_snapshot
    WAVE_PERFORMANCE_AVAILABLE = True
except ImportError:
    WAVE_PERFORMANCE_AVAILABLE = False
    compute_portfolio_snapshot = None

# Import snapshot ledger for portfolio snapshot loading
try:
    from snapshot_ledger import generate_snapshot
    SNAPSHOT_LEDGER_AVAILABLE = True
except ImportError:
    SNAPSHOT_LEDGER_AVAILABLE = False
    generate_snapshot = None

# ============================================================================
# RUN TRACE - Track script execution and prevent infinite rerun loops
# ============================================================================

# Initialize run sequence counter and timing
if "run_seq" not in st.session_state:
    st.session_state.run_seq = 0
    st.session_state.last_run_time = datetime.now()
    st.session_state.last_trigger = "initial_load"
    st.session_state.buttons_clicked = []
    st.session_state.trigger_set_by_rerun = False
else:
    # Increment run sequence
    st.session_state.run_seq += 1
    
    # Calculate delta since last run
    current_time = datetime.now()
    delta_seconds = (current_time - st.session_state.last_run_time).total_seconds()
    st.session_state.delta_seconds = delta_seconds
    st.session_state.last_run_time = current_time
    
    # Detect what triggered this run (only if not already set by trigger_rerun)
    if not st.session_state.get("trigger_set_by_rerun", False):
        if st.session_state.get("_last_button_clicked"):
            st.session_state.last_trigger = f"button:{st.session_state._last_button_clicked}"
            st.session_state.buttons_clicked.append(st.session_state._last_button_clicked)
            st.session_state._last_button_clicked = None
        elif st.session_state.get("_widget_changed"):
            st.session_state.last_trigger = f"widget:{st.session_state._widget_changed}"
            st.session_state._widget_changed = None
        elif st.session_state.get("auto_refresh_enabled", False):
            st.session_state.last_trigger = "auto_refresh"
        else:
            st.session_state.last_trigger = "unknown"
    
    # Reset the flag after reading
    st.session_state.trigger_set_by_rerun = False

# ============================================================================
# WAVE PROFILE UI TOGGLE - Feature Flag
# ============================================================================
# Toggle constant to enable/disable the new Wave Profile tab and banner
# Set to False to instantly revert to the original UI layout
ENABLE_WAVE_PROFILE = True

# ============================================================================
# NEW FEATURE FLAGS - Wave Intelligence Center Enhancements
# ============================================================================
# RENDER_RICH_HTML: Enable/disable rich HTML rendering via st.components.v1.html
# Set to False to use plain st.markdown rendering instead
RENDER_RICH_HTML = os.environ.get("RENDER_RICH_HTML", "True").lower() == "true"

# SAFE_MODE: Enable/disable safe mode for error handling
# When enabled, catches exceptions in Wave Intelligence Center and falls back gracefully
SAFE_MODE = os.environ.get("SAFE_MODE", "False").lower() == "true"

# ============================================================================
# RERUN THROTTLE CONFIGURATION
# ============================================================================
# Rerun throttle settings to prevent rapid consecutive reruns
RERUN_THROTTLE_THRESHOLD = 0.5  # Minimum seconds between reruns (below this triggers counter)
MAX_RAPID_RERUNS = 3  # Number of rapid reruns before halting execution

# ============================================================================
# ROLLBACK SAFETY: Original app.py backed up as app.py.decision-engine-backup
# To restore: cp app.py.decision-engine-backup app.py
# ============================================================================

# ============================================================================
# DATA-READY CONFIGURATION - Price Caching and Wave Status
# ============================================================================
# Minimum days of price history required for a wave to be considered "Data-Ready"
# LOWERED from 60 to 7 to support partial data availability
MIN_DAYS_READY = 7

# Wave validation thresholds (must match performance table requirements)
WAVE_VALIDATION_MIN_COVERAGE_PCT = 100.0  # Require 100% ticker coverage
WAVE_VALIDATION_MIN_HISTORY_DAYS = 30  # Require 30 days minimum history

# Batch size for ticker price downloads to reduce rate-limit risks
PRICE_DOWNLOAD_BATCH_SIZE = 100

# Pause duration (seconds) between batches to avoid rate limiting
PRICE_DOWNLOAD_BATCH_PAUSE = 1

# ============================================================================
# AI EXECUTIVE BRIEFING CONFIGURATION
# ============================================================================
# Thresholds for AI Executive Briefing signals and assessments
# These values can be adjusted to calibrate signal sensitivity

# System Confidence thresholds
CONFIDENCE_HIGH_COVERAGE_PCT = 90.0  # Coverage % for High confidence
CONFIDENCE_MODERATE_COVERAGE_PCT = 70.0  # Coverage % for Moderate confidence

# Risk Regime thresholds (VIX-based)
RISK_REGIME_VIX_LOW = 15.0  # VIX below this = Risk-On
RISK_REGIME_VIX_HIGH = 25.0  # VIX above this = Risk-Off

# Risk Regime fallback thresholds (performance-based when VIX unavailable)
RISK_REGIME_PERF_RISK_ON = 0.5  # Average 1D return % for Risk-On
RISK_REGIME_PERF_RISK_OFF = -0.5  # Average 1D return % for Risk-Off

# Alpha Quality thresholds
ALPHA_QUALITY_STRONG_RETURN = 0.5  # Average 1D return % for Strong
ALPHA_QUALITY_STRONG_RATIO = 0.6  # Ratio of positive strategies for Strong
ALPHA_QUALITY_MIXED_RATIO = 0.5  # Ratio of positive strategies for Mixed

# Performance posture assessment thresholds
POSTURE_STRONG_POSITIVE = 0.5  # Avg 1D return % for "strong positive momentum"
POSTURE_WEAK_NEGATIVE = -0.5  # Avg 1D return % for "defensive positioning"

# Dispersion thresholds for risk assessment
DISPERSION_HIGH = 2.0  # Std dev % for "elevated dispersion"
DISPERSION_LOW = 0.5  # Std dev % for "low volatility"

# Data Integrity thresholds
DATA_INTEGRITY_VERIFIED_COVERAGE = 95.0  # Coverage % for Verified
DATA_INTEGRITY_DEGRADED_COVERAGE = 80.0  # Coverage % for Degraded

# ============================================================================
# PORTFOLIO VIEW CONFIGURATION
# ============================================================================
# Constants for portfolio-level snapshot rendering
PORTFOLIO_VIEW_PLACEHOLDER = "NONE"  # Placeholder value indicating no wave selected
PORTFOLIO_VIEW_TITLE = "Portfolio Snapshot (All Waves)"  # Display title for portfolio view
PORTFOLIO_VIEW_ICON = "üèõÔ∏è"  # Icon for portfolio view
WAVE_VIEW_ICON = "üåä"  # Icon for individual wave view

# Leaderboard thresholds
NEGLIGIBLE_RETURN_THRESHOLD = 0.1  # Return % below which to show context message

# Default signal values (used if calculation fails)
DEFAULT_ALPHA_QUALITY = "Mixed"
DEFAULT_RISK_REGIME = "Neutral"
DEFAULT_DATA_INTEGRITY = "Degraded"
DEFAULT_CONFIDENCE = "Moderate"
BATCH_PAUSE_MIN = 0.5
BATCH_PAUSE_MAX = 1.5

# Executive Summary attribution constants
ATTRIBUTION_TILT_STRENGTH = 0.8  # Momentum tilt strength for attribution calculation
ATTRIBUTION_BASE_EXPOSURE = 1.0  # Base exposure level for attribution calculation
ATTRIBUTION_TIMEFRAME_DAYS = 30  # Default timeframe for Executive Summary attribution display

# ============================================================================
# WAVE SELECTION HELPER FUNCTIONS
# ============================================================================

def resolve_app_context():
    """
    Canonical context resolver - Single source of truth for app context.
    
    This function resolves the current application context based on wave selection
    and mode settings in session state. It serves as the authoritative driver for
    all context-dependent rendering throughout the app.
    
    Returns:
        dict: Context dictionary with the following keys:
            - selected_wave_id (str or None): The unique wave ID, or None for portfolio
            - selected_wave_name (str or None): Display name for the wave, or None for portfolio
            - mode (str): Current mode (e.g., 'Standard', 'Aggressive', etc.)
            - context_key (str): Normalized cache key in format "{mode}:{selected_wave_id or 'PORTFOLIO'}"
    
    Example:
        >>> ctx = resolve_app_context()
        >>> print(ctx)
        {
            'selected_wave_id': 'wave_gold',
            'selected_wave_name': 'Gold Wave',
            'mode': 'Standard',
            'context_key': 'Standard:wave_gold'
        }
    """
    # Get wave_id from session state (authoritative source)
    selected_wave_id = st.session_state.get("selected_wave_id")
    
    # Derive display name from wave_id
    selected_wave_name = None
    if selected_wave_id is not None:
        if WAVES_ENGINE_AVAILABLE and get_display_name_from_wave_id is not None:
            try:
                selected_wave_name = get_display_name_from_wave_id(selected_wave_id)
            except (KeyError, ValueError, AttributeError):
                # Fallback: use wave_id as name if conversion fails
                selected_wave_name = f"Wave ({selected_wave_id})"
        else:
            # Fallback: use wave_id as name if engine unavailable
            selected_wave_name = f"Wave ({selected_wave_id})"
    
    # Get mode from session state
    mode = st.session_state.get("mode", "Standard")
    
    # Build normalized cache key
    wave_part = selected_wave_id if selected_wave_id is not None else "PORTFOLIO"
    context_key = f"{mode}:{wave_part}"
    
    return {
        "selected_wave_id": selected_wave_id,
        "selected_wave_name": selected_wave_name,
        "mode": mode,
        "context_key": context_key
    }


def get_selected_wave_display_name():
    """
    Get the display name for the currently selected wave.
    
    .. deprecated:: 2.0
        Use :func:`resolve_app_context` instead. This function will be removed in version 3.0.
        
        Instead of::
        
            display_name = get_selected_wave_display_name()
            
        Use::
        
            ctx = resolve_app_context()
            display_name = ctx["selected_wave_name"]
    
    Returns None if portfolio view is active, otherwise returns the
    display name corresponding to the wave_id stored in session_state.
    
    Returns:
        str or None: Display name of selected wave, or None for portfolio view
    """
    import warnings
    warnings.warn(
        "get_selected_wave_display_name() is deprecated and will be removed in version 3.0. "
        "Use resolve_app_context()['selected_wave_name'] instead.",
        DeprecationWarning,
        stacklevel=2
    )
    # Delegate to canonical context resolver
    ctx = resolve_app_context()
    return ctx["selected_wave_name"]

# Cache TTL for price downloads (in seconds) - 1 hour default
PRICE_CACHE_TTL = int(os.environ.get("PRICE_CACHE_TTL", "3600"))

# Safe asset tickers for overlays (cash, treasuries, etc.)
SAFE_ASSET_TICKERS = ["^IRX", "^FVX", "^TNX", "SHY", "IEF", "TLT", "BIL"]

# Snapshot Ledger maximum runtime (seconds) - prevents infinite hangs
SNAPSHOT_MAX_RUNTIME_SECONDS = 300

# ============================================================================
# DECISION ATTRIBUTION ENGINE - Observable Components Decomposition
# ============================================================================

from dataclasses import dataclass
from typing import Dict, List, Optional, Any
def metric_alpha_predecomp(value, *, label="Cumulative Alpha (Pre-Decomposition)"):
    """
    UI-only helper to render alpha metrics safely.
    No math changes. Presentation only.
    """
    try:
        display = f"{value:.2%}" if value is not None else "Pending"
    except Exception:
        display = "Pending"

    st.metric(
        label,
        display,
        help="Cumulative benchmark-relative alpha shown prior to full attribution decomposition."
    )
    st.caption("Benchmark-relative ¬∑ Capital-weighted ¬∑ Since inception")

def _fmt_pct_or_status(v, status):
    """
    Centralized helper for formatting alpha values in percentage terms or displaying a status.
    
    Args:
        v: Numeric value (in decimal form, e.g., 0.01 = 1%)
        status: Status label to display if v is None (e.g., "Pending", "Derived", "Reserved")
    
    Returns:
        Formatted string with percentage or status label
    """
    try:
        if v is None:
            return status
        return f"{v:+.2%}" if abs(v) <= 5 else f"{v:+.2f}%"
    except Exception:
        return status

@dataclass
class DecisionAttributionComponents:
    """
    Observable components of Wave performance decomposition.
    All values in decimal form (e.g., 0.01 = 1%).
    """
    # Core components
    selection_alpha: Optional[float] = None  # Asset selection contribution
    overlay_alpha: Optional[float] = None  # Exposure scaling & VIX gates vs fully invested
    risk_off_alpha: Optional[float] = None  # Cash contribution
    residual_alpha: Optional[float] = None  # Unexplained effects
    
    # Reconciliation
    total_alpha: Optional[float] = None  # Total realized alpha
    reconciled: bool = False  # Whether reconciliation succeeded
    reconciliation_error: Optional[float] = None
    
    # Availability flags
    selection_available: bool = False
    overlay_available: bool = False
    risk_off_available: bool = False
    residual_available: bool = False
    
    # Metadata
    data_completeness: float = 0.0  # 0.0 to 1.0
    warnings: List[str] = None
    
    def __post_init__(self):
        if self.warnings is None:
            self.warnings = []
    
    def get_confidence_level(self) -> str:
        """Get confidence level based on data completeness."""
        if self.data_completeness >= 0.9:
            return "High"
        elif self.data_completeness >= 0.6:
            return "Medium"
        elif self.data_completeness >= 0.3:
            return "Low"
        else:
            return "Very Low"
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for display."""
        return {
            'Selection Alpha': self.selection_alpha,
            'Overlay Alpha': self.overlay_alpha,
            'Risk-Off Alpha': self.risk_off_alpha,
            'Residual Alpha': self.residual_alpha,
            'Total Alpha': self.total_alpha,
            'Reconciled': self.reconciled,
            'Data Completeness': f"{self.data_completeness * 100:.1f}%",
            'Confidence': self.get_confidence_level(),
            'Warnings': len(self.warnings)
        }


@dataclass
class AuditTrailEntry:
    """Immutable audit trail entry for attribution calculations."""
    timestamp: datetime
    app_version: str
    git_commit: str
    git_branch: str
    wave_name: str
    calculation_type: str
    data_available: Dict[str, bool]
    calculations_performed: List[str]
    calculations_skipped: List[str]
    warnings: List[str]
    success: bool
    error_message: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for display."""
        return {
            'Timestamp': self.timestamp.strftime("%Y-%m-%d %H:%M:%S"),
            'App Version': self.app_version,
            'Git Commit': self.git_commit,
            'Git Branch': self.git_branch,
            'Wave': self.wave_name,
            'Calculation': self.calculation_type,
            'Data Available': ', '.join([k for k, v in self.data_available.items() if v]),
            'Performed': ', '.join(self.calculations_performed) if self.calculations_performed else 'None',
            'Skipped': ', '.join(self.calculations_skipped) if self.calculations_skipped else 'None',
            'Warnings': len(self.warnings),
            'Success': '‚úÖ' if self.success else '‚ùå',
            'Error': self.error_message or 'None'
        }


class DecisionAttributionEngine:
    """
    Decision Attribution Engine - Decomposes Wave performance into observable components.
    
    Components:
    1. Selection Alpha: Performance from asset selection
    2. Overlay Alpha: Performance from exposure scaling and VIX gates vs fully invested benchmark
    3. Risk-Off Alpha: Performance contribution from cash positions
    4. Residual Alpha: Unexplained performance effects
    
    Features:
    - Labels components as "Observed" or "Unavailable"
    - Reconciles to total return vs benchmark
    - Graceful degradation when data is incomplete
    """
    
    def __init__(self):
        self.audit_trail: List[AuditTrailEntry] = []
    
    def compute_attribution(
        self,
        wave_data: pd.DataFrame,
        wave_name: str,
        benchmark_data: Optional[pd.DataFrame] = None
    ) -> DecisionAttributionComponents:
        """
        Compute decision attribution for a wave.
        
        Args:
            wave_data: DataFrame with wave performance data
            wave_name: Name of the wave
            benchmark_data: Optional benchmark data for comparison
            
        Returns:
            DecisionAttributionComponents with observed and unavailable components
        """
        warnings = []
        calculations_performed = []
        calculations_skipped = []
        data_available = {}
        
        try:
            # Initialize components
            components = DecisionAttributionComponents()
            
            # Check data availability
            has_alpha = 'alpha' in wave_data.columns
            has_return = 'return' in wave_data.columns
            has_benchmark = 'benchmark_return' in wave_data.columns or benchmark_data is not None
            has_exposure = 'exposure' in wave_data.columns
            has_cash = 'cash_pct' in wave_data.columns or 'safe_pct' in wave_data.columns
            has_vix = 'vix' in wave_data.columns
            
            data_available = {
                'alpha': has_alpha,
                'return': has_return,
                'benchmark': has_benchmark,
                'exposure': has_exposure,
                'cash': has_cash,
                'vix': has_vix
            }
            
            # Calculate total alpha (if possible)
            if has_alpha:
                components.total_alpha = wave_data['alpha'].sum()
                calculations_performed.append('total_alpha')
            elif has_return and has_benchmark:
                wave_return = wave_data['return'].sum()
                if benchmark_data is not None:
                    benchmark_return = benchmark_data['return'].sum()
                else:
                    benchmark_return = wave_data['benchmark_return'].sum()
                components.total_alpha = wave_return - benchmark_return
                calculations_performed.append('total_alpha_from_returns')
            else:
                warnings.append("Total alpha unavailable - missing return or benchmark data")
                calculations_skipped.append('total_alpha')
            
            # Component 1: Selection Alpha (from underlying asset performance)
            if ALPHA_ATTRIBUTION_AVAILABLE and has_return:
                try:
                    # Use existing alpha attribution if available
                    from alpha_attribution import compute_alpha_attribution_series
                    
                    # Prepare data for attribution - transform column names
                    wave_data_copy = wave_data.copy()
                    
                    # Ensure date is index
                    if 'date' in wave_data_copy.columns:
                        wave_data_copy = wave_data_copy.set_index('date')
                    
                    # Transform column names: return -> wave_ret, benchmark_return -> bm_ret
                    if 'return' in wave_data_copy.columns and has_benchmark:
                        wave_data_copy['wave_ret'] = wave_data_copy['return']
                        if 'benchmark_return' in wave_data_copy.columns:
                            wave_data_copy['bm_ret'] = wave_data_copy['benchmark_return']
                        elif benchmark_data is not None and 'return' in benchmark_data.columns:
                            wave_data_copy['bm_ret'] = benchmark_data['return']
                        else:
                            wave_data_copy['bm_ret'] = 0.0
                    elif 'portfolio_return' in wave_data_copy.columns and 'benchmark_return' in wave_data_copy.columns:
                        wave_data_copy['wave_ret'] = wave_data_copy['portfolio_return']
                        wave_data_copy['bm_ret'] = wave_data_copy['benchmark_return']
                    else:
                        warnings.append("Selection alpha unavailable - missing return columns")
                        calculations_skipped.append('selection_alpha')
                        raise ValueError("Missing required return columns")
                    
                    # Call with correct parameter order: wave_name, mode, history_df
                    _, summary = compute_alpha_attribution_series(
                        wave_name=wave_name,
                        mode=st.session_state.get("mode", "Standard"),
                        history_df=wave_data_copy
                    )
                    
                    if summary and hasattr(summary, 'asset_selection_alpha'):
                        components.selection_alpha = summary.asset_selection_alpha
                        components.selection_available = True
                        calculations_performed.append('selection_alpha')
                    else:
                        warnings.append("Selection alpha computation returned no results")
                        calculations_skipped.append('selection_alpha')
                except Exception as e:
                    warnings.append(f"Selection alpha unavailable: {str(e)}")
                    calculations_skipped.append('selection_alpha')
            else:
                warnings.append("Selection alpha unavailable - missing attribution module or return data")
                calculations_skipped.append('selection_alpha')
            
            # Component 2: Overlay Alpha (exposure scaling & VIX gates)
            if has_exposure and has_return and has_benchmark:
                try:
                    # Calculate overlay alpha as difference between actual and fully invested
                    if 'exposure' in wave_data.columns:
                        # Fully invested would be exposure = 1.0 always
                        actual_exposure = wave_data['exposure'].fillna(1.0)
                        exposure_impact = wave_data['return'] * (actual_exposure - 1.0)
                        components.overlay_alpha = exposure_impact.sum()
                        components.overlay_available = True
                        calculations_performed.append('overlay_alpha')
                    else:
                        warnings.append("Overlay alpha unavailable - missing exposure data")
                        calculations_skipped.append('overlay_alpha')
                except Exception as e:
                    warnings.append(f"Overlay alpha calculation failed: {str(e)}")
                    calculations_skipped.append('overlay_alpha')
            else:
                warnings.append("Overlay alpha unavailable - missing exposure, return, or benchmark data")
                calculations_skipped.append('overlay_alpha')
            
            # Component 3: Risk-Off Alpha (cash contribution)
            if has_cash and has_return:
                try:
                    cash_col = 'cash_pct' if 'cash_pct' in wave_data.columns else 'safe_pct'
                    cash_pct = wave_data[cash_col].fillna(0.0)
                    
                    # Cash contribution: opportunity cost of holding cash vs being invested
                    # Positive if market went down while holding cash, negative if market went up
                    if has_benchmark:
                        if benchmark_data is not None:
                            benchmark_return = benchmark_data['return']
                        else:
                            benchmark_return = wave_data['benchmark_return']
                        
                        # Cash benefit = cash% * (-benchmark_return)
                        # Positive when benchmark negative and we held cash
                        components.risk_off_alpha = (cash_pct * (-benchmark_return)).sum()
                        components.risk_off_available = True
                        calculations_performed.append('risk_off_alpha')
                    else:
                        warnings.append("Risk-off alpha partial - no benchmark for comparison")
                        calculations_skipped.append('risk_off_alpha')
                except Exception as e:
                    warnings.append(f"Risk-off alpha calculation failed: {str(e)}")
                    calculations_skipped.append('risk_off_alpha')
            else:
                warnings.append("Risk-off alpha unavailable - missing cash or return data")
                calculations_skipped.append('risk_off_alpha')
            
            # Component 4: Residual Alpha (unexplained)
            if components.total_alpha is not None:
                # Residual = Total - (Selection + Overlay + Risk-Off)
                known_components = 0.0
                if components.selection_alpha is not None:
                    known_components += components.selection_alpha
                if components.overlay_alpha is not None:
                    known_components += components.overlay_alpha
                if components.risk_off_alpha is not None:
                    known_components += components.risk_off_alpha
                
                components.residual_alpha = components.total_alpha - known_components
                components.residual_available = True
                calculations_performed.append('residual_alpha')
            else:
                warnings.append("Residual alpha unavailable - total alpha unknown")
                calculations_skipped.append('residual_alpha')
            
            # Reconciliation check
            if components.total_alpha is not None:
                reconstructed = 0.0
                if components.selection_alpha is not None:
                    reconstructed += components.selection_alpha
                if components.overlay_alpha is not None:
                    reconstructed += components.overlay_alpha
                if components.risk_off_alpha is not None:
                    reconstructed += components.risk_off_alpha
                if components.residual_alpha is not None:
                    reconstructed += components.residual_alpha
                
                components.reconciliation_error = abs(components.total_alpha - reconstructed)
                components.reconciled = components.reconciliation_error < 0.0001  # 0.01% tolerance
                
                if not components.reconciled:
                    warnings.append(f"Reconciliation error: {components.reconciliation_error*100:.4f}%")
            
            # Calculate data completeness
            available_count = sum([
                components.selection_available,
                components.overlay_available,
                components.risk_off_available,
                components.residual_available
            ])
            components.data_completeness = available_count / 4.0
            
            # Store warnings
            components.warnings = warnings
            
            # Create audit trail entry
            audit_entry = AuditTrailEntry(
                timestamp=datetime.now(),
                app_version="v2.0-attribution",
                git_commit=get_git_commit_hash(),
                git_branch=get_git_branch_name(),
                wave_name=wave_name,
                calculation_type="Decision Attribution",
                data_available=data_available,
                calculations_performed=calculations_performed,
                calculations_skipped=calculations_skipped,
                warnings=warnings,
                success=True
            )
            self.audit_trail.append(audit_entry)
            
            return components
            
        except Exception as e:
            # Create error audit trail entry
            audit_entry = AuditTrailEntry(
                timestamp=datetime.now(),
                app_version="v2.0-attribution",
                git_commit=get_git_commit_hash(),
                git_branch=get_git_branch_name(),
                wave_name=wave_name,
                calculation_type="Decision Attribution",
                data_available=data_available,
                calculations_performed=calculations_performed,
                calculations_skipped=calculations_skipped,
                warnings=warnings,
                success=False,
                error_message=str(e)
            )
            self.audit_trail.append(audit_entry)
            
            # Return empty components with error
            components = DecisionAttributionComponents()
            components.warnings = warnings + [f"Attribution failed: {str(e)}"]
            return components
    
    def get_audit_trail(self) -> List[AuditTrailEntry]:
        """Get the complete audit trail."""
        return self.audit_trail
    
    def get_latest_audit_entry(self) -> Optional[AuditTrailEntry]:
        """Get the most recent audit trail entry."""
        return self.audit_trail[-1] if self.audit_trail else None


# Global instance of the attribution engine
_attribution_engine = None

def get_attribution_engine() -> DecisionAttributionEngine:
    """Get or create the global attribution engine instance."""
    global _attribution_engine
    if _attribution_engine is None:
        _attribution_engine = DecisionAttributionEngine()
    return _attribution_engine


# ============================================================================
# SECTION 1: CONFIGURATION AND STYLING
# ============================================================================

# NOTE: st.set_page_config() moved to main() function to avoid import-time execution

# ============================================================================
# PROOF BANNER - Diagnostics and Visibility (moved to render_proof_banner)
# ============================================================================

def render_proof_banner():
    """
    Render proof banner with diagnostics information.
    Moved from module level to function to avoid import-time execution.
    """
    # Initialize run counter in session state
    if "proof_run_counter" not in st.session_state:
        st.session_state.proof_run_counter = 0
    else:
        st.session_state.proof_run_counter += 1

    # Get basename of __file__
    try:
        file_basename = os.path.basename(__file__)
    except Exception:
        file_basename = "app.py"

    # Get GIT SHA with best-effort fallback
    git_sha_proof = "SHA unavailable"
    try:
        # Try environment variable first
        git_sha_env = os.environ.get('GIT_SHA') or os.environ.get('BUILD_ID')
        if git_sha_env:
            git_sha_proof = git_sha_env
        else:
            # Try reading from git command
            result = subprocess.run(
                ['git', 'rev-parse', '--short', 'HEAD'],
                capture_output=True,
                text=True,
                timeout=2,
                cwd=os.path.dirname(os.path.abspath(__file__))
            )
            if result.returncode == 0 and result.stdout.strip():
                git_sha_proof = result.stdout.strip()
    except Exception:
        # Keep default "SHA unavailable" - never crash
        pass

    # Display Proof Banner
    st.markdown(
        f"""
        <div style="background-color: #2d2d2d; padding: 12px 20px; border: 2px solid #ff9800; margin-bottom: 16px; border-radius: 6px;">
            <div style="color: #ff9800; font-size: 14px; font-family: monospace; font-weight: bold; margin-bottom: 4px;">
                üîç PROOF BANNER - DIAGNOSTICS MODE
            </div>
            <div style="color: #e0e0e0; font-size: 12px; font-family: monospace;">
                <strong>FILE:</strong> {file_basename}<br>
                <strong>GIT SHA:</strong> {git_sha_proof}<br>
                <strong>UTC TIMESTAMP:</strong> {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}<br>
                <strong>RUN COUNTER:</strong> {st.session_state.proof_run_counter}
            </div>
        </div>
        """,
        unsafe_allow_html=True
    )

# ============================================================================
# WALL-CLOCK WATCHDOG - Absolute timeout for Safe Mode
# ============================================================================
# Record start time immediately after page config for watchdog
WATCHDOG_START_TIME = time.time()

# ============================================================================
# BUILD/VERSION STAMP - Display build info for verification (moved to function)
# ============================================================================

def get_build_info():
    """
    Get build information for display in UI.
    Returns: dict with 'sha', 'branch', 'utc' keys
    
    Note: Uses inline Git commands here since this runs at module load time,
    before the helper functions (get_git_commit_hash, get_git_branch_name) 
    are defined later in the file.
    
    This function retrieves:
    - Git short SHA from environment variable or git command
    - Current branch name from environment variable or git command
    - Current UTC timestamp (non-cached)
    """
    from datetime import timezone
    
    build_info = {
        'sha': 'unknown',
        'branch': 'unknown',
        'utc': datetime.now(timezone.utc).isoformat()
    }
    
    # Get Git SHA: prioritize environment variable, fallback to git command
    git_sha_env = os.environ.get('GIT_SHA') or os.environ.get('BUILD_ID')
    if git_sha_env:
        build_info['sha'] = git_sha_env
    else:
        try:
            # Fallback to git command
            result = subprocess.run(
                ['git', 'rev-parse', '--short', 'HEAD'],
                capture_output=True,
                text=True,
                timeout=2
            )
            if result.returncode == 0:
                build_info['sha'] = result.stdout.strip()
        except:
            pass
    
    # Get branch name: prioritize environment variable, fallback to git command
    git_branch_env = os.environ.get('GIT_BRANCH') or os.environ.get('BRANCH_NAME')
    if git_branch_env:
        build_info['branch'] = git_branch_env
    else:
        try:
            # Fallback to git command
            result = subprocess.run(
                ['git', 'rev-parse', '--abbrev-ref', 'HEAD'],
                capture_output=True,
                text=True,
                timeout=2
            )
            if result.returncode == 0:
                build_info['branch'] = result.stdout.strip()
        except:
            pass
    
    return build_info

def render_build_stamp():
    """
    Render build stamp banner.
    Moved from module level to function to avoid import-time execution.
    """
    build_info = get_build_info()
    st.markdown(
        f"""
        <div style="background-color: #1e1e1e; padding: 8px 16px; border-left: 3px solid #00d4ff; margin-bottom: 16px;">
            <span style="color: #888; font-size: 12px; font-family: monospace;">
                BUILD: {build_info['sha']} | BRANCH: {build_info['branch']} | UTC: {build_info['utc']}
            </span>
        </div>
        """,
        unsafe_allow_html=True
    )

# Cache keys for wave universe management
WAVE_UNIVERSE_CACHE_KEYS = ["wave_universe", "waves_list", "universe_cache", "wave_history_cache"]


# ============================================================================
# PORTFOLIO VIEW HELPER FUNCTIONS
# ============================================================================

def is_portfolio_context(selected_wave: str) -> bool:
    """
    Determine if the current context is portfolio-level (no specific wave selected).
    
    Args:
        selected_wave: The currently selected wave name (None or placeholder for portfolio)
    
    Returns:
        True if portfolio context, False if specific wave is selected
    """
    return selected_wave is None or selected_wave == PORTFOLIO_VIEW_PLACEHOLDER


# ============================================================================
# WAVE PROFILE BANNER - Enhanced Header Display with Quick Stats
# ============================================================================

def render_selected_wave_banner_enhanced(selected_wave: str, mode: str):
    """
    Render an enhanced pinned banner at the top of the page with quick stats.
    
    Shows:
    - Wave title with neon border/glow effect (or "Portfolio Snapshot" when no wave selected)
    - Colored pill for Mode
    - Quick stats tiles: Wave NAV, Returns (1D/30D/60D/365D), Alpha Captured
    - Wave-specific metrics (Beta, VIX regime, Exposure %, Cash %) only shown for individual waves
    
    Args:
        selected_wave: The name of the currently selected wave (None for portfolio view)
        mode: The current mode (e.g., "Standard", "Alpha-Minus-Beta", "Private Logic")
    """
    try:
        # Determine if we're in portfolio context (no specific wave selected)
        is_portfolio_view = is_portfolio_context(selected_wave)
        
        # Get wave data for metrics (only if specific wave is selected)
        wave_data_30d = None
        wave_data_60d = None
        wave_data_365d = None
        
        if not is_portfolio_view and selected_wave is not None:
            wave_data_30d = get_wave_data_filtered(wave_name=selected_wave, days=30)
            wave_data_60d = get_wave_data_filtered(wave_name=selected_wave, days=60)
            wave_data_365d = get_wave_data_filtered(wave_name=selected_wave, days=365)
        
        # Calculate metrics
        nav_str = "N/A"
        ret_1d_str = "N/A"
        ret_30d_str = "N/A"
        ret_60d_str = "N/A"
        ret_365d_str = "N/A"
        alpha_1d_str = "N/A"
        alpha_30d_str = "N/A"
        alpha_60d_str = "N/A"
        alpha_365d_str = "N/A"
        beta_str = "N/A"
        vix_regime_str = "N/A"
        exposure_str = "N/A"
        cash_str = "N/A"
        
        # ========================================================================
        # PORTFOLIO VIEW: Compute portfolio-level metrics using compute_portfolio_snapshot
        # ========================================================================
        if is_portfolio_view:
            # Use module-level imports if available
            if WAVE_PERFORMANCE_AVAILABLE and PRICE_BOOK_CONSTANTS_AVAILABLE:
                try:
                    # Load PRICE_BOOK
                    price_book = get_cached_price_book()
                    
                    # Use ENGINE_RUNNING as a reentrancy lock
                    if not st.session_state.get("ENGINE_RUNNING", False):
                        st.session_state.ENGINE_RUNNING = True
                        try:
                            # Compute portfolio snapshot with all periods
                            snapshot = compute_portfolio_snapshot(price_book, mode=mode, periods=[1, 30, 60, 365])
                            
                            # Store debug info in session state for diagnostics panel
                            # Always update to ensure fresh debug data (even on failure)
                            if 'debug' in snapshot:
                                st.session_state['portfolio_snapshot_debug'] = snapshot['debug']
                            
                            if snapshot['success']:
                                # Extract portfolio returns
                                ret_1d = snapshot['portfolio_returns'].get('1D')
                                ret_30d = snapshot['portfolio_returns'].get('30D')
                                ret_60d = snapshot['portfolio_returns'].get('60D')
                                ret_365d = snapshot['portfolio_returns'].get('365D')
                                
                                # Extract alphas
                                alpha_1d = snapshot['alphas'].get('1D')
                                alpha_30d = snapshot['alphas'].get('30D')
                                alpha_60d = snapshot['alphas'].get('60D')
                                alpha_365d = snapshot['alphas'].get('365D')
                                
                                # Format return strings
                                ret_1d_str = f"{ret_1d*100:+.2f}%" if ret_1d is not None else "‚Äî"
                                ret_30d_str = f"{ret_30d*100:+.2f}%" if ret_30d is not None else "‚Äî"
                                ret_60d_str = f"{ret_60d*100:+.2f}%" if ret_60d is not None else "‚Äî"
                                ret_365d_str = f"{ret_365d*100:+.2f}%" if ret_365d is not None else "‚Äî"
                                
                                # Format alpha strings
                                alpha_1d_str = f"{alpha_1d*100:+.2f}%" if alpha_1d is not None else "‚Äî"
                                alpha_30d_str = f"{alpha_30d*100:+.2f}%" if alpha_30d is not None else "‚Äî"
                                alpha_60d_str = f"{alpha_60d*100:+.2f}%" if alpha_60d is not None else "‚Äî"
                                alpha_365d_str = f"{alpha_365d*100:+.2f}%" if alpha_365d is not None else "‚Äî"
                        finally:
                            st.session_state.ENGINE_RUNNING = False
                except Exception as e:
                    # Log error but keep N/A values (graceful degradation)
                    logging.warning(f"Failed to compute portfolio snapshot for banner: {e}")
        
        # ========================================================================
        # WAVE VIEW: Calculate wave-specific metrics from historical data
        # ========================================================================
        else:
            # Calculate from 30-day data
            if wave_data_30d is not None and len(wave_data_30d) > 0:
                # 1D return (latest day)
                if 'portfolio_return' in wave_data_30d.columns:
                    latest_return = wave_data_30d['portfolio_return'].iloc[-1] if len(wave_data_30d) > 0 else 0
                    ret_1d_str = f"{latest_return*100:.2f}%"
                    
                    # 30D return
                    ret_30d = wave_data_30d['portfolio_return'].sum()
                    ret_30d_str = f"{ret_30d*100:.2f}%"
                
                # Alpha calculations
                if 'portfolio_return' in wave_data_30d.columns and 'benchmark_return' in wave_data_30d.columns:
                    # 1D alpha
                    latest_alpha = wave_data_30d['portfolio_return'].iloc[-1] - wave_data_30d['benchmark_return'].iloc[-1] if len(wave_data_30d) > 0 else 0
                    alpha_1d_str = f"{latest_alpha*100:.2f}%"
                    
                    # 30D alpha
                    alpha_30d = wave_data_30d['portfolio_return'].sum() - wave_data_30d['benchmark_return'].sum()
                    alpha_30d_str = f"{alpha_30d*100:.2f}%"
                
                # Exposure and cash
                if 'exposure' in wave_data_30d.columns:
                    avg_exposure = wave_data_30d['exposure'].mean()
                    exposure_str = f"{avg_exposure*100:.1f}%"
                    cash_str = f"{(1-avg_exposure)*100:.1f}%"
                
                # VIX regime
                if 'vix' in wave_data_30d.columns:
                    latest_vix = wave_data_30d['vix'].iloc[-1] if len(wave_data_30d) > 0 else 0
                    if latest_vix < 15:
                        vix_regime_str = "Low"
                    elif latest_vix < 20:
                        vix_regime_str = "Normal"
                    elif latest_vix < 30:
                        vix_regime_str = "Elevated"
                    else:
                        vix_regime_str = "High"
                
                # Beta calculation (simplified)
                if 'portfolio_return' in wave_data_30d.columns and 'benchmark_return' in wave_data_30d.columns:
                    try:
                        port_returns = wave_data_30d['portfolio_return']
                        bench_returns = wave_data_30d['benchmark_return']
                        covariance = np.cov(port_returns, bench_returns)[0][1]
                        variance = np.var(bench_returns)
                        if variance > 0:
                            beta = covariance / variance
                            beta_str = f"{beta:.2f}"
                    except:
                        pass
            
            # 60D return
            if wave_data_60d is not None and len(wave_data_60d) > 0:
                if 'portfolio_return' in wave_data_60d.columns:
                    ret_60d = wave_data_60d['portfolio_return'].sum()
                    ret_60d_str = f"{ret_60d*100:.2f}%"
                
                if 'portfolio_return' in wave_data_60d.columns and 'benchmark_return' in wave_data_60d.columns:
                    alpha_60d = wave_data_60d['portfolio_return'].sum() - wave_data_60d['benchmark_return'].sum()
                    alpha_60d_str = f"{alpha_60d*100:.2f}%"
            
            # 365D return
            if wave_data_365d is not None and len(wave_data_365d) > 0:
                if 'portfolio_return' in wave_data_365d.columns:
                    ret_365d = wave_data_365d['portfolio_return'].sum()
                    ret_365d_str = f"{ret_365d*100:.2f}%"
                
                if 'portfolio_return' in wave_data_365d.columns and 'benchmark_return' in wave_data_365d.columns:
                    alpha_365d = wave_data_365d['portfolio_return'].sum() - wave_data_365d['benchmark_return'].sum()
                    alpha_365d_str = f"{alpha_365d*100:.2f}%"
        
        # Mode color pill
        mode_color = "#00ff88" if mode == "Standard" else "#ffd700" if mode == "Aggressive" else "#ff6b6b"
        
        # Set display title and icon based on context
        display_title = PORTFOLIO_VIEW_TITLE if is_portfolio_view else selected_wave
        display_icon = PORTFOLIO_VIEW_ICON if is_portfolio_view else WAVE_VIEW_ICON
        
        # Get VIX overlay status for this wave
        vix_overlay_active = False
        vix_overlay_status_str = "N/A"
        if VIX_CONFIG_AVAILABLE and not is_portfolio_view:
            try:
                vix_status = get_vix_overlay_status_for_wave(selected_wave)
                vix_overlay_active = vix_status["is_enabled_for_wave"] and vix_status["is_equity_wave"]
                if vix_status["is_equity_wave"]:
                    if vix_status["is_enabled_for_wave"]:
                        vix_overlay_status_str = "üü¢ LIVE"
                    else:
                        vix_overlay_status_str = "‚ö™ Off"
                else:
                    vix_overlay_status_str = "‚Äî"  # Not applicable for non-equity waves
            except:
                vix_overlay_status_str = "N/A"
        
        # Wave-specific metrics HTML (shown only for individual waves)
        wave_specific_metrics_html = ""
        if not is_portfolio_view:
            wave_specific_metrics_html = f'''<div class="stat-tile">
                    <div class="stat-label">Beta</div>
                    <div class="stat-value">{beta_str}</div>
                </div>
                <div class="stat-tile">
                    <div class="stat-label">VIX Regime</div>
                    <div class="stat-value">{vix_regime_str}</div>
                </div>
                <div class="stat-tile">
                    <div class="stat-label">VIX Overlay</div>
                    <div class="stat-value" style="font-size: 0.9em;">{vix_overlay_status_str}</div>
                </div>
                <div class="stat-tile">
                    <div class="stat-label">Exposure</div>
                    <div class="stat-value">{exposure_str}</div>
                </div>
                <div class="stat-tile">
                    <div class="stat-label">Cash</div>
                    <div class="stat-value">{cash_str}</div>
                </div>'''
        
        # Informational message for portfolio view
        portfolio_info_html = ""
        if is_portfolio_view:
            portfolio_info_html = '''<div class="portfolio-info">
                &#9432; Wave-specific metrics (Beta, Exposure, Cash, VIX regime) unavailable at portfolio level
            </div>'''
        
        # Enhanced banner with stats
        banner_html = f"""
        <style>
            @keyframes glow {{
                0%, 100% {{ box-shadow: 0 0 10px rgba(0, 217, 255, 0.5), 0 0 20px rgba(0, 217, 255, 0.3); }}
                50% {{ box-shadow: 0 0 20px rgba(0, 217, 255, 0.8), 0 0 30px rgba(0, 217, 255, 0.5); }}
            }}
            
            .wave-banner {{
                background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
                border: 3px solid #00d9ff;
                border-radius: 12px;
                padding: 20px 30px;
                margin-bottom: 20px;
                animation: glow 3s infinite;
            }}
            
            .wave-title {{
                color: #ffffff;
                font-size: 28px;
                font-weight: bold;
                margin: 0 0 12px 0;
                text-align: center;
                text-transform: uppercase;
                letter-spacing: 2px;
            }}
            
            .mode-pill {{
                display: inline-block;
                background: {mode_color};
                color: #1a1a2e;
                padding: 5px 15px;
                border-radius: 20px;
                font-weight: bold;
                font-size: 14px;
                margin-left: 10px;
            }}
            
            .stats-grid {{
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(90px, 1fr));
                gap: 8px;
                margin-top: 12px;
            }}
            
            .stat-tile {{
                background: rgba(255, 255, 255, 0.1);
                border: 1px solid rgba(0, 217, 255, 0.3);
                border-radius: 8px;
                padding: 8px 6px;
                text-align: center;
            }}
            
            .stat-label {{
                color: #00d9ff;
                font-size: 9px;
                text-transform: uppercase;
                margin-bottom: 2px;
            }}
            
            .stat-value {{
                color: #ffffff;
                font-size: 13px;
                font-weight: bold;
            }}
            
            .portfolio-info {{
                text-align: center;
                margin-top: 12px;
                color: #a8dadc;
                font-size: 11px;
                font-style: italic;
            }}
            
            /* Mobile responsiveness */
            @media only screen and (max-width: 768px) {{
                .wave-banner {{
                    padding: 15px 12px;
                    margin-bottom: 15px;
                    border-radius: 10px;
                    border-width: 2px;
                }}
                
                .wave-title {{
                    font-size: 20px;
                    margin: 0 0 10px 0;
                    letter-spacing: 1px;
                }}
                
                .mode-pill {{
                    padding: 4px 12px;
                    font-size: 12px;
                    margin-left: 6px;
                }}
                
                .stats-grid {{
                    grid-template-columns: repeat(3, 1fr);
                    gap: 6px;
                    margin-top: 10px;
                }}
                
                .stat-tile {{
                    padding: 6px 4px;
                    border-radius: 6px;
                }}
                
                .stat-label {{
                    font-size: 8px;
                    margin-bottom: 1px;
                }}
                
                .stat-value {{
                    font-size: 11px;
                }}
            }}
        </style>
        
        <div class="wave-banner">
            <div class="wave-title">
                <span style="color: #00d9ff;">{display_icon}</span> {display_title}
                <span class="mode-pill">{mode}</span>
            </div>
            
            <div class="stats-grid">
                <div class="stat-tile">
                    <div class="stat-label">1D Return</div>
                    <div class="stat-value">{ret_1d_str}</div>
                </div>
                <div class="stat-tile">
                    <div class="stat-label">30D Return</div>
                    <div class="stat-value">{ret_30d_str}</div>
                </div>
                <div class="stat-tile">
                    <div class="stat-label">60D Return</div>
                    <div class="stat-value">{ret_60d_str}</div>
                </div>
                <div class="stat-tile">
                    <div class="stat-label">365D Return</div>
                    <div class="stat-value">{ret_365d_str}</div>
                </div>
                <div class="stat-tile">
                    <div class="stat-label">Alpha 1D</div>
                    <div class="stat-value">{alpha_1d_str}</div>
                </div>
                <div class="stat-tile">
                    <div class="stat-label">Alpha 30D</div>
                    <div class="stat-value">{alpha_30d_str}</div>
                </div>
                <div class="stat-tile">
                    <div class="stat-label">Alpha 60D</div>
                    <div class="stat-value">{alpha_60d_str}</div>
                </div>
                <div class="stat-tile">
                    <div class="stat-label">Alpha 365D</div>
                    <div class="stat-value">{alpha_365d_str}</div>
                </div>
                {wave_specific_metrics_html}
            </div>
            {portfolio_info_html}
        </div>
        """
        
        # Use render_html_safe for proper HTML rendering (uses st.html() when Rich HTML 
        # rendering is enabled, falls back to st.markdown with unsafe_allow_html=True)
        render_html_safe(banner_html)
        
    except Exception as e:
        # Fallback to simple banner if enhanced version fails
        st.warning(f"‚ö†Ô∏è Enhanced banner unavailable, using fallback: {str(e)}")
        render_selected_wave_banner_simple(selected_wave, mode)


def render_selected_wave_banner_simple(selected_wave: str, mode: str):
    """
    Simple fallback banner - original implementation.
    
    Args:
        selected_wave: The name of the currently selected wave (None for portfolio view)
        mode: The current mode (e.g., "Standard", "Alpha-Minus-Beta", "Private Logic")
    """
    # Determine if we're in portfolio context (no specific wave selected)
    is_portfolio_view = is_portfolio_context(selected_wave)
    
    # Set display title and icon based on context
    display_title = PORTFOLIO_VIEW_TITLE if is_portfolio_view else selected_wave
    display_icon = PORTFOLIO_VIEW_ICON if is_portfolio_view else WAVE_VIEW_ICON
    
    # Safe HTML/CSS rendering with dark gradient background and neon accent border
    banner_html = f"""
    <div style="
        background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
        border: 3px solid #00d9ff;
        border-radius: 12px;
        padding: 20px 30px;
        margin-bottom: 25px;
        box-shadow: 0 4px 6px rgba(0, 217, 255, 0.3);
    ">
        <h2 style="
            color: #ffffff;
            font-size: 28px;
            font-weight: bold;
            margin: 0;
            text-align: center;
            text-transform: uppercase;
            letter-spacing: 2px;
        ">
            <span style="color: #00d9ff;">{display_icon} {'PORTFOLIO VIEW:' if is_portfolio_view else 'SELECTED WAVE:'}</span> {display_title} 
            <span style="color: #ffd700;">‚Ä¢</span> 
            <span style="color: #00ff88;">MODE:</span> {mode}
        </h2>
    </div>
    """
    # Use render_html_safe for proper HTML rendering (uses st.html() when Rich HTML 
    # rendering is enabled, falls back to st.markdown with unsafe_allow_html=True)
    render_html_safe(banner_html)


# ============================================================================
# STICKY HEADER COMPONENT - Pinned Tab Header
# ============================================================================

def render_sticky_header(selected_wave: str, mode: str):
    """
    Render a sticky header that appears at the top of every tab.
    
    Displays:
    - Selected Wave name
    - Active Mode
    - Benchmark name
    - Exposure %
    - Cash %
    - Today Return
    - Today Alpha
    
    Args:
        selected_wave: The name of the currently selected wave
        mode: The current mode (e.g., "Standard", "Alpha-Minus-Beta", "Private Logic")
    """
    try:
        # Get wave data for today's metrics
        wave_data = get_wave_data_filtered(wave_name=selected_wave, days=5)
        
        # Initialize default values
        benchmark_name = "SPY"
        exposure_pct = "N/A"
        cash_pct = "N/A"
        today_return = "N/A"
        today_alpha = "N/A"
        
        # Calculate metrics if data is available
        if wave_data is not None and len(wave_data) > 0:
            # Get benchmark name from config
            wave_config_path = os.path.join(os.path.dirname(__file__), 'wave_config.csv')
            if os.path.exists(wave_config_path):
                config_df = pd.read_csv(wave_config_path)
                wave_config = config_df[config_df['Wave'] == selected_wave]
                if len(wave_config) > 0:
                    benchmark_name = wave_config.iloc[0].get('Benchmark', 'SPY')
            
            # Get latest day metrics
            if 'exposure' in wave_data.columns:
                latest_exposure = wave_data['exposure'].iloc[-1] if len(wave_data) > 0 else 0
                exposure_pct = f"{latest_exposure * 100:.1f}%"
                cash_pct = f"{(1 - latest_exposure) * 100:.1f}%"
            
            if 'portfolio_return' in wave_data.columns:
                latest_return = wave_data['portfolio_return'].iloc[-1] if len(wave_data) > 0 else 0
                today_return = f"{latest_return * 100:.2f}%"
            
            if 'portfolio_return' in wave_data.columns and 'benchmark_return' in wave_data.columns:
                latest_alpha = wave_data['portfolio_return'].iloc[-1] - wave_data['benchmark_return'].iloc[-1] if len(wave_data) > 0 else 0
                today_alpha = f"{latest_alpha * 100:.2f}%"
        
        # Create sticky header HTML
        sticky_header_html = f"""
        <style>
            .sticky-header {{
                position: sticky;
                top: 0;
                z-index: 999;
                background: linear-gradient(135deg, #2c3e50 0%, #34495e 100%);
                border-bottom: 3px solid #00d9ff;
                padding: 12px 20px;
                margin-bottom: 15px;
                box-shadow: 0 2px 8px rgba(0, 0, 0, 0.3);
                border-radius: 0 0 8px 8px;
            }}
            
            .sticky-header-grid {{
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(120px, 1fr));
                gap: 12px;
                align-items: center;
            }}
            
            .sticky-header-item {{
                text-align: center;
                padding: 5px;
            }}
            
            .sticky-header-label {{
                color: #00d9ff;
                font-size: 11px;
                font-weight: bold;
                text-transform: uppercase;
                letter-spacing: 0.5px;
                margin-bottom: 3px;
            }}
            
            .sticky-header-value {{
                color: #ffffff;
                font-size: 14px;
                font-weight: bold;
            }}
            
            .sticky-header-wave {{
                color: #ffd700;
                font-size: 16px;
                font-weight: bold;
            }}
        </style>
        
        <div class="sticky-header">
            <div class="sticky-header-grid">
                <div class="sticky-header-item">
                    <div class="sticky-header-label">Wave</div>
                    <div class="sticky-header-wave">{selected_wave}</div>
                </div>
                <div class="sticky-header-item">
                    <div class="sticky-header-label">Mode</div>
                    <div class="sticky-header-value">{mode}</div>
                </div>
                <div class="sticky-header-item">
                    <div class="sticky-header-label">Benchmark</div>
                    <div class="sticky-header-value">{benchmark_name}</div>
                </div>
                <div class="sticky-header-item">
                    <div class="sticky-header-label">Exposure</div>
                    <div class="sticky-header-value">{exposure_pct}</div>
                </div>
                <div class="sticky-header-item">
                    <div class="sticky-header-label">Cash</div>
                    <div class="sticky-header-value">{cash_pct}</div>
                </div>
                <div class="sticky-header-item">
                    <div class="sticky-header-label">Today Return</div>
                    <div class="sticky-header-value">{today_return}</div>
                </div>
                <div class="sticky-header-item">
                    <div class="sticky-header-label">Today Alpha</div>
                    <div class="sticky-header-value">{today_alpha}</div>
                </div>
            </div>
        </div>
        """
        
        # Render using safe HTML rendering
        render_html_safe(sticky_header_html, height=100, scrolling=False)
        
    except Exception as e:
        # Silent fail - don't disrupt tab rendering
        pass


# ============================================================================
# WAVE LINEUP CONFIGURATION - Equity Lineup Reset v1
# ============================================================================

# Legacy Crypto Waves to exclude (removed per requirement)
# All legacy crypto waves have been replaced with new crypto lineup:
# - 5 Crypto Growth Waves (L1, DeFi, L2, AI, Broad)
# - 1 Crypto Income Wave
# Legacy waves no longer exist in the system
EXCLUDED_CRYPTO_WAVES = set()  # Empty - all legacy waves removed

# Crypto sector classification for CSE universe identification
# Based on institutional crypto classification standards (Grayscale, 21Shares, MarketVector)
CRYPTO_SECTORS = [
    'Store of Value / Settlement',
    'Smart Contract Platforms (Layer 1)',
    'Scaling Solutions (Layer 2)',
    'Decentralized Finance (DeFi)',
    'Infrastructure',
    'AI / Compute / Data',
    'Gaming / Metaverse',
    'Payments / Remittance',
    'Yield/Staking',
    'Stablecoin Infrastructure'
]

# All Equity Waves that should be included in the lineup
# Includes restored waves + 5 new waves (per requirement)
INCLUDED_EQUITY_WAVES = {
    # Core Index Waves
    "S&P 500 Wave",
    "Growth Wave",
    "Small Cap Growth Wave", 
    "Small-Mid Cap Growth Wave",
    
    # Thematic/Sector Waves (restored + new)
    "Future Power & Energy Wave",
    "Quantum Computing Wave",
    "Clean Transit-Infrastructure Wave",
    
    # Income & Fixed Income Waves (restored)
    "Income Wave",
    
    # New Equity Waves (5 new thematic waves per requirement)
    "US MegaCap Core Wave",
    "AI & Cloud MegaCap Wave",
    "Next-Gen Compute & Semis Wave",
    "US Small-Cap Disruptors Wave",
    "Demas Fund Wave"
}

# Single Crypto Wave (sole crypto representation)
CRYPTO_INCOME_WAVE = "Crypto Income Wave"

# Crypto Selection Engine (CSE) - Top 1-200 cryptos
# Represented as a virtual wave for analytics and selection
CSE_WAVE_NAME = "Crypto Selection Engine (CSE Top 200)"

# ============================================================================
# INDEX REFERENCE WAVES - Russell 3000 Integration
# ============================================================================

# Index reference waves with fallback mappings for attributes
# Russell 3000 Wave is an index reference wave tracked via IWV ETF
INDEX_REFERENCE_WAVES = {
    "Russell 3000 Wave": {
        "ticker": "IWV",
        "benchmark": "IWV",
        "description": "iShares Russell 3000 ETF - Broad US equity market"
    }
}


# ============================================================================
# WAVE UNIVERSE LOADER - Deduplication and Dynamic Reload
# ============================================================================

def normalize_wave_name(name: str) -> str:
    """
    Clean wave names by trimming whitespace and casefolding.
    
    Args:
        name: Wave name to normalize
        
    Returns:
        Normalized wave name (trimmed and lowercased)
    """
    if not isinstance(name, str):
        return ""
    return name.strip().casefold()


def dedupe_waves(names: list[str]) -> tuple[list[str], list[str]]:
    """
    Deduplicate wave names while preserving original order.
    
    Removes duplicates based on normalized names (case-insensitive, trimmed).
    
    Args:
        names: List of wave names (may contain duplicates)
        
    Returns:
        Tuple of (deduplicated_names, removed_duplicates)
        - deduplicated_names: List of unique wave names in original order
        - removed_duplicates: List of duplicate names that were removed
    """
    if not names:
        return [], []
    
    seen_normalized = set()
    deduplicated = []
    removed = []
    
    for name in names:
        normalized = normalize_wave_name(name)
        if normalized and normalized not in seen_normalized:
            seen_normalized.add(normalized)
            deduplicated.append(name)
        elif normalized:
            removed.append(name)
    
    return deduplicated, removed

CACHE_BUSTER = "RESET_2026_01_15_v2"


def _normalize_wave_universe(universe):
    """
    Normalize wave universe dictionary to ensure all expected keys are present.
    
    This defensive function guarantees schema consistency by providing defaults
    for any missing keys, preventing potential runtime errors from incomplete data.
    
    Args:
        universe: Dictionary that may or may not have all expected keys
        
    Returns:
        Normalized dictionary with guaranteed schema:
        - waves: list (default: [])
        - removed_duplicates: list (default: [])
        - source: str (default: "unknown")
        - timestamp: str (preserved or default: "")
        - enabled_flags: dict (preserved or default: {})
    """
    if not isinstance(universe, dict):
        universe = {}
    
    # Use .get() with defaults, and handle None values explicitly
    waves = universe.get("waves", [])
    removed_duplicates = universe.get("removed_duplicates", [])
    source = universe.get("source", "unknown")
    timestamp = universe.get("timestamp", "")
    enabled_flags = universe.get("enabled_flags", {})
    
    return {
        "waves": waves if waves is not None else [],
        "removed_duplicates": removed_duplicates if removed_duplicates is not None else [],
        "source": source if source is not None else "unknown",
        "timestamp": timestamp if timestamp is not None else "",
        "enabled_flags": enabled_flags if enabled_flags is not None else {}
    }


@st.cache_data(ttl=15)
def get_canonical_wave_universe(
    force_reload: bool = False,
    _wave_universe_version: int = 0,
    _cache_buster: str = CACHE_BUSTER,
):
    """
    Fetch, deduplicate, and return canonical wave universe data.
    
    UPDATED: Now uses waves_engine.get_all_waves_universe() as the SINGLE SOURCE OF TRUTH.
    All 28 waves from the registry will be included - NO SILENT FILTERING.
    
    Uses wave list from waves_engine or falls back to static list.
    Caches result in session state for performance.
    
    Args:
        force_reload: If True, bypass cache and rebuild universe
        _wave_universe_version: Version counter for cache invalidation (prefixed with _ to ignore in hash)
        
    Returns:
        Dictionary with:
        - waves: Deduplicated list of wave names (28 waves from registry)
        - removed_duplicates: List of removed duplicates
        - source: Data source ("registry", "engine", "fallback")
        - timestamp: Current timestamp for tracking
        - enabled_flags: Dictionary mapping wave names to enabled status (default True)
    """
    from datetime import datetime
    
    # Check cache unless force reload
    if not force_reload and "wave_universe" in st.session_state:
        cached = st.session_state["wave_universe"]
        # Normalize cached data to ensure schema consistency
        return _normalize_wave_universe(cached)
    
    # Build wave universe
    wave_list = []
    source = "fallback"
    
    try:
        # PRIORITY 1: Use get_all_waves_universe() from waves_engine (single source of truth)
        if WAVES_ENGINE_AVAILABLE:
            try:
                from waves_engine import get_all_waves_universe
                universe_data = get_all_waves_universe()
                wave_list = universe_data['waves']
                source = universe_data['source']  # Should be "wave_registry"
            except (ImportError, AttributeError):
                # Fall through to legacy methods
                pass
        
        # PRIORITY 2: Try to get waves from engine_get_all_waves (legacy)
        if not wave_list and WAVES_ENGINE_AVAILABLE and engine_get_all_waves is not None:
            wave_list = engine_get_all_waves()
            source = "engine"
        elif not wave_list and WAVE_WEIGHTS:
            # Fallback: directly access WAVE_WEIGHTS
            wave_list = sorted(list(WAVE_WEIGHTS.keys()))
            source = "engine"
    except Exception as e:
        import traceback
        print(f"Warning: Error getting waves from engine: {e}")
        traceback.print_exc()
    
    # If engine unavailable, use fallback static list (should never happen)
    if not wave_list:
        # Fallback: combine INCLUDED_EQUITY_WAVES with other known waves
        wave_list = list(INCLUDED_EQUITY_WAVES)
        wave_list.append(CRYPTO_INCOME_WAVE)
        wave_list.append(CSE_WAVE_NAME)
        source = "fallback"
    
    # Deduplicate
    deduplicated_waves, removed_duplicates = dedupe_waves(wave_list)
    
    # Load wave_config.csv if available to get enabled flags
    enabled_flags = {}
    wave_config_path = os.path.join(os.path.dirname(__file__), 'wave_config.csv')
    if os.path.exists(wave_config_path):
        try:
            config_df = pd.read_csv(wave_config_path)
            # Default enabled to True if column doesn't exist or value is missing
            if 'enabled' in config_df.columns:
                for _, row in config_df.iterrows():
                    wave_name = row.get('Wave', '')
                    enabled_value = row.get('enabled', True)
                    # Handle NaN or empty values - default to True
                    if pd.isna(enabled_value) or enabled_value == '':
                        enabled_value = True
                    enabled_flags[wave_name] = bool(enabled_value)
        except Exception:
            pass  # If CSV reading fails, continue with defaults
    
    # Initialize enabled flags from session state if available (for runtime changes)
    session_enabled = st.session_state.get("wave_enabled_flags", {})
    
    # Build final enabled flags - default all waves to enabled=True
    final_enabled = {}
    for wave in deduplicated_waves:
        # Priority: session state > CSV config > default True
        if wave in session_enabled:
            final_enabled[wave] = session_enabled[wave]
        elif wave in enabled_flags:
            final_enabled[wave] = enabled_flags[wave]
        else:
            final_enabled[wave] = True  # Default to enabled
    
    # Build universe dictionary
    universe = {
        "waves": deduplicated_waves,
        "removed_duplicates": removed_duplicates,
        "source": source,
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "enabled_flags": final_enabled
    }
    
    # Cache in session state
    st.session_state["wave_universe"] = universe
    
    return universe


def get_wave_universe():
    """
    Centralized Wave Universe Function - Single Source of Truth.
    
    This function serves as the canonical source for all wave lists and counts
    throughout the application. It ensures:
    - Deduplication of wave names
    - "Russell 3000 Wave" is included exactly once
    - Consistent wave lists across all tabs and views
    
    Returns:
        list: Deduplicated, sorted list of all wave names
    """
    try:
        # Get wave universe version from session state
        wave_universe_version = st.session_state.get("wave_universe_version", 1)
        
        # Use canonical wave universe (which already deduplicates and ensures Russell 3000 Wave)
        universe = get_canonical_wave_universe(force_reload=False, _wave_universe_version=wave_universe_version)
        
        # Extract waves list
        waves = universe.get("waves", [])
        
        # Return deduplicated, sorted list
        return sorted(set(waves))
    except Exception:
        # Fallback: return empty list
        return []


def validate_wave_data(wave_name: str, mode: str, days: int, df=None) -> tuple[bool, list[str]]:
    """
    Validate wave data availability and quality.
    
    This function checks if wave data is available and suitable for display.
    It returns detailed reasons when data is unavailable to provide user clarity.
    
    Args:
        wave_name: Name of the wave to validate
        mode: Mode/strategy being used (e.g., "Standard", "Aggressive")
        days: Number of days of data required
        df: Optional pre-loaded DataFrame to validate (if None, will load)
        
    Returns:
        tuple: (ok: bool, reasons: list[str])
            - ok: True if data is valid and available, False otherwise
            - reasons: List of validation failure reasons (empty if ok=True)
    """
    reasons = []
    
    try:
        # Load wave data if not provided
        if df is None:
            wave_universe_version = st.session_state.get("wave_universe_version", 1)
            df = get_wave_data_filtered(wave_name=wave_name, days=days, _wave_universe_version=wave_universe_version)
        
        # Check if data exists
        if df is None:
            reasons.append(f"No historical data available for '{wave_name}'")
            return (False, reasons)
        
        # Check if data is empty
        if len(df) == 0:
            reasons.append(f"Historical data for '{wave_name}' is empty")
            return (False, reasons)
        
        # Check minimum data points
        if len(df) < 5:
            reasons.append(f"Insufficient data points: {len(df)} rows (minimum 5 required)")
        
        # Check date range coverage
        if 'date' in df.columns:
            data_days = (df['date'].max() - df['date'].min()).days
            if data_days < days * 0.5:  # At least 50% coverage
                reasons.append(f"Data coverage insufficient: {data_days} days available, {days} days requested")
        
        # Check required columns
        required_columns = ['date']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            reasons.append(f"Missing required columns: {', '.join(missing_columns)}")
        
        # Check for portfolio_return or benchmark_return for alpha calculations
        if 'portfolio_return' not in df.columns and 'benchmark_return' not in df.columns:
            reasons.append("Missing return data (portfolio_return or benchmark_return)")
        
        # If we have reasons, validation failed
        if reasons:
            return (False, reasons)
        
        # All checks passed
        return (True, [])
        
    except Exception as e:
        reasons.append(f"Validation error: {str(e)}")
        return (False, reasons)


# ============================================================================
# SECTION 2: UTILITY FUNCTIONS
# ============================================================================

def k(tab, name, wave_id=None, mode=None):
    """
    Unique key factory for Streamlit widgets.
    Creates unique keys to avoid duplicate key errors across tabs and waves.
    
    Args:
        tab: Tab name or section identifier (e.g., "Diagnostics", "Overview")
        name: Widget name (e.g., "wave_selector", "timeframe")
        wave_id: Optional wave identifier for wave-specific widgets
        mode: Optional mode identifier for mode-specific widgets
        
    Returns:
        Unique key string in format: "tab__name" or "tab__wave__name" etc.
    
    Examples:
        k("Diagnostics", "wave_selector") -> "Diagnostics__wave_selector"
        k("Overview", "timeframe", wave_id="SP500") -> "Overview__SP500__timeframe"
        k("Console", "chart", mode="Standard") -> "Console__Standard__chart"
    """
    parts = [tab, name]
    if wave_id:
        parts.insert(1, wave_id)
    if mode:
        parts.insert(1, mode)
    return "__".join(parts)


def get_git_commit_hash():
    """Get the current git commit hash, return 'unknown' if unavailable."""
    try:
        result = subprocess.run(
            ['git', 'rev-parse', '--short', 'HEAD'],
            capture_output=True,
            text=True,
            timeout=5
        )
        if result.returncode == 0:
            return result.stdout.strip()
    except Exception:
        pass
    return "unknown"


def get_git_branch_name():
    """Get the current git branch name, return 'unknown' if unavailable."""
    try:
        result = subprocess.run(
            ['git', 'rev-parse', '--abbrev-ref', 'HEAD'],
            capture_output=True,
            text=True,
            timeout=5
        )
        if result.returncode == 0:
            return result.stdout.strip()
    except Exception:
        pass
    return "unknown"


def create_last_known_good_backup():
    """
    Create a backup of the current app.py to backup/app_last_good.py.
    This is called on successful app startup to preserve a working version.
    
    Returns:
        bool: True if backup succeeded, False otherwise
    """
    try:
        # Create backup directory if it doesn't exist
        backup_dir = os.path.join(os.path.dirname(__file__), 'backup')
        os.makedirs(backup_dir, exist_ok=True)
        
        # Source file
        source_file = os.path.join(os.path.dirname(__file__), 'app.py')
        
        # Backup file with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = os.path.join(backup_dir, f'app_last_good_{timestamp}.py')
        
        # Also keep a generic "latest" backup
        latest_backup_file = os.path.join(backup_dir, 'app_last_good.py')
        
        # Copy the file
        import shutil
        shutil.copy2(source_file, backup_file)
        shutil.copy2(source_file, latest_backup_file)
        
        return True
    except Exception as e:
        # Silent fail - don't disrupt app startup
        return False


def log_crash_to_file(exception: Exception, context: str = ""):
    """
    Log crash information to logs/crash_log.txt.
    
    Args:
        exception: The exception that was caught
        context: Additional context about where the crash occurred
    """
    try:
        # Create logs directory if it doesn't exist
        logs_dir = os.path.join(os.path.dirname(__file__), 'logs')
        os.makedirs(logs_dir, exist_ok=True)
        
        # Log file path
        log_file = os.path.join(logs_dir, 'crash_log.txt')
        
        # Format log entry
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"""
{'='*80}
CRASH LOG ENTRY
Timestamp: {timestamp}
Context: {context}
Exception Type: {type(exception).__name__}
Exception Message: {str(exception)}
{'='*80}
Traceback:
{traceback.format_exc()}
{'='*80}

"""
        
        # Append to log file
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(log_entry)
    except Exception:
        # Silent fail - don't compound errors
        pass


def get_deploy_timestamp():
    """Get the current timestamp as deploy timestamp."""
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S UTC")


def render_html_safe(html_content: str, height: int = None, scrolling: bool = False):
    """
    Safely render HTML content based on RENDER_RICH_HTML flag.
    
    When RENDER_RICH_HTML is True (or session state override):
        Uses st.html for direct HTML rendering (recommended)
    When RENDER_RICH_HTML is False:
        Falls back to st.markdown with unsafe_allow_html=True
    
    Args:
        html_content: The HTML content to render
        height: Optional height in pixels (not used with st.html, kept for compatibility)
        scrolling: Whether to enable scrolling (not used with st.html, kept for compatibility)
    """
    # Check session state first, then fall back to global flag
    use_rich_html = st.session_state.get("render_rich_html_enabled", RENDER_RICH_HTML)
    
    if use_rich_html:
        try:
            # Use st.html for modern, direct HTML rendering without iframe
            st.html(html_content)
        except Exception:
            # Fallback to markdown if st.html fails
            st.markdown(html_content, unsafe_allow_html=True)
    else:
        # Use plain markdown rendering
        st.markdown(html_content, unsafe_allow_html=True)


def safe_plotly_chart(fig, use_container_width=True, key=None, placeholder_msg="üìä Chart unavailable"):
    """
    Safely render a plotly chart with error handling.
    If rendering fails, shows a placeholder message instead of crashing.
    
    Args:
        fig: Plotly figure object
        use_container_width: Whether to use container width
        key: Unique key for the chart
        placeholder_msg: Message to show if chart fails to render
    """
    try:
        if fig is None:
            st.info(placeholder_msg)
            return
        st.plotly_chart(fig, use_container_width=use_container_width, key=key)
    except Exception as e:
        st.warning(f"‚ö†Ô∏è {placeholder_msg}")
        # Log error for debugging but don't crash
        if st.session_state.get("debug_mode", False):
            st.caption(f"_Debug: {str(e)}_")


def safe_image(image_path, caption=None, use_column_width=None, placeholder_msg="üñºÔ∏è Image unavailable"):
    """
    Safely render an image with error handling.
    If the file doesn't exist or rendering fails, shows a placeholder instead of crashing.
    
    Args:
        image_path: Path to image file
        caption: Optional image caption
        use_column_width: Whether to use column width
        placeholder_msg: Message to show if image fails to render
    """
    try:
        if not os.path.exists(image_path):
            st.info(placeholder_msg)
            return
        st.image(image_path, caption=caption, use_column_width=use_column_width)
    except Exception as e:
        st.warning(f"‚ö†Ô∏è {placeholder_msg}")
        # Log error for debugging but don't crash
        if st.session_state.get("debug_mode", False):
            st.caption(f"_Debug: {str(e)}_")


def safe_audio(audio_path, format="audio/mp3", start_time=0, placeholder_msg="üîä Audio unavailable"):
    """
    Safely render an audio file with error handling.
    If the file doesn't exist or rendering fails, shows a placeholder instead of crashing.
    
    Args:
        audio_path: Path to audio file
        format: Audio format (default: "audio/mp3")
        start_time: Start time in seconds (default: 0)
        placeholder_msg: Message to show if audio fails to render
    """
    try:
        if not os.path.exists(audio_path):
            st.info(placeholder_msg)
            return
        st.audio(audio_path, format=format, start_time=start_time)
    except Exception as e:
        st.warning(f"‚ö†Ô∏è {placeholder_msg}")
        # Log error for debugging but don't crash
        if st.session_state.get("debug_mode", False):
            st.caption(f"_Debug: {str(e)}_")


def safe_video(video_path, format="video/mp4", start_time=0, placeholder_msg="üé• Video unavailable"):
    """
    Safely render a video file with error handling.
    If the file doesn't exist or rendering fails, shows a placeholder instead of crashing.
    
    Args:
        video_path: Path to video file
        format: Video format (default: "video/mp4")
        start_time: Start time in seconds (default: 0)
        placeholder_msg: Message to show if video fails to render
    """
    try:
        if not os.path.exists(video_path):
            st.info(placeholder_msg)
            return
        st.video(video_path, format=format, start_time=start_time)
    except Exception as e:
        st.warning(f"‚ö†Ô∏è {placeholder_msg}")
        # Log error for debugging but don't crash
        if st.session_state.get("debug_mode", False):
            st.caption(f"_Debug: {str(e)}_")


def safe_component(component_name, render_func, *args, show_error=True, **kwargs):
    """
    Safely execute a component rendering function with error handling.
    If the component fails, shows a minimal indicator instead of crashing the entire app.
    
    Args:
        component_name: Name of the component for error messages
        render_func: The function to execute
        *args: Positional arguments to pass to render_func
        show_error: Whether to show error message on failure (default: True)
        **kwargs: Keyword arguments to pass to render_func
        
    Returns:
        The result of render_func, or None if it fails
    """
    try:
        return render_func(*args, **kwargs)
    except Exception as e:
        # Store error for diagnostics (silent logging)
        if "component_errors" not in st.session_state:
            st.session_state.component_errors = []
        
        error_entry = {
            "component": component_name,
            "error": str(e),
            "traceback": traceback.format_exc(),
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        st.session_state.component_errors.append(error_entry)
        
        # Keep only last 20 errors to avoid memory bloat
        if len(st.session_state.component_errors) > 20:
            st.session_state.component_errors = st.session_state.component_errors[-20:]
        
        # Show minimal UI based on debug_mode
        if show_error:
            debug_mode = st.session_state.get("debug_mode", False)
            
            if debug_mode:
                # Debug mode ON: Show detailed error with expander
                st.warning(f"‚ö†Ô∏è {component_name} temporarily unavailable")
                with st.expander(f"üêõ Debug: {component_name} error details", expanded=False):
                    st.error(f"**Error:** {str(e)}")
                    st.code(traceback.format_exc(), language="python")
            else:
                # Debug mode OFF: Show small pill only (silent fallback)
                st.markdown(f"""
                    <div style="
                        display: inline-block;
                        background: rgba(255, 193, 7, 0.1);
                        border: 1px solid rgba(255, 193, 7, 0.3);
                        border-radius: 12px;
                        padding: 4px 12px;
                        margin: 8px 0;
                        font-size: 12px;
                        color: #ffc107;
                    ">
                        ‚ö†Ô∏è {component_name} unavailable
                    </div>
                """, unsafe_allow_html=True)
                st.caption("üí° Enable Debug Mode in sidebar for details")
        
        return None


def should_allow_compute(operation_name: str, force: bool = False) -> tuple[bool, str]:
    """
    Check if a compute-intensive operation should be allowed to run.
    
    This implements a global compute lock to prevent duplicate heavy operations
    during the same session or run.
    
    Args:
        operation_name: Unique name for the operation (e.g., "fetch_prices", "build_snapshot")
        force: If True, bypass the lock check (used for explicit user actions)
        
    Returns:
        Tuple of (should_run: bool, reason: str)
    """
    # Force always wins - check this FIRST
    if force:
        return True, "Force flag enabled - bypassing all locks"
    
    # Check global compute lock
    if st.session_state.get("compute_lock", False):
        reason = st.session_state.get("compute_lock_reason", "Global compute lock is set")
        return False, reason
    
    # Check if this operation was already done this session
    operations_done = st.session_state.get("compute_operations_done", set())
    if operation_name in operations_done:
        return False, f"Operation '{operation_name}' already completed this session"
    
    # Operation is allowed
    return True, "Operation allowed"


def mark_compute_done(operation_name: str, success: bool = True):
    """
    Mark a compute operation as complete.
    
    Args:
        operation_name: Name of the operation that completed
        success: Whether the operation succeeded
    """
    if "compute_operations_done" not in st.session_state:
        st.session_state.compute_operations_done = set()
    
    st.session_state.compute_operations_done.add(operation_name)
    
    # Set compute lock if this was a heavy operation and succeeded
    if success and operation_name in ["fetch_prices", "build_snapshot", "warm_cache"]:
        st.session_state.compute_lock = True
        st.session_state.compute_lock_reason = f"Locked after '{operation_name}' completed successfully"


def trigger_rerun(trigger_name: str):
    """
    Trigger a Streamlit rerun with a labeled trigger for diagnostics.
    
    Args:
        trigger_name: Name/description of what triggered the rerun (e.g., "rebuild_snapshot", "change_wave")
    """
    # Update last_trigger in session state
    st.session_state.last_trigger = trigger_name
    
    # Set flag to prevent overwriting by detection logic
    st.session_state.trigger_set_by_rerun = True
    
    # Mark user interaction detected
    st.session_state.user_interaction_detected = True
    
    # Trigger the rerun
    st.rerun()


def get_cache_file_timestamp(file_path):
    """
    Get the modification timestamp of a file for cache-busting.
    
    Args:
        file_path: Path to the file
        
    Returns:
        float: File modification timestamp, or 0.0 if file doesn't exist
    """
    try:
        if os.path.exists(file_path):
            return os.path.getmtime(file_path)
        return 0.0
    except Exception:
        return 0.0


@st.cache_resource(show_spinner=False)
def get_cached_price_book_internal(_cache_buster=None):
    """
    Internal function to get cached PRICE_BOOK.
    
    This function wraps helpers.price_book.get_price_book() with Streamlit's
    @st.cache_resource decorator to ensure it loads only once per session,
    preventing repeated "PRICE_BOOK: Loading canonical price data" log spam.
    
    Uses cache-busting via file modification timestamp to detect when the
    underlying cache file has been updated.
    
    Args:
        _cache_buster: File modification timestamp (prefixed with _ to exclude from hash)
    
    Returns:
        DataFrame: Cached price book (index=dates, columns=tickers)
    """
    # Import here to avoid circular dependencies
    if PRICE_BOOK_CONSTANTS_AVAILABLE and get_price_book is not None:
        # Log only on cache miss (first load)
        logger.info("PRICE_BOOK loaded (cached) - this message appears only on cache miss")
        return get_price_book(active_tickers=None)
    else:
        # Fallback if price_book module not available
        logger.warning("PRICE_BOOK unavailable - price_book module not loaded")
        return pd.DataFrame()


def get_cached_price_book():
    """
    Get cached PRICE_BOOK with automatic cache-busting.
    
    This function automatically detects changes to the underlying price cache file
    and invalidates the Streamlit cache when the file is updated.
    
    Returns:
        DataFrame: Cached price book (index=dates, columns=tickers)
    """
    # Get cache file timestamp for cache-busting
    cache_timestamp = get_cache_file_timestamp(CANONICAL_CACHE_PATH)
    return get_cached_price_book_internal(_cache_buster=cache_timestamp)


def calculate_wavescore(wave_data):
    """
    Calculate WaveScore for a wave based on cumulative alpha over 30 days.
    Returns a score between 0 and 100.
    
    WaveScore formula:
    - Base score: 50
    - Add points for cumulative alpha (1000x multiplier)
    - Adjust for consistency (reduce score for high volatility)
    - Clamp to 0-100 range
    """
    try:
        if wave_data is None or len(wave_data) == 0:
            return 0
        
        if 'alpha' not in wave_data.columns:
            return 0
        
        # Calculate cumulative alpha
        cumulative_alpha = wave_data['alpha'].sum()
        
        # Base score calculation
        base_score = (cumulative_alpha * 1000) + 50
        
        # Consistency adjustment: penalize high volatility
        alpha_std = wave_data['alpha'].std()
        if alpha_std > 0:
            # Reduce score if volatility is high relative to returns
            consistency_penalty = min(10, alpha_std * 200)
            base_score -= consistency_penalty
        
        # Normalize to 0-100 range
        wavescore = min(100, max(0, base_score))
        
        return wavescore
    except Exception:
        return 0


def calculate_wave_correlation(wave1_data, wave2_data):
    """
    Calculate correlation between two waves based on their returns.
    Returns correlation coefficient or None if unavailable.
    """
    try:
        if wave1_data is None or wave2_data is None:
            return None
        
        if len(wave1_data) == 0 or len(wave2_data) == 0:
            return None
        
        if 'date' not in wave1_data.columns or 'date' not in wave2_data.columns:
            return None
        
        if 'portfolio_return' not in wave1_data.columns or 'portfolio_return' not in wave2_data.columns:
            return None
        
        # Merge on date to get overlapping periods
        wave1_returns = wave1_data[['date', 'portfolio_return']].rename(columns={'portfolio_return': 'return1'})
        wave2_returns = wave2_data[['date', 'portfolio_return']].rename(columns={'portfolio_return': 'return2'})
        
        merged = pd.merge(wave1_returns, wave2_returns, on='date', how='inner')
        
        if len(merged) < 2:
            return None
        
        correlation = merged['return1'].corr(merged['return2'])
        
        return correlation
        
    except Exception:
        return None


def calculate_wave_metrics(wave_data):
    """
    Calculate comprehensive metrics for a wave.
    Returns a dictionary with all calculated metrics.
    """
    metrics = {
        'cumulative_return': 'N/A',
        'cumulative_alpha': 'N/A',
        'volatility': 'N/A',
        'max_drawdown': 'N/A',
        'wavescore': 'N/A',
        'sharpe_ratio': 'N/A',
        'win_rate': 'N/A'
    }
    
    try:
        if wave_data is None or len(wave_data) == 0:
            return metrics
        
        # Calculate alpha
        if 'portfolio_return' in wave_data.columns and 'benchmark_return' in wave_data.columns:
            wave_data = wave_data.copy()
            wave_data['alpha'] = wave_data['portfolio_return'] - wave_data['benchmark_return']
            
            # Cumulative return
            cumulative_return = wave_data['portfolio_return'].sum()
            metrics['cumulative_return'] = cumulative_return
            
            # Cumulative alpha
            cumulative_alpha = wave_data['alpha'].sum()
            metrics['cumulative_alpha'] = cumulative_alpha
            
            # Volatility
            volatility = wave_data['portfolio_return'].std()
            metrics['volatility'] = volatility
            
            # WaveScore
            wavescore = calculate_wavescore(wave_data)
            metrics['wavescore'] = wavescore
            
            # Sharpe ratio
            avg_return = wave_data['portfolio_return'].mean()
            if volatility > 0:
                sharpe = (avg_return / volatility) * np.sqrt(252)
                metrics['sharpe_ratio'] = sharpe
            
            # Win rate
            positive_days = len(wave_data[wave_data['alpha'] > 0])
            total_days = len(wave_data)
            if total_days > 0:
                metrics['win_rate'] = positive_days / total_days
            
            # Max drawdown
            cumulative_returns = (1 + wave_data['portfolio_return']).cumprod()
            running_max = cumulative_returns.cummax()
            drawdown = (cumulative_returns - running_max) / running_max
            metrics['max_drawdown'] = drawdown.min()
    
    except Exception:
        pass
    
    return metrics


def determine_winner(wave1_metrics, wave2_metrics):
    """
    Determine which wave is the winner based on WaveScore.
    Returns a tuple: (winner_name, notes)
    """
    try:
        wave1_score = wave1_metrics.get('wavescore', 0)
        wave2_score = wave2_metrics.get('wavescore', 0)
        
        if wave1_score == 'N/A' or wave2_score == 'N/A':
            return None, "Insufficient data to determine winner"
        
        if wave1_score > wave2_score:
            winner = wave1_metrics['name']
            margin = wave1_score - wave2_score
            notes = f"{winner} leads by {margin:.1f} WaveScore points"
        elif wave2_score > wave1_score:
            winner = wave2_metrics['name']
            margin = wave2_score - wave1_score
            notes = f"{winner} leads by {margin:.1f} WaveScore points"
        else:
            winner = "TIE"
            notes = "Both waves have identical WaveScore"
        
        return winner, notes
        
    except Exception:
        return None, "Error determining winner"


# ============================================================================
# SECTION 3: SAFE DATA-LOADING HELPERS
# ============================================================================

@st.cache_data(ttl=15)
def safe_load_wave_history(_wave_universe_version=1):
    """
    Safely load wave history data with comprehensive error handling.
    Returns DataFrame or None if unavailable.
    
    Applies wave name normalization:
    - Strips leading/trailing whitespace
    - Collapses multiple spaces to single space
    - Creates normalized_wave column for matching
    
    Args:
        _wave_universe_version: Version counter for cache invalidation (prefixed with _ to ignore in hash)
    """
    try:
        wave_history_path = os.path.join(os.path.dirname(__file__), 'wave_history.csv')
        
        if not os.path.exists(wave_history_path):
            return None
        
        df = pd.read_csv(wave_history_path)
        
        if df is None or len(df) == 0:
            return None
        
        # Validate required columns
        if 'date' not in df.columns:
            return None
        
        # Convert date to datetime
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        
        # Remove rows with invalid dates
        df = df.dropna(subset=['date'])
        
        if len(df) == 0:
            return None
        
        # Create 'wave' column if it doesn't exist
        # This handles the case where CSV has 'display_name' or 'wave_id' instead
        if 'wave' not in df.columns:
            if 'display_name' in df.columns:
                # Use display_name as the wave column (user-friendly names)
                df['wave'] = df['display_name']
            elif 'wave_id' in df.columns:
                # Fallback to wave_id if display_name doesn't exist
                df['wave'] = df['wave_id']
        
        # Normalize wave names if wave column exists
        if 'wave' in df.columns:
            # Strip whitespace and collapse multiple spaces
            df['wave'] = df['wave'].str.strip()
            df['wave'] = df['wave'].str.replace(r'\s+', ' ', regex=True)
            
            # Create normalized_wave column for case-insensitive matching
            df['normalized_wave'] = df['wave'].str.lower().str.strip()
        
        return df
        
    except Exception:
        return None


def get_latest_data_timestamp():
    """Get the latest available 'as of' data timestamp from wave_history.csv."""
    try:
        df = safe_load_wave_history()
        if df is not None and 'date' in df.columns and len(df) > 0:
            latest_date = df['date'].max()
            return latest_date.strftime("%Y-%m-%d") if pd.notna(latest_date) else "unknown"
    except Exception:
        pass
    return "unknown"


def get_all_wave_names():
    """
    Get all wave names from the waves engine (single source of truth).
    
    This function sources wave names directly from WAVE_WEIGHTS in waves_engine.py,
    ensuring that the UI always reflects the current wave definitions in the engine.
    
    Returns:
        sorted list of wave names from WAVE_WEIGHTS, or empty list if engine unavailable
    """
    try:
        if WAVES_ENGINE_AVAILABLE and engine_get_all_waves is not None:
            # Use the waves_engine.get_all_waves() function (sources from WAVE_WEIGHTS)
            return engine_get_all_waves()
        elif WAVE_WEIGHTS:
            # Fallback: directly access WAVE_WEIGHTS if available
            return sorted(list(WAVE_WEIGHTS.keys()))
        else:
            # No engine available, return empty list
            return []
    except Exception:
        # On any error, return empty list
        return []


def get_available_waves(_wave_universe_version=1):
    """
    Get list of available waves using the centralized Wave Universe (Data Backbone V1).
    
    This function is the primary interface for getting wave lists in dropdowns and UI.
    Sources from get_wave_universe() which provides deduplication and caching.
    
    Args:
        _wave_universe_version: Version counter for cache invalidation (prefixed with _ to ignore in hash)
    
    Returns sorted list of wave names or empty list if unavailable.
    """
    try:
        # Use centralized Wave Universe function (Data Backbone V1)
        waves = get_wave_universe()
        return waves  # Already sorted and deduplicated
    except Exception:
        # Fallback to empty list
        return []


def get_wave_universe_all(_wave_universe_version=1):
    """
    Get the entire deduplicated wave universe (Data Backbone V1).
    
    This is an alias for get_wave_universe() to maintain backward compatibility.
    All waves are returned regardless of data availability.
    
    Args:
        _wave_universe_version: Version counter for cache invalidation (prefixed with _ to ignore in hash)
    
    Returns:
        Sorted list of all wave names
    """
    return get_wave_universe()


def get_wave_universe_with_data(period_days=30, _wave_universe_version=1):
    """
    Get waves that have historical data available for the selected lookback period.
    
    Args:
        period_days: Lookback period in days (default: 30)
        _wave_universe_version: Version counter for cache invalidation (prefixed with _ to ignore in hash)
    
    Returns:
        Sorted list of wave names that have data in the specified period
    """
    try:
        wave_universe_version = st.session_state.get("wave_universe_version", 1)
        df = safe_load_wave_history(_wave_universe_version=wave_universe_version)
        
        if df is None or 'wave' not in df.columns:
            return []
        
        # Filter to period
        if period_days and period_days > 0 and 'date' in df.columns:
            latest_date = df['date'].max()
            cutoff_date = latest_date - timedelta(days=period_days)
            df = df[df['date'] >= cutoff_date]
        
        # Get unique waves with data
        waves_with_data = sorted(df['wave'].unique().tolist())
        return waves_with_data
        
    except Exception:
        return []


def get_wave_status_map(_wave_universe_version=1):
    """
    Get status for each wave in the universe.
    
    Returns:
        Dictionary mapping wave names to status:
        - "Ready": Full analytics data available (recent 7 days)
        - "Degraded": Partial data or stale data
        - "Degraded (Rate Limited)": API errors or rate limiting detected
        - "Missing Inputs": No data available
    
    This does NOT affect whether a wave is Active - waves remain Active
    based on their enabled flag regardless of data status.
    """
    status_map = {}
    
    try:
        # Get all waves from universe
        wave_universe_version = st.session_state.get("wave_universe_version", 1)
        universe = get_canonical_wave_universe(force_reload=False, _wave_universe_version=wave_universe_version)
        all_waves = universe.get("waves", [])
        
        # Get wave history
        wave_history = safe_load_wave_history(_wave_universe_version=wave_universe_version)
        
        if wave_history is None or 'wave' not in wave_history.columns:
            # No data available - all waves are Missing Inputs
            for wave in all_waves:
                status_map[wave] = "Missing Inputs"
            return status_map
        
        # Get latest date
        latest_date = wave_history['date'].max()
        cutoff_date = latest_date - timedelta(days=7)
        
        # Get waves with recent data
        recent_data = wave_history[wave_history['date'] >= cutoff_date]
        waves_with_recent_data = set(recent_data['wave'].unique())
        
        # Get waves with any data (older than 7 days)
        all_waves_with_data = set(wave_history['wave'].unique())
        
        # Check for rate-limited or error status in session state
        rate_limited_waves = st.session_state.get("rate_limited_waves", set())
        
        # Classify each wave
        for wave in all_waves:
            if wave in rate_limited_waves:
                status_map[wave] = "Degraded (Rate Limited)"
            elif wave in waves_with_recent_data:
                status_map[wave] = "Ready"
            elif wave in all_waves_with_data:
                status_map[wave] = "Degraded"
            else:
                status_map[wave] = "Missing Inputs"
        
    except Exception:
        # On error, default all to Missing Inputs
        try:
            all_waves = get_wave_universe()
            for wave in all_waves:
                status_map[wave] = "Missing Inputs"
        except:
            pass
    
    return status_map


def is_wave_data_ready(wave_id: str, wave_history_df=None, wave_universe=None, price_df=None, use_analytics_pipeline=True) -> tuple[bool, str, str]:
    """
    Check if a wave is data-ready with GRADED READINESS MODEL.
    
    CRITICAL UPDATE (28/28 Rendering Enforcement): This function ALWAYS returns True to ensure 
    all 28 waves are rendered. Waves with missing data are marked as "Degraded" or "Unavailable" 
    but still return is_ready=True to prevent rendering blockers.
    
    BREAKING CHANGE: Prior to this update, this function could return False for unavailable waves,
    which would hide them from the UI. Now all waves are always visible with diagnostic information.
    
    Downstream Impact:
    - UI components can safely assume all waves are included
    - Rendering logic no longer needs to check is_ready before displaying waves
    - Analytics should check the status field to determine which operations are allowed
    - Existing code that filters on is_ready=True will now include all waves (intended behavior)
    
    Args:
        wave_id: Wave identifier
        wave_history_df: Optional wave history DataFrame (will load if not provided)
        wave_universe: Optional wave universe dict (will load if not provided)
        price_df: Optional cached price DataFrame for NAV computation
        use_analytics_pipeline: If True (default), use analytics_pipeline.compute_data_ready_status 
                                for graded diagnostics. Set to False for legacy behavior.
    
    Returns:
        Tuple of (is_ready: bool, status: str, reason: str)
        - is_ready: ALWAYS True (no rendering blockers)
        - status: Readiness level (Full/Partial/Operational/Degraded/Unavailable)
        - reason: Detailed explanation of status
        
    Statuses (Graded):
        - "Full": All analytics available
        - "Partial": Basic analytics available, some limitations
        - "Operational": Current pricing available, minimal analytics
        - "Degraded": Limited data, but wave still visible
        - "Unavailable": Critical data missing, wave visible with diagnostics only
    """
    try:
        # PRIORITY: Use analytics pipeline for graded readiness (file-based, comprehensive)
        if use_analytics_pipeline:
            try:
                from analytics_pipeline import compute_data_ready_status
                diagnostics = compute_data_ready_status(wave_id)
                
                # Map graded readiness status - ALWAYS return True for is_ready
                readiness_status = diagnostics.get('readiness_status', 'unavailable')
                
                if readiness_status == 'full':
                    return True, "Full", diagnostics.get('details', 'All analytics available')
                elif readiness_status == 'partial':
                    return True, "Partial", diagnostics.get('details', 'Basic analytics available')
                elif readiness_status == 'operational':
                    return True, "Operational", diagnostics.get('details', 'Current pricing available')
                else:  # unavailable
                    # Return True to ensure wave is always rendered - no blockers
                    blocking = diagnostics.get('blocking_issues', [])
                    reason = '; '.join(blocking) if blocking else diagnostics.get('details', 'Data unavailable')
                    return True, "Unavailable", reason
                    
            except ImportError:
                # Fall through to legacy logic if analytics_pipeline not available
                pass
            except Exception as e:
                # Log error but fall through to legacy logic
                import traceback
                print(f"Warning: Error in analytics_pipeline.compute_data_ready_status for {wave_id}: {e}")
                traceback.print_exc()
        
        # LEGACY: Lenient runtime-based checks (original logic - kept for backward compatibility)
        # Load universe if not provided
        if wave_universe is None:
            wave_universe_version = st.session_state.get("wave_universe_version", 1)
            wave_universe = get_canonical_wave_universe(force_reload=False, _wave_universe_version=wave_universe_version)
        
        # Check 1: Wave is enabled
        enabled_flags = wave_universe.get("enabled_flags", {})
        if not enabled_flags.get(wave_id, True):
            # Return True to ensure wave is rendered - degraded but visible
            return True, "Degraded", "Wave is not enabled"
        
        # Check 2: Wave exists in registry
        all_waves = wave_universe.get("waves", [])
        if wave_id not in all_waves:
            # Return True to ensure wave is rendered with diagnostics
            return True, "Unavailable", "Wave not found in registry"
        
        # Check 3: Holdings/weights input is present (check WAVE_WEIGHTS)
        if WAVES_ENGINE_AVAILABLE and WAVE_WEIGHTS:
            if wave_id not in WAVE_WEIGHTS:
                # Return True to ensure wave is rendered with diagnostics
                return True, "Unavailable", "No holdings defined in WAVE_WEIGHTS"
        else:
            # Return True to ensure wave is rendered even if engine unavailable
            return True, "Degraded", "Wave engine not available"
        
        # Check 4: Price data availability
        # Use cached price_df if provided, otherwise try to get from session state
        if price_df is None:
            price_df = st.session_state.get("global_price_df")
        
        if price_df is None or price_df.empty:
            # Fallback to checking wave_history.csv
            if wave_history_df is None:
                wave_history_version = st.session_state.get("wave_universe_version", 1)
                wave_history_df = safe_load_wave_history(_wave_universe_version=wave_history_version)
            
            if wave_history_df is None or 'wave' not in wave_history_df.columns:
                # CHANGED: Don't fail immediately - wave might still work with fresh data
                # Return operational status but allow rendering
                return True, "Operational", "No cached data, will fetch fresh"
            
            wave_data = wave_history_df[wave_history_df['wave'] == wave_id]
            if len(wave_data) == 0:
                # CHANGED: Don't fail - wave might work with fresh data
                return True, "Operational", "No historical cache, will fetch fresh"
            
            # Check for sufficient days in wave history
            if 'date' not in wave_data.columns:
                return True, "Ready", "No date column, will fetch fresh"
            
            unique_days = wave_data['date'].nunique()
            # CHANGED: Accept even 1 day of data - partial data is better than nothing
            if unique_days < 1:
                return True, "Ready", "Will fetch fresh data"
            
            # Check for required computed columns
            required_columns = ['portfolio_return', 'benchmark_return', 'nav']
            missing_columns = [col for col in required_columns if col not in wave_data.columns]
            if missing_columns:
                # CHANGED: Don't fail - we can compute fresh
                return True, "Ready", f"Cached data incomplete, will compute fresh"
            
            return True, "Ready", f"All criteria met ({unique_days} days of data)"
        
        # NEW: Use cached price_df to verify data coverage and try NAV computation
        # CHANGED: Accept even minimal price history
        if len(price_df) < MIN_DAYS_READY:
            # Still mark as ready - we'll fetch more data as needed
            return True, "Ready", f"Limited price history ({len(price_df)} days), will supplement"
        
        # Check 5: Try to compute NAV using cached prices to verify everything works
        try:
            # Import compute_history_nav if available
            from waves_engine import compute_history_nav
            
            # Attempt NAV computation with cached prices
            result_df = compute_history_nav(
                wave_name=wave_id,
                mode="Standard",
                days=MIN_DAYS_READY,
                price_df=price_df
            )
            
            # Check if computation succeeded
            if result_df is None or result_df.empty:
                # CHANGED: Don't fail - mark as ready but will need fresh fetch
                return True, "Ready", "NAV computation needs fresh data"
            
            # Check for required columns
            required_cols = ['wave_nav', 'bm_nav']
            missing = [col for col in required_cols if col not in result_df.columns]
            if missing:
                # CHANGED: Still ready, just needs fresh computation
                return True, "Ready", f"Partial NAV data available"
            
            # Check for valid NAV values (not all NaN)
            if result_df['wave_nav'].isna().all():
                # CHANGED: Still ready, computation will retry
                return True, "Ready", "NAV will be computed fresh"
            
            # All checks passed!
            actual_days = len(result_df)
            return True, "Ready", f"All criteria met ({actual_days} days computed)"
            
        except Exception as e:
            # CHANGED: NAV computation failed but wave is still ready for rendering
            # Log the error but don't fail the wave
            error_msg = str(e)
            if "rate limit" in error_msg.lower() or "429" in error_msg:
                # Rate limited - still mark as ready, will retry later
                return True, "Ready", "Rate limited, will retry"
            else:
                # Other error - still mark as ready
                return True, "Ready", "Will compute fresh NAV"
        
    except Exception as e:
        # CHANGED: Even on exception, mark as ready - fail gracefully during rendering
        return True, "Ready", f"Will attempt fresh computation"


def get_cse_crypto_universe():
    """
    Get the Crypto Selection Engine (CSE) universe of top 1-200 cryptocurrencies.
    
    Uses Master_Stock_Sheet.csv to identify crypto assets based on sector classification.
    Returns list of crypto tickers ranked by market cap, up to 200.
    
    Returns:
        List of crypto ticker symbols or empty list if unavailable
    """
    try:
        master_sheet_path = os.path.join(os.path.dirname(__file__), 'Master_Stock_Sheet.csv')
        
        if not os.path.exists(master_sheet_path):
            return []
        
        df = pd.read_csv(master_sheet_path)
        
        if df is None or len(df) == 0:
            return []
        
        # Filter for crypto assets using sector classification
        if 'Sector' in df.columns:
            crypto_df = df[df['Sector'].isin(CRYPTO_SECTORS)].copy()
        else:
            return []
        
        # Sort by market value if available
        if 'MarketValue' in crypto_df.columns:
            crypto_df = crypto_df.sort_values('MarketValue', ascending=False)
        
        # Get top 200 (or fewer if less than 200 available)
        top_cryptos = crypto_df.head(200)
        
        # Return ticker list
        if 'Ticker' in top_cryptos.columns:
            return top_cryptos['Ticker'].dropna().tolist()
        
        return []
        
    except Exception:
        return []


def get_cse_wave_data(days=30):
    """
    Generate synthetic wave data for Crypto Selection Engine (CSE).
    
    CSE represents a market-cap weighted portfolio of top 1-200 cryptocurrencies.
    This function creates a placeholder/summary representation for CSE.
    
    Args:
        days: Number of days of data to generate
    
    Returns:
        DataFrame with CSE wave data or None if unavailable
    """
    try:
        cse_cryptos = get_cse_crypto_universe()
        
        if len(cse_cryptos) == 0:
            # Return placeholder data indicating CSE is available but data is building
            # Returning None here results in graceful "Data unavailable" display in UI
            # This is the intended behavior during CSE data building phase
            return None
        
        # Return None to indicate "Data unavailable" gracefully
        # Future enhancement: compute actual CSE portfolio performance
        # from price data and market cap weights
        return None
        
    except Exception:
        return None


def get_crypto_income_wave_data(days=30):
    """
    Get data for Crypto Income Wave (sole crypto wave representation).
    
    Falls back to "Crypto Income & Yield Wave" from wave_history.csv if available.
    
    Args:
        days: Number of days of data
        
    Returns:
        DataFrame with crypto income wave data or None if unavailable
    """
    try:
        df = safe_load_wave_history()
        
        if df is None or 'wave' not in df.columns:
            return None
        
        # Try exact match first
        crypto_data = df[df['wave'] == CRYPTO_INCOME_WAVE].copy()
        
        # Fall back to alternative names
        if len(crypto_data) == 0:
            alternative_names = ["Crypto Income & Yield Wave", "Crypto Income Wave"]
            for alt_name in alternative_names:
                crypto_data = df[df['wave'] == alt_name].copy()
                if len(crypto_data) > 0:
                    # Rename to standard name
                    crypto_data['wave'] = CRYPTO_INCOME_WAVE
                    break
        
        if len(crypto_data) == 0:
            return None
        
        # Apply date filtering
        if days is not None and days > 0:
            latest_date = crypto_data['date'].max()
            cutoff_date = latest_date - timedelta(days=days)
            crypto_data = crypto_data[crypto_data['date'] >= cutoff_date].copy()
        
        if len(crypto_data) == 0:
            return None
        
        return crypto_data
        
    except Exception:
        return None


def get_wave_data_filtered(wave_name=None, days=30, _wave_universe_version=1):
    """
    Get wave data filtered by wave name and/or date range.
    
    Handles special cases:
    - Crypto Income Wave: maps to underlying crypto income data
    - CSE (Crypto Selection Engine): returns synthetic/computed data
    - Excluded legacy crypto waves: returns None
    
    Uses normalized wave names for matching (case-insensitive, trimmed, collapsed spaces).
    
    Returns DataFrame or None if unavailable.
    
    Args:
        wave_name: Name of wave to filter (None for all waves)
        days: Number of days to include (from latest date)
        _wave_universe_version: Version counter for cache invalidation (prefixed with _ to ignore in hash)
    """
    try:
        # Get wave_universe_version from session state
        wave_universe_version = st.session_state.get("wave_universe_version", 1)
        
        # Special case: Russell 3000 Wave (index reference wave)
        if wave_name == "Russell 3000 Wave":
            # Return None to gracefully display "Data unavailable"
            # This wave is tracked but may not have historical data yet
            return None
        
        # Special case: CSE (Crypto Selection Engine)
        if wave_name == CSE_WAVE_NAME:
            return get_cse_wave_data(days=days)
        
        # Special case: Crypto Income Wave
        if wave_name == CRYPTO_INCOME_WAVE:
            return get_crypto_income_wave_data(days=days)
        
        # Exclude legacy crypto waves
        if wave_name in EXCLUDED_CRYPTO_WAVES:
            return None
        
        # Load standard wave data
        df = safe_load_wave_history(_wave_universe_version=wave_universe_version)
        
        if df is None:
            return None
        
        # Filter by wave if specified
        if wave_name is not None:
            if 'wave' not in df.columns:
                return None
            
            # Normalize the selected wave name for matching
            normalized_selection = wave_name.strip().replace('  ', ' ').lower()
            
            # Filter using normalized_wave column if available
            if 'normalized_wave' in df.columns:
                df = df[df['normalized_wave'] == normalized_selection].copy()
            else:
                # Fallback: exact match on wave column
                df = df[df['wave'] == wave_name].copy()
        
        if len(df) == 0:
            return None
        
        # Filter by date range if days specified
        if days is not None and days > 0:
            latest_date = df['date'].max()
            cutoff_date = latest_date - timedelta(days=days)
            df = df[df['date'] >= cutoff_date].copy()
        
        if len(df) == 0:
            return None
        
        return df
        
    except Exception:
        return None


def calculate_alpha_components(wave_data, wave_name):
    """
    Calculate alpha decomposition components.
    
    Returns:
        Dictionary with alpha components or None if unavailable
    """
    try:
        if wave_data is None or len(wave_data) == 0:
            return None
        
        # Ensure required columns exist
        if 'portfolio_return' not in wave_data.columns or 'benchmark_return' not in wave_data.columns:
            return None
        
        wave_data = wave_data.copy()
        wave_data['alpha'] = wave_data['portfolio_return'] - wave_data['benchmark_return']
        
        # Total alpha
        total_alpha = wave_data['alpha'].sum()
        
        # Selection Alpha: Base alpha from wave return vs benchmark
        # This is the primary component - the actual differential
        selection_alpha = total_alpha * 0.70  # Estimate: 70% from selection
        
        # Overlay Alpha: Impact of exposure scaling and VIX gates
        # Check if we have exposure data
        overlay_alpha = 0.0
        if 'exposure' in wave_data.columns:
            # Calculate impact of exposure scaling
            # Overlay alpha = (1 - avg_exposure) * avg_portfolio_return
            avg_exposure = wave_data['exposure'].mean()
            if avg_exposure < 1.0:
                overlay_alpha = total_alpha * 0.20  # Estimate: 20% from overlay
        else:
            # No exposure data, estimate based on alpha variance
            overlay_alpha = total_alpha * 0.15  # Conservative estimate
        
        # Cash/Risk-Off Contribution: Remaining component
        cash_contribution = total_alpha - selection_alpha - overlay_alpha
        
        return {
            'total_alpha': total_alpha,
            'selection_alpha': selection_alpha,
            'overlay_alpha': overlay_alpha,
            'cash_contribution': cash_contribution,
            'wave_return': wave_data['portfolio_return'].sum(),
            'benchmark_return': wave_data['benchmark_return'].sum()
        }
        
    except Exception:
        return None


def calculate_attribution_matrix(wave_data, wave_name):
    """
    Calculate attribution matrix with regime-based breakdown.
    
    Returns:
        Dictionary with attribution metrics or None if unavailable
    """
    try:
        if wave_data is None or len(wave_data) == 0:
            return None
        
        wave_data = wave_data.copy()
        
        # Ensure alpha column exists
        if 'portfolio_return' not in wave_data.columns or 'benchmark_return' not in wave_data.columns:
            return None
        
        wave_data['alpha'] = wave_data['portfolio_return'] - wave_data['benchmark_return']
        
        # Total alpha
        total_alpha = wave_data['alpha'].sum()
        
        # Risk-On vs Risk-Off: Use VIX if available
        risk_on_alpha = 0.0
        risk_off_alpha = 0.0
        
        if 'vix' in wave_data.columns or 'regime' in wave_data.columns:
            # Determine regime
            if 'regime' in wave_data.columns:
                risk_on_mask = wave_data['regime'].str.contains('risk-on|growth|bullish', case=False, na=False)
                risk_off_mask = ~risk_on_mask
            elif 'vix' in wave_data.columns:
                # VIX < 20 = risk-on, VIX >= 20 = risk-off
                risk_on_mask = wave_data['vix'] < 20
                risk_off_mask = wave_data['vix'] >= 20
            else:
                risk_on_mask = wave_data.index % 2 == 0  # Fallback
                risk_off_mask = ~risk_on_mask
            
            risk_on_alpha = wave_data.loc[risk_on_mask, 'alpha'].sum()
            risk_off_alpha = wave_data.loc[risk_off_mask, 'alpha'].sum()
        else:
            # Estimate based on volatility
            volatility = wave_data['portfolio_return'].std()
            if volatility < 0.015:
                risk_on_alpha = total_alpha * 0.7
                risk_off_alpha = total_alpha * 0.3
            else:
                risk_on_alpha = total_alpha * 0.5
                risk_off_alpha = total_alpha * 0.5
        
        # Capital-Weighted Alpha: Use exposure if available
        capital_weighted_alpha = total_alpha
        if 'exposure' in wave_data.columns:
            avg_exposure = wave_data['exposure'].mean()
            capital_weighted_alpha = total_alpha * avg_exposure
        
        # Exposure-Adjusted Alpha: Adjust for average exposure
        exposure_adjusted_alpha = total_alpha
        if 'exposure' in wave_data.columns:
            avg_exposure = wave_data['exposure'].mean()
            if avg_exposure > 0:
                exposure_adjusted_alpha = total_alpha / avg_exposure
        
        return {
            'total_alpha': total_alpha,
            'risk_on_alpha': risk_on_alpha,
            'risk_off_alpha': risk_off_alpha,
            'capital_weighted_alpha': capital_weighted_alpha,
            'exposure_adjusted_alpha': exposure_adjusted_alpha
        }
        
    except Exception:
        return None


def render_data_diagnostic_card(wave_name, days=30):
    """
    Render a diagnostic card when wave data is unavailable.
    
    Shows:
    - File path loaded
    - Number of rows in history
    - Top 25 unique wave names
    - Date range (min/max)
    - Whether selected wave exists after normalization
    
    Args:
        wave_name: The wave name that was selected
        days: Lookback period in days
    """
    try:
        st.warning(f"‚ö†Ô∏è No data available for **{wave_name}** in the selected period ({days} days)")
        
        with st.expander("üîç Data Diagnostics - Click to expand", expanded=True):
            wave_universe_version = st.session_state.get("wave_universe_version", 1)
            df = safe_load_wave_history(_wave_universe_version=wave_universe_version)
            
            # File information
            wave_history_path = os.path.join(os.path.dirname(__file__), 'wave_history.csv')
            st.markdown("**üìÅ Data Source**")
            st.code(wave_history_path)
            
            if df is None:
                st.error("‚ùå Unable to load wave_history.csv")
                return
            
            # Row count
            st.markdown("**üìä Dataset Statistics**")
            col1, col2 = st.columns(2)
            with col1:
                st.metric("Total Rows Loaded", f"{len(df):,}")
            with col2:
                if 'wave' in df.columns:
                    unique_waves = df['wave'].nunique()
                    st.metric("Unique Waves", unique_waves)
            
            # Date range
            if 'date' in df.columns:
                st.markdown("**üìÖ Date Range**")
                col1, col2 = st.columns(2)
                with col1:
                    min_date = df['date'].min()
                    st.metric("Earliest Date", min_date.strftime("%Y-%m-%d") if pd.notna(min_date) else "N/A")
                with col2:
                    max_date = df['date'].max()
                    st.metric("Latest Date", max_date.strftime("%Y-%m-%d") if pd.notna(max_date) else "N/A")
            
            # Top 25 wave names
            if 'wave' in df.columns:
                st.markdown("**üåä Top 25 Unique Wave Names Found in History**")
                unique_waves = sorted(df['wave'].unique().tolist())
                top_25 = unique_waves[:25]
                
                # Display in 3 columns
                col1, col2, col3 = st.columns(3)
                for i, wave in enumerate(top_25):
                    col = [col1, col2, col3][i % 3]
                    with col:
                        st.text(f"‚Ä¢ {wave}")
                
                if len(unique_waves) > 25:
                    st.caption(f"... and {len(unique_waves) - 25} more")
            
            # Check if selected wave exists (with normalization)
            st.markdown("**üîé Selected Wave Lookup**")
            if 'wave' in df.columns:
                normalized_selection = wave_name.strip().replace('  ', ' ').lower()
                
                # Check exact match
                exact_match = wave_name in df['wave'].values
                
                # Check normalized match
                if 'normalized_wave' in df.columns:
                    normalized_match = normalized_selection in df['normalized_wave'].values
                else:
                    normalized_match = False
                
                col1, col2 = st.columns(2)
                with col1:
                    if exact_match:
                        st.success(f"‚úÖ Exact match found for '{wave_name}'")
                    else:
                        st.error(f"‚ùå No exact match for '{wave_name}'")
                
                with col2:
                    if normalized_match:
                        st.success("‚úÖ Normalized match found")
                    else:
                        st.error("‚ùå No normalized match")
                
                # Show similar wave names if no match
                if not exact_match and not normalized_match:
                    st.markdown("**üí° Similar Wave Names (suggestions):**")
                    # Find similar names using simple string matching
                    similar_waves = []
                    wave_lower = wave_name.lower()
                    for unique_wave in unique_waves:
                        if wave_lower in unique_wave.lower() or unique_wave.lower() in wave_lower:
                            similar_waves.append(unique_wave)
                    
                    if similar_waves:
                        for sim_wave in similar_waves[:5]:
                            st.text(f"  ‚Üí {sim_wave}")
                    else:
                        st.text("No similar wave names found")
            
            # Show filter applied for period
            if 'date' in df.columns and days:
                st.markdown(f"**‚è±Ô∏è Period Filter Applied: Last {days} Days**")
                latest_date = df['date'].max()
                cutoff_date = latest_date - timedelta(days=days)
                st.text(f"Cutoff Date: {cutoff_date.strftime('%Y-%m-%d')}")
                st.text(f"Latest Date: {latest_date.strftime('%Y-%m-%d')}")
                
                # Check if wave has data in this period
                if 'wave' in df.columns:
                    period_df = df[df['date'] >= cutoff_date]
                    if normalized_selection in period_df.get('normalized_wave', pd.Series()).values:
                        wave_rows_in_period = len(period_df[period_df['normalized_wave'] == normalized_selection])
                        st.info(f"‚ÑπÔ∏è Wave exists in full history but has only {wave_rows_in_period} row(s) in this period")
                    elif wave_name in period_df['wave'].values:
                        wave_rows_in_period = len(period_df[period_df['wave'] == wave_name])
                        st.info(f"‚ÑπÔ∏è Wave exists in full history but has only {wave_rows_in_period} row(s) in this period")
    
    except Exception as e:
        st.error(f"Error rendering diagnostics: {str(e)}")


def calculate_portfolio_metrics(wave_names, weights, days):
    """
    Calculate blended portfolio metrics for multiple waves.
    
    Args:
        wave_names: List of wave names
        weights: Dictionary mapping wave names to weights (decimal)
        days: Number of days for analysis
        
    Returns:
        Dictionary with portfolio metrics or None if unavailable
    """
    try:
        if not wave_names or not weights:
            return None
        
        # Load data for all waves
        wave_data_dict = {}
        for wave_name in wave_names:
            wave_data = get_wave_data_filtered(wave_name=wave_name, days=days)
            if wave_data is not None and len(wave_data) > 0:
                wave_data_dict[wave_name] = wave_data
        
        if len(wave_data_dict) == 0:
            return None
        
        # Align dates - find common dates
        all_dates = None
        for wave_name, wave_data in wave_data_dict.items():
            wave_dates = set(wave_data['date'])
            if all_dates is None:
                all_dates = wave_dates
            else:
                all_dates = all_dates.intersection(wave_dates)
        
        if not all_dates or len(all_dates) == 0:
            return None
        
        all_dates = sorted(list(all_dates))
        
        # Calculate blended returns
        blended_returns = []
        for date in all_dates:
            daily_return = 0.0
            for wave_name, weight in weights.items():
                if wave_name in wave_data_dict:
                    wave_data = wave_data_dict[wave_name]
                    date_data = wave_data[wave_data['date'] == date]
                    if len(date_data) > 0:
                        daily_return += weight * date_data['portfolio_return'].iloc[0]
            blended_returns.append(daily_return)
        
        blended_returns = np.array(blended_returns)
        
        # Calculate metrics
        blended_return = blended_returns.sum()
        blended_volatility = blended_returns.std()
        
        # Calculate drawdown
        cumulative_returns = (1 + blended_returns).cumprod()
        running_max = np.maximum.accumulate(cumulative_returns)
        drawdown = (cumulative_returns - running_max) / running_max
        max_drawdown = drawdown.min()
        
        # Calculate blended WaveScore (weighted average)
        blended_wavescore = 0.0
        for wave_name, weight in weights.items():
            if wave_name in wave_data_dict:
                wave_data = wave_data_dict[wave_name]
                wavescore = calculate_wavescore(wave_data)
                blended_wavescore += weight * wavescore
        
        # Calculate correlation matrix
        correlation_matrix = None
        if len(wave_names) > 1:
            # Build return matrix
            return_matrix = {}
            for wave_name in wave_names:
                if wave_name in wave_data_dict:
                    wave_returns = []
                    wave_data = wave_data_dict[wave_name]
                    for date in all_dates:
                        date_data = wave_data[wave_data['date'] == date]
                        if len(date_data) > 0:
                            wave_returns.append(date_data['portfolio_return'].iloc[0])
                        else:
                            wave_returns.append(0.0)
                    return_matrix[wave_name] = wave_returns
            
            if len(return_matrix) > 1:
                return_df = pd.DataFrame(return_matrix)
                correlation_matrix = return_df.corr()
        
        # Calculate individual contributions
        contributions = {}
        for wave_name, weight in weights.items():
            if wave_name in wave_data_dict:
                wave_data = wave_data_dict[wave_name]
                wave_return = 0.0
                for date in all_dates:
                    date_data = wave_data[wave_data['date'] == date]
                    if len(date_data) > 0:
                        wave_return += date_data['portfolio_return'].iloc[0]
                contributions[wave_name] = weight * wave_return
        
        return {
            'blended_return': blended_return,
            'blended_volatility': blended_volatility,
            'max_drawdown': max_drawdown,
            'blended_wavescore': blended_wavescore,
            'correlation_matrix': correlation_matrix,
            'contributions': contributions
        }
        
    except Exception:
        return None


# ============================================================================
# OVERVIEW PAGE HELPER FUNCTIONS
# ============================================================================

def get_multi_timeframe_wave_data(wave_name, timeframes=[1, 30, 60, 365]):
    """
    Get wave data for multiple timeframes.
    
    Args:
        wave_name: Name of the wave
        timeframes: List of timeframes in days (e.g., [1, 30, 60, 365])
    
    Returns:
        Dictionary mapping timeframe to metrics dict or None
    """
    try:
        results = {}
        for days in timeframes:
            wave_data = get_wave_data_filtered(wave_name=wave_name, days=days)
            if wave_data is not None and len(wave_data) > 0:
                # Calculate metrics for this timeframe
                wave_return = wave_data['portfolio_return'].sum() if 'portfolio_return' in wave_data.columns else None
                benchmark_return = wave_data['benchmark_return'].sum() if 'benchmark_return' in wave_data.columns else None
                alpha = (wave_return - benchmark_return) if wave_return is not None and benchmark_return is not None else None
                
                results[days] = {
                    'wave_return': wave_return,
                    'benchmark_return': benchmark_return,
                    'alpha': alpha,
                    'data_points': len(wave_data)
                }
            else:
                results[days] = None
        
        return results
    except Exception:
        return {}


def get_all_waves_multi_timeframe_data(timeframes=[1, 30, 60, 365]):
    """
    Get multi-timeframe data for all waves.
    
    Args:
        timeframes: List of timeframes in days
        
    Returns:
        DataFrame with columns: Wave, 1D_Wave, 1D_BM, 1D_Alpha, 30D_Wave, etc.
    """
    try:
        waves = get_available_waves()
        if not waves:
            return pd.DataFrame()
        
        rows = []
        for wave_name in waves:
            row = {'Wave': wave_name}
            
            for days in timeframes:
                wave_data = get_wave_data_filtered(wave_name=wave_name, days=days)
                
                if wave_data is not None and len(wave_data) > 0:
                    wave_return = wave_data['portfolio_return'].sum() if 'portfolio_return' in wave_data.columns else None
                    benchmark_return = wave_data['benchmark_return'].sum() if 'benchmark_return' in wave_data.columns else None
                    alpha = (wave_return - benchmark_return) if wave_return is not None and benchmark_return is not None else None
                    
                    row[f'{days}D_Wave'] = wave_return
                    row[f'{days}D_BM'] = benchmark_return
                    row[f'{days}D_Alpha'] = alpha
                else:
                    row[f'{days}D_Wave'] = None
                    row[f'{days}D_BM'] = None
                    row[f'{days}D_Alpha'] = None
            
            rows.append(row)
        
        df = pd.DataFrame(rows)
        return df
        
    except Exception:
        return pd.DataFrame()


def get_system_statistics(timeframe_days=30):
    """
    Compute system-wide statistics across all waves.
    
    Args:
        timeframe_days: Number of days for analysis (default: 30)
        
    Returns:
        Dictionary with system-level statistics
    """
    try:
        waves = get_available_waves()
        if not waves:
            return None
        
        alpha_values = []
        wave_returns = []
        benchmark_returns = []
        positive_alpha_count = 0
        total_count = 0
        
        best_wave = None
        best_alpha = -float('inf')
        worst_wave = None
        worst_alpha = float('inf')
        
        for wave_name in waves:
            wave_data = get_wave_data_filtered(wave_name=wave_name, days=timeframe_days)
            
            if wave_data is not None and len(wave_data) > 0:
                wave_return = wave_data['portfolio_return'].sum() if 'portfolio_return' in wave_data.columns else None
                benchmark_return = wave_data['benchmark_return'].sum() if 'benchmark_return' in wave_data.columns else None
                alpha = (wave_return - benchmark_return) if wave_return is not None and benchmark_return is not None else None
                
                if alpha is not None:
                    alpha_values.append(alpha)
                    wave_returns.append(wave_return)
                    benchmark_returns.append(benchmark_return)
                    total_count += 1
                    
                    if alpha > 0:
                        positive_alpha_count += 1
                    
                    if alpha > best_alpha:
                        best_alpha = alpha
                        best_wave = wave_name
                    
                    if alpha < worst_alpha:
                        worst_alpha = alpha
                        worst_wave = wave_name
        
        if total_count == 0:
            return None
        
        return {
            'avg_alpha': np.mean(alpha_values) if alpha_values else 0.0,
            'avg_wave_return': np.mean(wave_returns) if wave_returns else 0.0,
            'avg_benchmark_return': np.mean(benchmark_returns) if benchmark_returns else 0.0,
            'pct_positive_alpha': (positive_alpha_count / total_count) * 100 if total_count > 0 else 0.0,
            'best_wave': best_wave,
            'best_alpha': best_alpha,
            'worst_wave': worst_wave,
            'worst_alpha': worst_alpha,
            'total_waves': total_count
        }
        
    except Exception:
        return None


def get_top_bottom_waves(timeframe_days=30, top_n=3, bottom_n=3):
    """
    Get top and bottom waves by alpha.
    
    Args:
        timeframe_days: Number of days for analysis
        top_n: Number of top waves to return
        bottom_n: Number of bottom waves to return
        
    Returns:
        Tuple of (top_waves, bottom_waves) where each is a list of (wave_name, alpha) tuples
    """
    try:
        waves = get_available_waves()
        if not waves:
            return [], []
        
        wave_alphas = []
        
        for wave_name in waves:
            wave_data = get_wave_data_filtered(wave_name=wave_name, days=timeframe_days)
            
            if wave_data is not None and len(wave_data) > 0:
                wave_return = wave_data['portfolio_return'].sum() if 'portfolio_return' in wave_data.columns else None
                benchmark_return = wave_data['benchmark_return'].sum() if 'benchmark_return' in wave_data.columns else None
                alpha = (wave_return - benchmark_return) if wave_return is not None and benchmark_return is not None else None
                
                if alpha is not None:
                    wave_alphas.append((wave_name, alpha))
        
        if not wave_alphas:
            return [], []
        
        # Sort by alpha
        wave_alphas.sort(key=lambda x: x[1], reverse=True)
        
        top_waves = wave_alphas[:top_n]
        bottom_waves = wave_alphas[-bottom_n:][::-1]  # Reverse to show worst first
        
        return top_waves, bottom_waves
        
    except Exception:
        return [], []


def compute_alpha_metrics_for_wave(wave_name, wave_id=None):
    """
    Compute Alpha and Exposure-Adjusted Alpha for a single wave across all timeframes.
    
    This is the canonical helper function for Alpha Capture calculations.
    Reuses existing engine outputs and datasets (wave_history.csv).
    
    Args:
        wave_name: Display name of the wave
        wave_id: Optional wave ID for reference
        
    Returns:
        Dictionary with:
        - wave_name: Display name
        - wave_id: Wave ID (if available)
        - alpha_1d: 1-day alpha
        - alpha_30d: 30-day alpha
        - alpha_60d: 60-day alpha  
        - alpha_365d: 365-day alpha
        - exp_adj_alpha_1d: Exposure-adjusted 1-day alpha
        - exp_adj_alpha_30d: Exposure-adjusted 30-day alpha
        - exp_adj_alpha_60d: Exposure-adjusted 60-day alpha
        - exp_adj_alpha_365d: Exposure-adjusted 365-day alpha
        - exposure_1d: 1-day exposure (defaults to 1.0)
        - exposure_30d: 30-day average exposure (defaults to 1.0)
        - exposure_60d: 60-day average exposure (defaults to 1.0)
        - exposure_365d: 365-day average exposure (defaults to 1.0)
        - last_updated: Last data update timestamp
    """
    metrics = {
        'wave_name': wave_name,
        'wave_id': wave_id if wave_id else 'N/A',
        'alpha_1d': None,
        'alpha_30d': None,
        'alpha_60d': None,
        'alpha_365d': None,
        'exp_adj_alpha_1d': None,
        'exp_adj_alpha_30d': None,
        'exp_adj_alpha_60d': None,
        'exp_adj_alpha_365d': None,
        'exposure_1d': 1.0,
        'exposure_30d': 1.0,
        'exposure_60d': 1.0,
        'exposure_365d': 1.0,
        'last_updated': None
    }
    
    try:
        # Get wave universe version
        wave_universe_version = st.session_state.get("wave_universe_version", 1)
        
        # Load data for all timeframes
        data_1d = get_wave_data_filtered(wave_name=wave_name, days=1, _wave_universe_version=wave_universe_version)
        data_30d = get_wave_data_filtered(wave_name=wave_name, days=30, _wave_universe_version=wave_universe_version)
        data_60d = get_wave_data_filtered(wave_name=wave_name, days=60, _wave_universe_version=wave_universe_version)
        data_365d = get_wave_data_filtered(wave_name=wave_name, days=365, _wave_universe_version=wave_universe_version)
        
        # Helper function to calculate alpha and exposure-adjusted alpha for a timeframe
        def calc_alpha_metrics(data):
            if data is None or len(data) == 0:
                return None, None, 1.0, None
            
            alpha = None
            exp_adj_alpha = None
            avg_exposure = 1.0
            last_date = None
            
            # Calculate alpha (Wave Return - Benchmark Return)
            if 'portfolio_return' in data.columns and 'benchmark_return' in data.columns:
                wave_return = data['portfolio_return'].sum()
                benchmark_return = data['benchmark_return'].sum()
                alpha = wave_return - benchmark_return
                
                # Calculate exposure-adjusted alpha
                if 'exposure' in data.columns:
                    # Use actual exposure data
                    avg_exposure = data['exposure'].fillna(1.0).mean()
                    # Exposure-Adjusted Alpha = (Wave Return - Benchmark Return) √ó Exposure
                    exp_adj_alpha = alpha * avg_exposure
                else:
                    # Default to 1.0 exposure (fully invested)
                    exp_adj_alpha = alpha * 1.0
                
                # Get last updated date
                if 'date' in data.columns:
                    last_date = data['date'].max()
            
            return alpha, exp_adj_alpha, avg_exposure, last_date
        
        # Calculate metrics for each timeframe
        alpha_1d, exp_adj_1d, exposure_1d, date_1d = calc_alpha_metrics(data_1d)
        alpha_30d, exp_adj_30d, exposure_30d, date_30d = calc_alpha_metrics(data_30d)
        alpha_60d, exp_adj_60d, exposure_60d, date_60d = calc_alpha_metrics(data_60d)
        alpha_365d, exp_adj_365d, exposure_365d, date_365d = calc_alpha_metrics(data_365d)
        
        # Update metrics
        metrics['alpha_1d'] = alpha_1d
        metrics['alpha_30d'] = alpha_30d
        metrics['alpha_60d'] = alpha_60d
        metrics['alpha_365d'] = alpha_365d
        metrics['exp_adj_alpha_1d'] = exp_adj_1d
        metrics['exp_adj_alpha_30d'] = exp_adj_30d
        metrics['exp_adj_alpha_60d'] = exp_adj_60d
        metrics['exp_adj_alpha_365d'] = exp_adj_365d
        metrics['exposure_1d'] = exposure_1d
        metrics['exposure_30d'] = exposure_30d
        metrics['exposure_60d'] = exposure_60d
        metrics['exposure_365d'] = exposure_365d
        
        # Use the most recent date available
        last_dates = [d for d in [date_1d, date_30d, date_60d, date_365d] if d is not None]
        if last_dates:
            metrics['last_updated'] = max(last_dates)
        
    except Exception:
        pass  # Return metrics with default/None values
    
    return metrics


def compute_alpha_drivers(wave_name, timeframe_days=30):
    """
    Compute alpha driver breakdown for a wave across a specified timeframe.
    
    Returns a 3-bucket breakdown:
    - Stock Selection (Portfolio vs Benchmark)
    - Risk Overlay (VIX/SafeSmart/Cash Shift)
    - Residual/Other
    
    Args:
        wave_name: Display name of the wave
        timeframe_days: Number of days to analyze (default 30)
        
    Returns:
        Dictionary with:
        - total_alpha: Total alpha in return points
        - selection_contribution: Selection alpha in return points
        - overlay_contribution: Overlay alpha in return points
        - residual_contribution: Residual alpha in return points
        - selection_percent: Selection share as percentage
        - overlay_percent: Overlay share as percentage
        - residual_percent: Residual share as percentage
        - wave_return: Wave return over the period
        - benchmark_return: Benchmark return over the period
        - avg_exposure: Average exposure over the period
        - last_updated: Last data update timestamp
        - has_diagnostics: Whether diagnostics data was used
    """
    result = {
        'total_alpha': 0.0,
        'selection_contribution': 0.0,
        'overlay_contribution': 0.0,
        'residual_contribution': 0.0,
        'selection_percent': None,
        'overlay_percent': None,
        'residual_percent': None,
        'wave_return': 0.0,
        'benchmark_return': 0.0,
        'avg_exposure': 1.0,
        'last_updated': None,
        'has_diagnostics': False
    }
    
    try:
        # Get wave universe version
        wave_universe_version = st.session_state.get("wave_universe_version", 1)
        
        # Load wave data for the specified timeframe
        wave_data = get_wave_data_filtered(
            wave_name=wave_name, 
            days=timeframe_days, 
            _wave_universe_version=wave_universe_version
        )
        
        if wave_data is None or len(wave_data) == 0:
            return result
        
        # Check required columns
        if 'portfolio_return' not in wave_data.columns or 'benchmark_return' not in wave_data.columns:
            return result
        
        # Calculate total returns
        wave_return = wave_data['portfolio_return'].sum()
        benchmark_return = wave_data['benchmark_return'].sum()
        total_alpha = wave_return - benchmark_return
        
        result['wave_return'] = wave_return
        result['benchmark_return'] = benchmark_return
        result['total_alpha'] = total_alpha
        
        # Get last updated date
        if 'date' in wave_data.columns:
            result['last_updated'] = wave_data['date'].max()
        
        # Calculate average exposure
        avg_exposure = 1.0
        if 'exposure' in wave_data.columns:
            avg_exposure = wave_data['exposure'].fillna(1.0).mean()
        result['avg_exposure'] = avg_exposure
        
        # ========================================================================
        # STEP 1: Try to use diagnostics data (preferred method)
        # ========================================================================
        # 
        # Correct Attribution Logic (Counterfactual Analysis):
        # 
        # Stock Selection Contribution:
        #   - Counterfactual: Same holdings, always fully invested (exposure=1.0, safe_fraction=0.0)
        #   - This isolates the contribution from the portfolio holdings themselves
        #   - Formula: sum(portfolio_risk_ret) - benchmark_return
        # 
        # Risk Overlay Contribution:
        #   - Counterfactual: Difference between actual returns and full-exposure scenario
        #   - This captures value from exposure management and safe asset allocation
        #   - Formula: actual_wave_return - stock_selection_counterfactual_return
        # 
        # Residual Contribution:
        #   - Compounding effects, timing interactions, rounding
        #   - NOT forced to zero - reflects real nonlinear behavior
        #   - Formula: total_alpha - (stock_selection + risk_overlay)
        # 
        selection_contribution = 0.0
        overlay_contribution = 0.0
        has_diagnostics = False
        
        if VIX_DIAGNOSTICS_AVAILABLE:
            try:
                # Get diagnostics for this wave
                diagnostics = get_wave_diagnostics(
                    wave_name=wave_name,
                    mode="Standard",
                    days=timeframe_days
                )
                
                if diagnostics is not None and not diagnostics.empty:
                    if 'Wave_Return' in diagnostics.columns and 'Benchmark_Return' in diagnostics.columns:
                        # Get actual wave and benchmark returns
                        actual_wave_return = diagnostics['Wave_Return'].sum()
                        benchmark_return = diagnostics['Benchmark_Return'].sum()
                        
                        # STOCK SELECTION CONTRIBUTION (Counterfactual Analysis)
                        # ======================================================
                        # Counterfactual: What would the return be with the same holdings
                        # but always fully invested (exposure=1.0, safe_fraction=0.0)?
                        # 
                        # The Wave return formula is:
                        #   wave_ret = safe_fraction * safe_ret + risk_fraction * exposure * portfolio_risk_ret
                        # 
                        # For the counterfactual (full exposure, no safe assets):
                        #   counterfactual_ret = portfolio_risk_ret
                        # 
                        # We need to reconstruct portfolio_risk_ret from the actual returns.
                        # Given: wave_ret = safe_frac * safe_ret + (1 - safe_frac) * exposure * portfolio_risk_ret
                        # Solve for: portfolio_risk_ret = (wave_ret - safe_frac * safe_ret) / ((1 - safe_frac) * exposure)
                        
                        if 'Exposure' in diagnostics.columns and 'Safe_Fraction' in diagnostics.columns:
                            # Calculate counterfactual return (fully invested in risky assets)
                            counterfactual_returns = []
                            
                            for idx, row in diagnostics.iterrows():
                                wave_ret = row.get('Wave_Return', 0.0)
                                exposure_val = row.get('Exposure', 1.0)
                                safe_frac = row.get('Safe_Fraction', 0.0)
                                risk_frac = 1.0 - safe_frac
                                
                                # Estimate safe asset return (use small constant for now)
                                # In the engine, safe assets return roughly 4 bps annually
                                # Daily return = 0.0004 / 252 ‚âà 0.0000016
                                safe_ret = 0.0000016  # ~4 bps annually, converted to daily
                                
                                # Reconstruct the portfolio risk return
                                # portfolio_risk_ret = (wave_ret - safe_frac * safe_ret) / (risk_frac * exposure)
                                denominator = risk_frac * exposure_val
                                if abs(denominator) > 0.001:  # Avoid division by very small numbers
                                    portfolio_risk_ret = (wave_ret - safe_frac * safe_ret) / denominator
                                else:
                                    # When exposure is near zero, wave return is mostly from safe assets
                                    # The counterfactual would have zero return from risky assets
                                    portfolio_risk_ret = 0.0
                                
                                # Counterfactual: fully invested in risky portfolio
                                counterfactual_returns.append(portfolio_risk_ret)
                            
                            # Stock selection is the counterfactual return minus benchmark
                            counterfactual_total = sum(counterfactual_returns)
                            selection_contribution = counterfactual_total - benchmark_return
                            
                            # RISK OVERLAY CONTRIBUTION (Counterfactual Comparison)
                            # =====================================================
                            # This is the difference between actual returns and the
                            # counterfactual full-exposure scenario
                            overlay_contribution = actual_wave_return - counterfactual_total
                            
                            has_diagnostics = True
                        else:
                            # Missing required columns - fall through to fallback
                            pass
            except Exception:
                pass  # Fall through to fallback method
        
        # ========================================================================
        # STEP 2: Fallback method using average exposure (less accurate)
        # ========================================================================
        if not has_diagnostics:
            # Without detailed diagnostics, we use a simplified approximation
            # This is less accurate but better than nothing
            
            # Estimate counterfactual using average exposure
            if avg_exposure > 0.01 and avg_exposure < 0.99:
                # There was meaningful exposure variation
                # Rough estimate: stock selection contributed based on scaled return
                safe_ret_estimate = 0.0000016 * timeframe_days  # ~4 bps annually, converted to daily
                avg_safe_fraction = max(0.0, 1.0 - avg_exposure)  # Rough estimate
                avg_risk_fraction = 1.0 - avg_safe_fraction
                
                # Estimate portfolio risk return from actual wave return
                # wave_return ‚âà safe_frac * safe_ret + risk_frac * exposure * portfolio_risk_ret
                denominator = avg_risk_fraction * avg_exposure
                if abs(denominator) > 0.001:
                    portfolio_risk_ret_est = (wave_return - avg_safe_fraction * safe_ret_estimate) / denominator
                    
                    # Stock selection: counterfactual fully invested
                    selection_contribution = portfolio_risk_ret_est - benchmark_return
                    
                    # Risk overlay: difference between actual and counterfactual
                    overlay_contribution = wave_return - portfolio_risk_ret_est
                else:
                    # Very low exposure - mostly in safe assets
                    selection_contribution = 0.0
                    overlay_contribution = total_alpha
            else:
                # Average exposure near 1.0 or no exposure data
                # Assume most alpha is from selection, minimal from overlay
                selection_contribution = total_alpha
                overlay_contribution = 0.0
        
        result['selection_contribution'] = selection_contribution
        result['overlay_contribution'] = overlay_contribution
        result['has_diagnostics'] = has_diagnostics
        
        # ========================================================================
        # STEP 3: Calculate residual contribution (NOT forced to zero)
        # ========================================================================
        # Residual captures:
        # - Compounding effects (multiplicative vs additive returns)
        # - Timing interactions between exposure changes and market moves
        # - Rounding and numerical precision
        # - Nonlinear behavior from strategy interactions
        # 
        # This is an honest reflection of what cannot be cleanly attributed
        residual_contribution = total_alpha - (selection_contribution + overlay_contribution)
        
        result['residual_contribution'] = residual_contribution
        
        # ========================================================================
        # STEP 4: Calculate percentage shares
        # ========================================================================
        if abs(total_alpha) > 1e-6:  # Avoid division by very small numbers
            result['selection_percent'] = (selection_contribution / total_alpha) * 100
            result['overlay_percent'] = (overlay_contribution / total_alpha) * 100
            result['residual_percent'] = (residual_contribution / total_alpha) * 100
        else:
            # Total alpha is effectively zero - set to N/A
            result['selection_percent'] = None
            result['overlay_percent'] = None
            result['residual_percent'] = None
        
    except Exception:
        pass  # Return default result
    
    return result


def compute_alpha_metrics_all_waves():
    """
    Compute Alpha and Exposure-Adjusted Alpha for all waves.
    
    Returns:
        List of dictionaries, one per wave, with alpha metrics across all timeframes.
    """
    all_metrics = []
    
    try:
        # Get all waves from the universe
        waves = get_wave_universe()
        
        # Get wave_history to extract wave_ids if available
        wave_universe_version = st.session_state.get("wave_universe_version", 1)
        wave_history = safe_load_wave_history(_wave_universe_version=wave_universe_version)
        
        wave_id_map = {}
        if wave_history is not None and 'wave' in wave_history.columns and 'wave_id' in wave_history.columns:
            # Create mapping from display_name to wave_id
            for _, row in wave_history.iterrows():
                wave_name = row.get('wave', row.get('display_name', ''))
                wave_id = row.get('wave_id', None)
                if wave_name and wave_id:
                    wave_id_map[wave_name] = wave_id
        
        # Compute metrics for each wave
        for wave_name in waves:
            wave_id = wave_id_map.get(wave_name, None)
            metrics = compute_alpha_metrics_for_wave(wave_name, wave_id)
            all_metrics.append(metrics)
    
    except Exception:
        pass  # Return empty list on error
    
    return all_metrics


# ============================================================================
# WAVE UNIVERSE TRUTH LAYER - Diagnostics and Freshness
# ============================================================================

def compute_wave_universe_diagnostics():
    """
    Compute comprehensive Wave Universe diagnostics.
    
    Returns:
        Dictionary with diagnostic metrics including:
        - universe_count: Total waves in canonical registry
        - active_count: Waves with enabled=True (NEW: not data-dependent)
        - data_ready_count: Waves with recent data (last 7 days)
        - history_unique_count: Unique waves in wave_history.csv
        - missing_waves: Waves in registry but no data
        - orphan_waves: Waves in data but not in registry
        - duplicate_waves: Duplicates removed during deduplication
        - data_freshness: Per-wave freshness information
    """
    diagnostics = {
        'universe_count': 0,
        'active_count': 0,
        'data_ready_count': 0,  # NEW: Waves with full analytics
        'history_unique_count': 0,
        'missing_waves': [],
        'orphan_waves': [],
        'duplicate_waves': [],
        'data_freshness': []
    }
    
    try:
        # Get canonical universe
        # Note: force_reload=False is intentional - diagnostics should use cached universe
        # to show current state. Use operator controls to force reload if needed.
        wave_universe_version = st.session_state.get("wave_universe_version", 1)
        universe = get_canonical_wave_universe(force_reload=False, _wave_universe_version=wave_universe_version)
        
        registry_waves = set(universe.get("waves", []))
        duplicate_waves = universe.get("removed_duplicates", [])
        enabled_flags = universe.get("enabled_flags", {})
        
        diagnostics['universe_count'] = len(registry_waves)
        diagnostics['duplicate_waves'] = duplicate_waves
        
        # NEW: Active count = enabled waves (not data-dependent)
        active_count = sum(1 for wave in registry_waves if enabled_flags.get(wave, True))
        diagnostics['active_count'] = active_count
        
        # Get wave history data
        wave_history = safe_load_wave_history(_wave_universe_version=wave_universe_version)
        
        if wave_history is not None and 'wave' in wave_history.columns:
            # Get unique waves in history
            history_waves = set(wave_history['wave'].unique())
            diagnostics['history_unique_count'] = len(history_waves)
            
            # Calculate missing waves (in registry but no data)
            missing_waves = registry_waves - history_waves
            diagnostics['missing_waves'] = sorted(list(missing_waves))
            
            # Calculate orphan waves (in data but not in registry)
            orphan_waves = history_waves - registry_waves
            diagnostics['orphan_waves'] = sorted(list(orphan_waves))
            
            # NEW: Calculate data-ready waves using explicit criteria
            if 'date' in wave_history.columns:
                latest_date = wave_history['date'].max()
                
                # Use is_wave_data_ready to check each wave
                data_ready_waves = set()
                wave_statuses = {}
                
                # Get cached price_df from session state
                price_df = st.session_state.get("global_price_df")
                
                for wave in registry_waves:
                    is_ready, status, reason = is_wave_data_ready(wave, wave_history, universe, price_df)
                    wave_statuses[wave] = {'status': status, 'reason': reason}
                    if is_ready:
                        data_ready_waves.add(wave)
                
                diagnostics['data_ready_count'] = len(data_ready_waves)
                diagnostics['wave_statuses'] = wave_statuses
                
                # Compute data freshness per wave
                data_freshness = []
                for wave in sorted(registry_waves):
                    wave_data = wave_history[wave_history['wave'] == wave]
                    
                    if len(wave_data) == 0:
                        # No data for this wave
                        data_freshness.append({
                            'wave': wave,
                            'latest_date': None,
                            'days_old': None,
                            'staleness': 'unavailable'
                        })
                    else:
                        latest_wave_date = wave_data['date'].max()
                        days_old = (latest_date - latest_wave_date).days
                        
                        # Determine staleness badge
                        if days_old == 0:
                            staleness = 'fresh'
                        elif days_old <= 3:
                            staleness = 'recent'
                        elif days_old <= 7:
                            staleness = 'stale'
                        else:
                            staleness = 'old'
                        
                        data_freshness.append({
                            'wave': wave,
                            'latest_date': latest_wave_date.strftime('%Y-%m-%d') if pd.notna(latest_wave_date) else 'N/A',
                            'days_old': days_old,
                            'staleness': staleness
                        })
                
                diagnostics['data_freshness'] = data_freshness
        
        return diagnostics
        
    except Exception as e:
        # Return diagnostics with error indicator
        diagnostics['error'] = str(e)
        return diagnostics


def render_wave_universe_truth_panel():
    """
    Render the Wave Universe Truth Panel - Executive diagnostic view.
    
    Displays:
    1. Metrics Panel: Universe, Active, HistoryUnique counts
    2. Diagnostics: Missing, Orphan, Duplicate waves
    3. Data Freshness Table with staleness badges
    4. Operator Controls: Force reload, clear cache, download CSV
    """
    st.markdown("### üî¨ Wave Universe Truth Panel")
    st.caption("Executive diagnostics and operator controls for the Wave Universe registry")
    
    # Compute diagnostics
    with st.spinner("Computing Wave Universe diagnostics..."):
        diagnostics = compute_wave_universe_diagnostics()
    
    # ========================================================================
    # SECTION 1: METRICS PANEL
    # ========================================================================
    st.markdown("#### üìä Universe Metrics")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            label="Universe",
            value=diagnostics.get('universe_count', 0),
            help="Total waves in canonical Wave Universe registry"
        )
    
    with col2:
        st.metric(
            label="Active Waves",
            value=diagnostics.get('active_count', 0),
            help="Waves with enabled=True (not data-dependent)"
        )
    
    with col3:
        st.metric(
            label="Data-Ready",
            value=diagnostics.get('data_ready_count', 0),
            help="Waves with full analytics data (recent 7 days)"
        )
    
    with col4:
        st.metric(
            label="History Unique",
            value=diagnostics.get('history_unique_count', 0),
            help="Unique waves in wave_history.csv"
        )
    
    st.divider()
    
    # ========================================================================
    # SECTION 2: DIAGNOSTICS
    # ========================================================================
    st.markdown("#### üîç Diagnostics")
    
    diag_col1, diag_col2, diag_col3 = st.columns(3)
    
    with diag_col1:
        missing_count = len(diagnostics.get('missing_waves', []))
        st.metric(
            label="Missing Waves",
            value=missing_count,
            help="Waves in registry but no data available"
        )
        if missing_count > 0:
            with st.expander("View Missing Waves"):
                for wave in diagnostics['missing_waves']:
                    st.text(f"‚Ä¢ {wave}")
    
    with diag_col2:
        orphan_count = len(diagnostics.get('orphan_waves', []))
        st.metric(
            label="Orphan Waves",
            value=orphan_count,
            help="Waves in data but not in registry"
        )
        if orphan_count > 0:
            with st.expander("View Orphan Waves"):
                for wave in diagnostics['orphan_waves']:
                    st.text(f"‚Ä¢ {wave}")
    
    with diag_col3:
        duplicate_count = len(diagnostics.get('duplicate_waves', []))
        st.metric(
            label="Duplicate Waves",
            value=duplicate_count,
            help="Duplicates removed during deduplication"
        )
        if duplicate_count > 0:
            with st.expander("View Duplicates Removed"):
                for wave in diagnostics['duplicate_waves']:
                    st.text(f"‚Ä¢ {wave}")
    
    st.divider()
    
    # ========================================================================
    # SECTION 2.5: WAVE STATUS SUMMARY AND TABLE (28/28 Rendering Enforcement)
    # ========================================================================
    st.markdown("#### üìä Wave Status Summary - All Active Waves Always Rendered")
    st.caption("üîî All active waves are always displayed regardless of data status - no blockers!")
    
    # Get wave statuses from diagnostics (uses is_wave_data_ready function)
    wave_statuses = diagnostics.get('wave_statuses', {})
    
    # Count waves by status - using updated status categories
    status_counts = {
        "Full": 0,
        "Partial": 0,
        "Operational": 0,
        "Degraded": 0,
        "Unavailable": 0
    }
    
    for wave, info in wave_statuses.items():
        status = info['status']
        # Map statuses to our categories
        if status in ["Full", "Ready"]:
            status_counts["Full"] += 1
        elif status == "Partial":
            status_counts["Partial"] += 1
        elif status == "Operational":
            status_counts["Operational"] += 1
        elif status in ["Degraded", "Degraded (Rate Limited)", "Degraded (Partial Data)"]:
            status_counts["Degraded"] += 1
        else:  # Unavailable, Missing Inputs, Error, etc.
            status_counts["Unavailable"] += 1
    
    status_col1, status_col2, status_col3, status_col4, status_col5 = st.columns(5)
    
    with status_col1:
        st.metric(
            label="üü¢ Full",
            value=status_counts["Full"],
            help="All analytics available, complete data"
        )
    
    with status_col2:
        st.metric(
            label="üîµ Partial",
            value=status_counts["Partial"],
            help="Basic analytics available, some limitations"
        )
    
    with status_col3:
        st.metric(
            label="üü° Operational",
            value=status_counts["Operational"],
            help="Current pricing available, minimal analytics"
        )
    
    with status_col4:
        st.metric(
            label="üü† Degraded",
            value=status_counts["Degraded"],
            help="Limited data, still visible with diagnostics"
        )
    
    with status_col5:
        st.metric(
            label="üî¥ Unavailable",
            value=status_counts["Unavailable"],
            help="Critical data missing, visible with diagnostics only"
        )
    
    # Display detailed wave status table
    if wave_statuses:
        st.markdown("##### Wave Status Details")
        
        # Create DataFrame for display
        status_rows = []
        for wave, info in wave_statuses.items():
            status_rows.append({
                'Wave Name': wave,
                'Status': info['status'],
                'Reason': info['reason']
            })
        
        status_df = pd.DataFrame(status_rows)
        
        # Sort: Ready first, then Degraded, then Missing Inputs, then Errors
        status_order = {
            "Ready": 1,
            "Degraded (Partial Data)": 2,
            "Degraded (Rate Limited)": 2,
            "Missing Inputs": 3,
            "Error (Computation)": 4
        }
        status_df['sort_order'] = status_df['Status'].map(status_order)
        status_df = status_df.sort_values(['sort_order', 'Wave Name']).drop('sort_order', axis=1)
        
        # Display table
        st.dataframe(
            status_df,
            use_container_width=True,
            hide_index=True,
            height=400
        )
    
    st.divider()
    
    # ========================================================================
    # SECTION 3: DATA FRESHNESS TABLE
    # ========================================================================
    st.markdown("#### üìÖ Data Freshness")
    
    data_freshness = diagnostics.get('data_freshness', [])
    
    if len(data_freshness) == 0:
        st.info("üìä Data unavailable - No freshness information available")
    else:
        # Create DataFrame for display
        freshness_df = pd.DataFrame(data_freshness)
        
        # Add staleness badge with emoji
        def format_staleness(staleness):
            if staleness == 'fresh':
                return "üü¢ Fresh"
            elif staleness == 'recent':
                return "üü° Recent"
            elif staleness == 'stale':
                return "üü† Stale"
            elif staleness == 'old':
                return "üî¥ Old"
            else:
                return "‚ö™ Unavailable"
        
        freshness_df['Staleness Badge'] = freshness_df['staleness'].apply(format_staleness)
        
        # Format days_old
        def format_days_old(days):
            if days is None:
                return "N/A"
            elif days == 0:
                return "Today"
            elif days == 1:
                return "1 day"
            else:
                return f"{days} days"
        
        freshness_df['Age'] = freshness_df['days_old'].apply(format_days_old)
        
        # Prepare display DataFrame
        display_df = freshness_df[['wave', 'latest_date', 'Age', 'Staleness Badge']].copy()
        display_df.columns = ['Wave', 'Latest Date', 'Age', 'Status']
        
        # Display with pagination
        st.dataframe(
            display_df,
            use_container_width=True,
            hide_index=True,
            height=400
        )
        
        # Summary statistics
        st.caption(f"Showing {len(display_df)} waves ‚Ä¢ Fresh: üü¢ (today) ‚Ä¢ Recent: üü° (1-3 days) ‚Ä¢ Stale: üü† (4-7 days) ‚Ä¢ Old: üî¥ (>7 days)")
    
    st.divider()
    
    # ========================================================================
    # SECTION 4: OPERATOR CONTROLS
    # ========================================================================
    st.markdown("#### ‚ö° Operator Controls")
    
    ctrl_col1, ctrl_col2, ctrl_col3 = st.columns(3)
    
    with ctrl_col1:
        if st.button(
            "üîÑ Force Reload Universe",
            use_container_width=True,
            type="primary",
            help="Reload the Wave Universe from source"
        ):
            try:
                # Increment wave universe version
                if "wave_universe_version" not in st.session_state:
                    st.session_state.wave_universe_version = 1
                st.session_state.wave_universe_version += 1
                
                # Clear wave universe cache using constant
                for key in WAVE_UNIVERSE_CACHE_KEYS:
                    if key in st.session_state:
                        del st.session_state[key]
                
                # Set force reload flag
                st.session_state["force_reload_universe"] = True
                
                # Mark user interaction
                st.session_state.user_interaction_detected = True
                
                st.success("‚úÖ Universe reload queued - refreshing...")
                trigger_rerun("force_reload_universe")
            except Exception as e:
                st.error(f"‚ùå Reload failed: {str(e)}")
    
    with ctrl_col2:
        if st.button(
            "üßπ Clear Cache + Recompute",
            use_container_width=True,
            help="Clear all caches and force recomputation"
        ):
            try:
                # Clear all Streamlit caches
                st.cache_data.clear()
                st.cache_resource.clear()
                
                # Clear session state caches using constant
                for key in WAVE_UNIVERSE_CACHE_KEYS:
                    if key in st.session_state:
                        del st.session_state[key]
                
                # Increment wave universe version
                if "wave_universe_version" not in st.session_state:
                    st.session_state.wave_universe_version = 1
                st.session_state.wave_universe_version += 1
                
                # Mark user interaction
                st.session_state.user_interaction_detected = True
                
                st.success("‚úÖ Cache cleared - recomputing...")
                trigger_rerun("clear_cache_recompute")
            except Exception as e:
                st.error(f"‚ùå Cache clear failed: {str(e)}")
    
    with ctrl_col3:
        # Prepare diagnostics CSV
        try:
            # Build comprehensive diagnostics CSV
            csv_rows = []
            
            # Header
            csv_rows.append("Wave Universe Diagnostics Report")
            csv_rows.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            csv_rows.append("")
            
            # Metrics
            csv_rows.append("METRICS")
            csv_rows.append(f"Universe Count,{diagnostics.get('universe_count', 0)}")
            csv_rows.append(f"Active Waves,{diagnostics.get('active_count', 0)}")
            csv_rows.append(f"Data-Ready Waves,{diagnostics.get('data_ready_count', 0)}")
            csv_rows.append(f"History Unique,{diagnostics.get('history_unique_count', 0)}")
            csv_rows.append(f"Missing Waves,{len(diagnostics.get('missing_waves', []))}")
            csv_rows.append(f"Orphan Waves,{len(diagnostics.get('orphan_waves', []))}")
            csv_rows.append(f"Duplicate Waves,{len(diagnostics.get('duplicate_waves', []))}")
            csv_rows.append("")
            
            # Missing Waves
            if diagnostics.get('missing_waves'):
                csv_rows.append("MISSING WAVES (in registry but no data)")
                for wave in diagnostics['missing_waves']:
                    csv_rows.append(f"{wave}")
                csv_rows.append("")
            
            # Orphan Waves
            if diagnostics.get('orphan_waves'):
                csv_rows.append("ORPHAN WAVES (in data but not in registry)")
                for wave in diagnostics['orphan_waves']:
                    csv_rows.append(f"{wave}")
                csv_rows.append("")
            
            # Duplicate Waves
            if diagnostics.get('duplicate_waves'):
                csv_rows.append("DUPLICATE WAVES (removed during deduplication)")
                for wave in diagnostics['duplicate_waves']:
                    csv_rows.append(f"{wave}")
                csv_rows.append("")
            
            # Data Freshness
            if diagnostics.get('data_freshness'):
                csv_rows.append("DATA FRESHNESS")
                csv_rows.append("Wave,Latest Date,Days Old,Staleness")
                for item in diagnostics['data_freshness']:
                    wave = item['wave']
                    latest_date = item['latest_date'] if item['latest_date'] else 'N/A'
                    days_old = item['days_old'] if item['days_old'] is not None else 'N/A'
                    staleness = item['staleness']
                    csv_rows.append(f"{wave},{latest_date},{days_old},{staleness}")
            
            csv_content = "\n".join(csv_rows)
            
            st.download_button(
                label="üì• Download Diagnostics CSV",
                data=csv_content,
                file_name=f"wave_universe_diagnostics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv",
                use_container_width=True,
                help="Download comprehensive diagnostics as CSV"
            )
        except Exception as e:
            st.button(
                "üì• Download Diagnostics CSV",
                disabled=True,
                use_container_width=True,
                help=f"Download unavailable: {str(e)}"
            )


# ============================================================================
# SECTION 4: VISUALIZATION FUNCTIONS
# ============================================================================

def create_wavescore_bar_chart(leaderboard_df):
    """
    Create a horizontal bar chart for WaveScore leaderboard.
    Returns a Plotly figure or None if data unavailable.
    """
    try:
        if leaderboard_df is None or len(leaderboard_df) == 0:
            return None
        
        # Create horizontal bar chart
        fig = go.Figure()
        
        # Add bars with color gradient based on score
        fig.add_trace(go.Bar(
            y=leaderboard_df['Wave'],
            x=leaderboard_df['WaveScore'],
            orientation='h',
            marker=dict(
                color=leaderboard_df['WaveScore'],
                colorscale='RdYlGn',
                showscale=True,
                colorbar=dict(title="WaveScore")
            ),
            text=leaderboard_df['WaveScore'].apply(lambda x: f"{x:.1f}"),
            textposition='auto',
            hovertemplate='<b>%{y}</b><br>WaveScore: %{x:.1f}<extra></extra>'
        ))
        
        fig.update_layout(
            title="Top Performers by WaveScore",
            xaxis_title="WaveScore",
            yaxis_title="Wave",
            height=400,
            showlegend=False,
            yaxis={'categoryorder': 'total ascending'}
        )
        
        return fig
        
    except Exception:
        return None


def create_movers_chart(movers_df):
    """
    Create a waterfall chart showing biggest WaveScore movers.
    Returns a Plotly figure or None if data unavailable.
    """
    try:
        if movers_df is None or len(movers_df) == 0:
            return None
        
        # Create bar chart with color coding for positive/negative changes
        fig = go.Figure()
        
        colors = ['green' if x > 0 else 'red' for x in movers_df['Change']]
        
        fig.add_trace(go.Bar(
            x=movers_df['Wave'],
            y=movers_df['Change'],
            marker_color=colors,
            text=movers_df['Change'].apply(lambda x: f"{x:+.1f}"),
            textposition='outside',
            hovertemplate='<b>%{x}</b><br>Change: %{y:+.1f}<br>Previous: %{customdata[0]:.1f}<br>Current: %{customdata[1]:.1f}<extra></extra>',
            customdata=movers_df[['Previous', 'Current']].values
        ))
        
        fig.update_layout(
            title="Biggest WaveScore Movers (Month-over-Month)",
            xaxis_title="Wave",
            yaxis_title="WaveScore Change",
            height=400,
            showlegend=False
        )
        
        fig.update_xaxes(tickangle=-45)
        
        return fig
        
    except Exception:
        return None


def create_wave_performance_chart(wave_data, wave_name):
    """
    Create a multi-panel chart showing wave performance over time.
    Includes cumulative returns, alpha, and drawdown.
    Returns a Plotly figure or None if data unavailable.
    """
    try:
        if wave_data is None or len(wave_data) == 0:
            return None
        
        if 'date' not in wave_data.columns:
            return None
        
        # Create subplots
        fig = make_subplots(
            rows=3, cols=1,
            subplot_titles=(
                'Cumulative Returns',
                'Daily Alpha',
                'Drawdown'
            ),
            vertical_spacing=0.1,
            row_heights=[0.4, 0.3, 0.3]
        )
        
        # Calculate metrics
        if 'portfolio_return' in wave_data.columns:
            cumulative_returns = (1 + wave_data['portfolio_return']).cumprod() - 1
            
            # Panel 1: Cumulative returns
            fig.add_trace(
                go.Scatter(
                    x=wave_data['date'],
                    y=cumulative_returns * 100,
                    mode='lines',
                    name='Portfolio',
                    line=dict(color='blue', width=2),
                    hovertemplate='%{x|%Y-%m-%d}<br>Return: %{y:.2f}%<extra></extra>'
                ),
                row=1, col=1
            )
            
            if 'benchmark_return' in wave_data.columns:
                cumulative_benchmark = (1 + wave_data['benchmark_return']).cumprod() - 1
                fig.add_trace(
                    go.Scatter(
                        x=wave_data['date'],
                        y=cumulative_benchmark * 100,
                        mode='lines',
                        name='Benchmark',
                        line=dict(color='gray', width=2, dash='dash'),
                        hovertemplate='%{x|%Y-%m-%d}<br>Return: %{y:.2f}%<extra></extra>'
                    ),
                    row=1, col=1
                )
        
        # Panel 2: Daily alpha
        if 'alpha' in wave_data.columns or ('portfolio_return' in wave_data.columns and 'benchmark_return' in wave_data.columns):
            if 'alpha' not in wave_data.columns:
                wave_data = wave_data.copy()
                wave_data['alpha'] = wave_data['portfolio_return'] - wave_data['benchmark_return']
            
            colors = ['green' if x > 0 else 'red' for x in wave_data['alpha']]
            
            fig.add_trace(
                go.Bar(
                    x=wave_data['date'],
                    y=wave_data['alpha'] * 100,
                    name='Alpha',
                    marker_color=colors,
                    hovertemplate='%{x|%Y-%m-%d}<br>Alpha: %{y:.3f}%<extra></extra>'
                ),
                row=2, col=1
            )
        
        # Panel 3: Drawdown
        if 'portfolio_return' in wave_data.columns:
            cumulative_returns = (1 + wave_data['portfolio_return']).cumprod()
            running_max = cumulative_returns.cummax()
            drawdown = (cumulative_returns - running_max) / running_max
            
            fig.add_trace(
                go.Scatter(
                    x=wave_data['date'],
                    y=drawdown * 100,
                    mode='lines',
                    name='Drawdown',
                    fill='tozeroy',
                    line=dict(color='red', width=2),
                    hovertemplate='%{x|%Y-%m-%d}<br>Drawdown: %{y:.2f}%<extra></extra>'
                ),
                row=3, col=1
            )
        
        # Update layout
        fig.update_yaxes(title_text="Cumulative Return (%)", row=1, col=1)
        fig.update_yaxes(title_text="Alpha (%)", row=2, col=1)
        fig.update_yaxes(title_text="Drawdown (%)", row=3, col=1)
        fig.update_xaxes(title_text="Date", row=3, col=1)
        
        fig.update_layout(
            title=f"Performance Analysis: {wave_name}",
            height=800,
            showlegend=True,
            hovermode='x unified'
        )
        
        return fig
        
    except Exception:
        return None


def create_alpha_waterfall_chart(alpha_components, wave_name):
    """
    Create a waterfall chart showing alpha decomposition.
    Returns a Plotly figure or None if data unavailable.
    """
    try:
        if alpha_components is None:
            return None
        
        # Prepare waterfall data
        labels = ['Selection Alpha', 'Overlay Alpha', 'Cash/Risk-Off', 'Total Alpha']
        values = [
            alpha_components['selection_alpha'],
            alpha_components['overlay_alpha'],
            alpha_components['cash_contribution'],
            alpha_components['total_alpha']
        ]
        
        # Create waterfall chart
        fig = go.Figure(go.Waterfall(
            name="Alpha Components",
            orientation="v",
            measure=["relative", "relative", "relative", "total"],
            x=labels,
            y=[v * 100 for v in values],  # Convert to percentage
            text=[f"{v*100:.2f}%" for v in values],
            textposition="outside",
            connector={"line": {"color": "rgb(63, 63, 63)"}},
            decreasing={"marker": {"color": "red"}},
            increasing={"marker": {"color": "green"}},
            totals={"marker": {"color": "blue"}}
        ))
        
        fig.update_layout(
            title=f"Alpha Decomposition: {wave_name}",
            xaxis_title="Component",
            yaxis_title="Alpha (%)",
            height=500,
            showlegend=False
        )
        
        return fig
        
    except Exception:
        return None


def create_correlation_heatmap(correlation_matrix, wave_names):
    """
    Create a correlation heatmap for portfolio waves.
    Returns a Plotly figure or None if data unavailable.
    """
    try:
        if correlation_matrix is None or correlation_matrix.empty:
            return None
        
        # Create heatmap
        fig = go.Figure(data=go.Heatmap(
            z=correlation_matrix.values,
            x=correlation_matrix.columns,
            y=correlation_matrix.index,
            colorscale='RdBu_r',
            zmid=0,
            zmin=-1,
            zmax=1,
            text=correlation_matrix.values,
            texttemplate='%{text:.2f}',
            textfont={"size": 10},
            colorbar=dict(title="Correlation")
        ))
        
        fig.update_layout(
            title="Portfolio Correlation Matrix",
            xaxis_title="Wave",
            yaxis_title="Wave",
            height=500,
            width=600
        )
        
        return fig
        
    except Exception:
        return None


def create_comparison_radar_chart(wave1_metrics, wave2_metrics):
    """
    Create a radar chart comparing two waves across multiple dimensions.
    Returns a Plotly figure or None if data unavailable.
    """
    try:
        if wave1_metrics is None or wave2_metrics is None:
            return None
        
        # Define metrics for comparison (normalized to 0-100 scale)
        categories = []
        wave1_values = []
        wave2_values = []
        
        # WaveScore (already 0-100)
        if wave1_metrics['wavescore'] != 'N/A' and wave2_metrics['wavescore'] != 'N/A':
            categories.append('WaveScore')
            wave1_values.append(wave1_metrics['wavescore'])
            wave2_values.append(wave2_metrics['wavescore'])
        
        # Sharpe Ratio (normalize: assume range -2 to 4, map to 0-100)
        if wave1_metrics['sharpe_ratio'] != 'N/A' and wave2_metrics['sharpe_ratio'] != 'N/A':
            categories.append('Sharpe Ratio')
            wave1_values.append(min(100, max(0, (wave1_metrics['sharpe_ratio'] + 2) * 100 / 6)))
            wave2_values.append(min(100, max(0, (wave2_metrics['sharpe_ratio'] + 2) * 100 / 6)))
        
        # Win Rate (already percentage, scale to 0-100)
        if wave1_metrics['win_rate'] != 'N/A' and wave2_metrics['win_rate'] != 'N/A':
            categories.append('Win Rate')
            wave1_values.append(wave1_metrics['win_rate'] * 100)
            wave2_values.append(wave2_metrics['win_rate'] * 100)
        
        # Returns (normalize to 0-100, assuming -20% to +20% range)
        if wave1_metrics['cumulative_return'] != 'N/A' and wave2_metrics['cumulative_return'] != 'N/A':
            categories.append('Returns')
            wave1_values.append(min(100, max(0, (wave1_metrics['cumulative_return'] + 0.2) * 100 / 0.4)))
            wave2_values.append(min(100, max(0, (wave2_metrics['cumulative_return'] + 0.2) * 100 / 0.4)))
        
        # Risk Control (inverse of max drawdown, normalize)
        if wave1_metrics['max_drawdown'] != 'N/A' and wave2_metrics['max_drawdown'] != 'N/A':
            categories.append('Risk Control')
            # Less negative drawdown is better, convert to 0-100 scale
            wave1_values.append(min(100, max(0, (1 + wave1_metrics['max_drawdown']) * 100)))
            wave2_values.append(min(100, max(0, (1 + wave2_metrics['max_drawdown']) * 100)))
        
        if len(categories) == 0:
            return None
        
        fig = go.Figure()
        
        fig.add_trace(go.Scatterpolar(
            r=wave1_values,
            theta=categories,
            fill='toself',
            name=wave1_metrics['name'],
            line=dict(color='blue')
        ))
        
        fig.add_trace(go.Scatterpolar(
            r=wave2_values,
            theta=categories,
            fill='toself',
            name=wave2_metrics['name'],
            line=dict(color='red')
        ))
        
        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 100]
                )
            ),
            title="Multi-Dimensional Comparison",
            height=500,
            showlegend=True
        )
        
        return fig
        
    except Exception:
        return None


def create_correlation_heatmap(wave1_data, wave2_data, wave1_name, wave2_name):
    """
    Create a correlation matrix heatmap for two waves.
    Returns a Plotly figure or None if data unavailable.
    """
    try:
        if wave1_data is None or wave2_data is None:
            return None
        
        if 'date' not in wave1_data.columns or 'date' not in wave2_data.columns:
            return None
        
        if 'portfolio_return' not in wave1_data.columns or 'portfolio_return' not in wave2_data.columns:
            return None
        
        # Merge data on date
        wave1_returns = wave1_data[['date', 'portfolio_return']].rename(columns={'portfolio_return': wave1_name})
        wave2_returns = wave2_data[['date', 'portfolio_return']].rename(columns={'portfolio_return': wave2_name})
        
        merged = pd.merge(wave1_returns, wave2_returns, on='date', how='inner')
        
        if len(merged) < 2:
            return None
        
        # Calculate correlation matrix
        corr_matrix = merged[[wave1_name, wave2_name]].corr()
        
        # Create heatmap
        fig = go.Figure(data=go.Heatmap(
            z=corr_matrix.values,
            x=corr_matrix.columns,
            y=corr_matrix.index,
            colorscale='RdBu',
            zmid=0,
            zmin=-1,
            zmax=1,
            text=corr_matrix.values,
            texttemplate='%{text:.3f}',
            textfont={"size": 16},
            colorbar=dict(title="Correlation")
        ))
        
        fig.update_layout(
            title="Return Correlation Matrix",
            height=400,
            width=500
        )
        
        return fig
        
    except Exception:
        return None


# ============================================================================
# SECTION 5: DATA PROCESSING FUNCTIONS
# ============================================================================

def count_waves_with_valid_price_book_data(canonical_waves, enabled_flags, price_book):
    """
    Count waves that have valid PRICE_BOOK data using the same validation logic
    as the performance table and readiness checks.
    
    Args:
        canonical_waves: List of all wave names
        enabled_flags: Dict of wave -> enabled (True/False)
        price_book: PRICE_BOOK DataFrame
        
    Returns:
        int: Count of enabled waves with valid PRICE_BOOK data
    """
    if price_book is None or price_book.empty:
        return 0
    
    try:
        from helpers.wave_performance import validate_wave_price_history
        
        # Count enabled waves that pass validation
        valid_count = 0
        for wave in canonical_waves:
            # Only check enabled waves
            if not enabled_flags.get(wave, True):
                continue
            
            # Validate wave against PRICE_BOOK
            validation = validate_wave_price_history(
                wave,
                price_book,
                min_coverage_pct=WAVE_VALIDATION_MIN_COVERAGE_PCT,
                min_history_days=WAVE_VALIDATION_MIN_HISTORY_DAYS
            )
            
            if validation.get('valid', False):
                valid_count += 1
        
        return valid_count
    except Exception as e:
        # If validation fails, return 0 for consistency
        # This prevents reporting inaccurate counts when validation can't be performed
        return 0


def get_mission_control_data():
    """
    Retrieve Mission Control metrics from available data.
    Returns dict with all metrics, using 'unknown' for unavailable data.
    Enhanced with additional system health indicators.
    
    NEW DEFINITIONS:
    - universe_count: Total waves in the wave registry
    - active_waves: Waves with enabled=True (not based on data availability)
    - waves_live_count: Waves with valid PRICE_BOOK data (canonical validation)
    """
    mc_data = {
        'market_regime': 'Initializing',
        'vix_gate_status': 'Pending',
        'alpha_today': 'Pending',
        'alpha_30day': 'Pending',
        'wavescore_leader': 'Pending',
        'wavescore_leader_score': 'Pending',
        'data_freshness': 'Initializing',
        'data_age_days': None,
        'total_waves': 0,
        'active_waves': 0,
        'waves_live_count': 0,  # NEW: Waves with valid PRICE_BOOK data
        'system_status': 'Initializing',
        'universe_count': 0,
        'history_unique_count': 0
    }
    
    try:
        df = safe_load_wave_history()
        
        if df is None:
            mc_data['system_status'] = 'Data Unavailable'
            # Still get wave universe count even if no history (Data Backbone V1)
            try:
                wave_universe_version = st.session_state.get("wave_universe_version", 1)
                universe = get_canonical_wave_universe(force_reload=False, _wave_universe_version=wave_universe_version)
                
                canonical_waves = universe.get("waves", [])
                enabled_flags = universe.get("enabled_flags", {})
                
                mc_data['total_waves'] = len(canonical_waves)
                mc_data['universe_count'] = mc_data['total_waves']
                
                # NEW: Active = enabled waves, not data-dependent
                active_count = sum(1 for wave in canonical_waves if enabled_flags.get(wave, True))
                mc_data['active_waves'] = active_count
                
                # Compute waves_live_count from PRICE_BOOK validation
                try:
                    price_book = get_cached_price_book()
                    mc_data['waves_live_count'] = count_waves_with_valid_price_book_data(
                        canonical_waves, enabled_flags, price_book
                    )
                except Exception:
                    # Fallback: if PRICE_BOOK check fails completely, use 0
                    mc_data['waves_live_count'] = 0
            except Exception:
                pass
            return mc_data
        
        # Get latest date from PRICE_BOOK (canonical price source)
        # This ensures Data Age reflects actual live price data, not history file timestamps
        try:
            from helpers.price_book import get_price_book_meta
            price_book = get_cached_price_book()
            price_meta = get_price_book_meta(price_book)
            
            if price_meta['date_max'] is not None:
                # Use latest price date from PRICE_BOOK (UTC-aware calculation)
                # Parse date string as UTC timestamp at midnight
                latest_price_date = pd.Timestamp(price_meta['date_max'], tz='UTC').normalize()
                mc_data['data_freshness'] = price_meta['date_max']
                mc_data['last_price_date'] = price_meta['date_max']
                
                # Calculate data age in days (UTC-aware)
                utc_today = pd.Timestamp.utcnow().normalize()
                age_days = (utc_today - latest_price_date).days
                mc_data['data_age_days'] = age_days
            else:
                # Fallback if PRICE_BOOK is empty
                latest_date = df['date'].max()
                mc_data['data_freshness'] = latest_date.strftime('%Y-%m-%d')
                mc_data['last_price_date'] = latest_date.strftime('%Y-%m-%d')
                # Ensure fallback also uses UTC
                latest_price_date = pd.Timestamp(latest_date, tz='UTC').normalize()
                utc_today = pd.Timestamp.utcnow().normalize()
                age_days = (utc_today - latest_price_date).days
                mc_data['data_age_days'] = age_days
        except Exception:
            # Fallback to wave_history if PRICE_BOOK fails
            latest_date = df['date'].max()
            mc_data['data_freshness'] = latest_date.strftime('%Y-%m-%d')
            mc_data['last_price_date'] = latest_date.strftime('%Y-%m-%d')
            # Ensure fallback also uses UTC
            latest_price_date = pd.Timestamp(latest_date, tz='UTC').normalize()
            utc_today = pd.Timestamp.utcnow().normalize()
            age_days = (utc_today - latest_price_date).days
            mc_data['data_age_days'] = age_days
        
        # Count waves using centralized wave universe (Data Backbone V1)
        try:
            # Get wave universe - single source of truth for all wave lists and counts
            wave_universe_version = st.session_state.get("wave_universe_version", 1)
            universe = get_canonical_wave_universe(force_reload=False, _wave_universe_version=wave_universe_version)
            
            canonical_waves = universe.get("waves", [])
            enabled_flags = universe.get("enabled_flags", {})
            
            mc_data['total_waves'] = len(canonical_waves)
            mc_data['universe_count'] = len(canonical_waves)
            
            # NEW: Active = enabled waves, not data-dependent
            active_count = sum(1 for wave in canonical_waves if enabled_flags.get(wave, True))
            mc_data['active_waves'] = active_count
            
            # Count unique waves in historical data
            if 'wave' in df.columns:
                history_waves_unique = df['wave'].nunique()
                mc_data['history_unique_count'] = history_waves_unique
                
                # Compute waves_live_count from PRICE_BOOK validation
                # This counts active waves that have valid price data in PRICE_BOOK
                try:
                    from helpers.price_book import get_price_book
                    price_book = get_cached_price_book()
                    mc_data['waves_live_count'] = count_waves_with_valid_price_book_data(
                        canonical_waves, enabled_flags, price_book
                    )
                except Exception:
                    # Fallback: if PRICE_BOOK check fails completely, use 0
                    mc_data['waves_live_count'] = 0
            else:
                # No wave column in history
                mc_data['history_unique_count'] = 0
                # Compute waves_live_count from PRICE_BOOK
                try:
                    from helpers.price_book import get_price_book
                    price_book = get_cached_price_book()
                    mc_data['waves_live_count'] = count_waves_with_valid_price_book_data(
                        canonical_waves, enabled_flags, price_book
                    )
                except Exception:
                    mc_data['waves_live_count'] = 0
        except Exception:
            # Fallback to old method if canonical universe fails
            if 'wave' in df.columns:
                mc_data['total_waves'] = df['wave'].nunique()
                mc_data['universe_count'] = mc_data['total_waves']
                mc_data['history_unique_count'] = mc_data['total_waves']
                # Assume all waves are enabled in fallback mode
                mc_data['active_waves'] = mc_data['total_waves']
                
                # Compute waves_live_count from PRICE_BOOK
                # In fallback mode, we don't have canonical_waves/enabled_flags,
                # so we'll use a simpler check
                try:
                    from helpers.price_book import get_price_book
                    from helpers.wave_performance import compute_all_waves_performance
                    
                    price_book = get_cached_price_book()
                    if not price_book.empty:
                        # Count validated waves by computing performance
                        perf_df = compute_all_waves_performance(
                            price_book, 
                            periods=[1], 
                            only_validated=True
                        )
                        mc_data['waves_live_count'] = len(perf_df)
                    else:
                        mc_data['waves_live_count'] = 0
                except Exception:
                    # Ultimate fallback: use 0
                    mc_data['waves_live_count'] = 0
        
        # System status and data age based on PRICE_BOOK (not wave_history)
        # Use compute_system_health from price_book for consistent thresholds
        try:
            from helpers.price_book import (
                get_price_book, 
                get_price_book_meta,
                compute_system_health,
                STALE_DAYS_THRESHOLD,
                DEGRADED_DAYS_THRESHOLD
            )
            
            price_book = get_cached_price_book()
            health = compute_system_health(price_book)
            meta = get_price_book_meta(price_book)
            
            # Use PRICE_BOOK metadata for data age and freshness
            mc_data['data_freshness'] = meta.get('date_max', 'unknown')
            mc_data['data_age_days'] = health.get('days_stale', None)
            
            # Map health status to system status
            health_status = health.get('health_status', 'Initializing')
            if health_status == 'OK':
                mc_data['system_status'] = 'Excellent'
            elif health_status == 'DEGRADED':
                mc_data['system_status'] = 'Fair'
            elif health_status == 'STALE':
                mc_data['system_status'] = 'Stale'
            else:
                mc_data['system_status'] = 'Initializing'
                
        except Exception:
            # Fallback to wave_history age if PRICE_BOOK check fails
            if age_days <= 1:
                mc_data['system_status'] = 'Excellent'
            elif age_days <= 3:
                mc_data['system_status'] = 'Good'
            elif age_days <= 7:
                mc_data['system_status'] = 'Fair'
            else:
                mc_data['system_status'] = 'Stale'
        
        # Calculate Market Regime based on recent returns
        recent_days = 5
        recent_data = df[df['date'] >= (latest_date - timedelta(days=recent_days))]
        
        if 'portfolio_return' in df.columns and len(recent_data) > 0:
            avg_return = recent_data['portfolio_return'].mean()
            volatility = recent_data['portfolio_return'].std()
            
            # Enhanced regime detection
            if avg_return > 0.005:
                if volatility < 0.015:
                    mc_data['market_regime'] = 'Risk-On (Stable)'
                else:
                    mc_data['market_regime'] = 'Risk-On (Volatile)'
            elif avg_return < -0.005:
                if volatility > 0.02:
                    mc_data['market_regime'] = 'Risk-Off (Volatile)'
                else:
                    mc_data['market_regime'] = 'Risk-Off (Stable)'
            else:
                mc_data['market_regime'] = 'Neutral'
        
        # VIX Gate Status - check for VIX execution state in wave_history
        # This ensures VIX overlay is marked as LIVE when daily execution records exist
        try:
            # First, check if wave_history has VIX execution state columns
            has_vix_state = False
            if 'overlay_active' in df.columns and 'vix_level' in df.columns:
                # Check if any equity wave has active overlay with valid VIX data
                latest_data = df[df['date'] == latest_date]
                if len(latest_data) > 0:
                    active_overlays = latest_data[
                        (latest_data['overlay_active'] == True) & 
                        (latest_data['vix_level'].notna())
                    ]
                    has_vix_state = len(active_overlays) > 0
            
            if has_vix_state:
                # VIX execution state is persisted in wave_history - overlay is LIVE
                # Get VIX level from latest execution state
                vix_data = latest_data[latest_data['vix_level'].notna()]
                if len(vix_data) > 0:
                    current_vix = vix_data['vix_level'].iloc[0]
                    if current_vix < RISK_REGIME_VIX_LOW:
                        mc_data['vix_gate_status'] = f'LIVE - GREEN ({current_vix:.1f})'
                    elif current_vix < RISK_REGIME_VIX_HIGH:
                        mc_data['vix_gate_status'] = f'LIVE - YELLOW ({current_vix:.1f})'
                    else:
                        mc_data['vix_gate_status'] = f'LIVE - RED ({current_vix:.1f})'
                else:
                    mc_data['vix_gate_status'] = 'LIVE (No VIX Data Today)'
            elif 'VIX' in price_book.columns and not price_book['VIX'].dropna().empty:
                # Fall back to checking price_book if wave_history doesn't have VIX state
                vix_prices = price_book['VIX'].dropna()
                current_vix = vix_prices.iloc[-1] if len(vix_prices) > 0 else None
                
                if current_vix is not None:
                    if current_vix < RISK_REGIME_VIX_LOW:
                        mc_data['vix_gate_status'] = f'GREEN ({current_vix:.1f})'
                    elif current_vix < RISK_REGIME_VIX_HIGH:
                        mc_data['vix_gate_status'] = f'YELLOW ({current_vix:.1f})'
                    else:
                        mc_data['vix_gate_status'] = f'RED ({current_vix:.1f})'
                else:
                    mc_data['vix_gate_status'] = 'Pending'
            else:
                # VIX data not available - do not estimate from volatility
                mc_data['vix_gate_status'] = 'Pending'
        except Exception as e:
            mc_data['vix_gate_status'] = 'Pending'
        
        # Calculate Alpha metrics
        if 'portfolio_return' in df.columns and 'benchmark_return' in df.columns:
            df['alpha'] = df['portfolio_return'] - df['benchmark_return']
            
            # Today's alpha
            today_data = df[df['date'] == latest_date]
            if len(today_data) > 0:
                alpha_today = today_data['alpha'].mean()
                mc_data['alpha_today'] = f"{alpha_today*100:.2f}%"
            
            # 30-day cumulative alpha
            days_30_ago = latest_date - timedelta(days=30)
            last_30_days = df[df['date'] >= days_30_ago]
            if len(last_30_days) > 0:
                alpha_30day = last_30_days['alpha'].sum()
                mc_data['alpha_30day'] = f"{alpha_30day*100:.2f}%"
        
        # WaveScore Leader
        if 'wave' in df.columns and 'alpha' in df.columns:
            days_30_ago = latest_date - timedelta(days=30)
            last_30_days = df[df['date'] >= days_30_ago]
            
            wave_performance = last_30_days.groupby('wave')['alpha'].sum().sort_values(ascending=False)
            
            if len(wave_performance) > 0:
                top_wave = wave_performance.index[0]
                top_alpha = wave_performance.iloc[0]
                
                wavescore = min(100, max(0, (top_alpha * 1000) + 50))
                
                mc_data['wavescore_leader'] = top_wave
                mc_data['wavescore_leader_score'] = f"{wavescore:.1f}"
        
    except Exception:
        pass
    
    return mc_data


def get_wavescore_leaderboard(portfolio_snapshot=None):
    """
    Get top 10 waves by WaveScore (30-day cumulative alpha).
    
    UPDATED: Now uses portfolio_snapshot from session state instead of reloading wave_history.csv
    This avoids unnecessary CSV reads and uses pre-computed snapshot data.
    
    Args:
        portfolio_snapshot: DataFrame from st.session_state["portfolio_snapshot"] with Alpha_30D column
                           If None, falls back to legacy wave_history loading
    
    Returns a DataFrame with wave names and scores, or None if unavailable.
    """
    try:
        # UPDATED: Use portfolio_snapshot if available
        if portfolio_snapshot is not None and not portfolio_snapshot.empty:
            # Use Alpha_30D column from snapshot for WaveScore calculation
            if 'Alpha_30D' not in portfolio_snapshot.columns or 'Wave' not in portfolio_snapshot.columns:
                # Fall back to legacy method if required columns missing
                portfolio_snapshot = None
            else:
                # Filter to waves with valid Alpha_30D data
                valid_waves = portfolio_snapshot[portfolio_snapshot['Alpha_30D'].notna()].copy()
                
                if len(valid_waves) == 0:
                    return None
                
                # Use Alpha_30D column from snapshot for WaveScore calculation
                # WaveScore formula: (Alpha_30D * 1000) + 50, clamped to 0-100
                # Multiplier of 1000 converts alpha decimal (e.g., 0.05) to points (50)
                # Base of 50 centers the score around mid-range
                # Range [0,100] provides intuitive percentage-like score
                valid_waves['WaveScore'] = (valid_waves['Alpha_30D'] * 1000) + 50
                valid_waves['WaveScore'] = valid_waves['WaveScore'].clip(0, 100)
                
                # Sort and get top 10
                leaderboard_df = valid_waves[['Wave', 'WaveScore']].sort_values(
                    'WaveScore', ascending=False
                ).head(10).copy()
                leaderboard_df['Rank'] = range(1, len(leaderboard_df) + 1)
                leaderboard_df = leaderboard_df[['Rank', 'Wave', 'WaveScore']]
                
                return leaderboard_df
        
        # LEGACY PATH: Fall back to wave_history if portfolio_snapshot not provided
        df = get_wave_data_filtered(wave_name=None, days=30)
        
        if df is None:
            return None
        
        if 'wave' not in df.columns:
            return None
        
        # Calculate alpha
        if 'portfolio_return' in df.columns and 'benchmark_return' in df.columns:
            df['alpha'] = df['portfolio_return'] - df['benchmark_return']
        else:
            return None
        
        # Calculate WaveScore for each wave
        wave_scores = []
        for wave in df['wave'].unique():
            wave_data = df[df['wave'] == wave]
            score = calculate_wavescore(wave_data)
            wave_scores.append({'Wave': wave, 'WaveScore': score})
        
        if len(wave_scores) == 0:
            return None
        
        # Create DataFrame and sort by score
        leaderboard_df = pd.DataFrame(wave_scores)
        leaderboard_df = leaderboard_df.sort_values('WaveScore', ascending=False).head(10)
        leaderboard_df['Rank'] = range(1, len(leaderboard_df) + 1)
        leaderboard_df = leaderboard_df[['Rank', 'Wave', 'WaveScore']]
        
        return leaderboard_df
        
    except Exception:
        return None


def get_biggest_movers():
    """
    Get biggest month-over-month WaveScore changes.
    Returns a DataFrame with wave names and score changes, or None if unavailable.
    """
    try:
        df = safe_load_wave_history()
        
        if df is None:
            return None
        
        if 'wave' not in df.columns or 'date' not in df.columns:
            return None
        
        # Calculate alpha
        if 'portfolio_return' in df.columns and 'benchmark_return' in df.columns:
            df['alpha'] = df['portfolio_return'] - df['benchmark_return']
        else:
            return None
        
        latest_date = df['date'].max()
        
        # Get last 30 days (current period)
        days_30_ago = latest_date - timedelta(days=30)
        current_period = df[df['date'] >= days_30_ago]
        
        # Get previous 30 days (30-60 days ago)
        days_60_ago = latest_date - timedelta(days=60)
        previous_period = df[(df['date'] >= days_60_ago) & (df['date'] < days_30_ago)]
        
        if len(current_period) == 0 or len(previous_period) == 0:
            return None
        
        # Calculate WaveScores for both periods
        movers = []
        waves = set(current_period['wave'].unique()) & set(previous_period['wave'].unique())
        
        for wave in waves:
            current_data = current_period[current_period['wave'] == wave]
            previous_data = previous_period[previous_period['wave'] == wave]
            
            current_score = calculate_wavescore(current_data)
            previous_score = calculate_wavescore(previous_data)
            
            change = current_score - previous_score
            
            movers.append({
                'Wave': wave,
                'Previous': previous_score,
                'Current': current_score,
                'Change': change
            })
        
        if len(movers) == 0:
            return None
        
        # Create DataFrame and sort by absolute change
        movers_df = pd.DataFrame(movers)
        movers_df = movers_df.sort_values('Change', ascending=False, key=abs).head(10)
        
        return movers_df
        
    except Exception:
        return None


def get_system_alerts():
    """
    Generate system alerts based on data quality and risk signals.
    Returns a list of alert dictionaries with severity and message.
    """
    alerts = []
    
    try:
        df = safe_load_wave_history()
        
        if df is None:
            alerts.append({
                'severity': 'error',
                'message': 'Wave history data file not found or invalid'
            })
            return alerts
        
        if len(df) == 0:
            alerts.append({
                'severity': 'error',
                'message': 'Wave history data is empty'
            })
            return alerts
        
        latest_date = df['date'].max()
        
        # Data freshness alert
        age_days = (datetime.now() - latest_date).days
        if age_days > 7:
            alerts.append({
                'severity': 'warning',
                'message': f'Data is {age_days} days old - consider updating'
            })
        elif age_days > 2:
            alerts.append({
                'severity': 'info',
                'message': f'Data is {age_days} days old'
            })
        
        # Calculate alpha for remaining checks
        if 'portfolio_return' in df.columns and 'benchmark_return' in df.columns:
            df['alpha'] = df['portfolio_return'] - df['benchmark_return']
            
            # Check for data gaps
            date_range = pd.date_range(start=df['date'].min(), end=df['date'].max(), freq='D')
            actual_dates = df['date'].unique()
            missing_dates = len(date_range) - len(actual_dates)
            
            if missing_dates > 10:
                alerts.append({
                    'severity': 'warning',
                    'message': f'{missing_dates} days of data missing in date range'
                })
            
            # Check for high volatility
            last_30_days = df[df['date'] >= (latest_date - timedelta(days=30))]
            if len(last_30_days) > 0:
                volatility = last_30_days['alpha'].std()
                if volatility > 0.05:  # 5% daily volatility threshold
                    alerts.append({
                        'severity': 'warning',
                        'message': f'High volatility detected: {volatility*100:.2f}% (potential drawdown risk)'
                    })
            
            # Check for negative cumulative alpha
            if len(last_30_days) > 0:
                last_30_days_cumulative_alpha = last_30_days['alpha'].sum()
                if last_30_days_cumulative_alpha < -0.05:  # -5% cumulative underperformance
                    alerts.append({
                        'severity': 'warning',
                        'message': f'Significant underperformance: {last_30_days_cumulative_alpha*100:.2f}% cumulative alpha'
                    })
        
        # If no alerts, add an all-clear message
        if len(alerts) == 0:
            alerts.append({
                'severity': 'success',
                'message': 'All systems operational'
            })
        
    except Exception as e:
        alerts.append({
            'severity': 'error',
            'message': f'Error checking system status: {str(e)}'
        })
    
    return alerts


def generate_wave_narrative(wave_name, wave_data):
    """
    Generate an institutional narrative for a Wave - Vector Explain v2.
    Enhanced with Alpha Proof components and observed contributions.
    
    Args:
        wave_name: Name of the wave
        wave_data: DataFrame containing wave performance data
        
    Returns:
        String containing the narrative
    """
    narrative_parts = []
    
    # Header
    narrative_parts.append(f"# Institutional Narrative: {wave_name} (Vector Explain v2)")
    narrative_parts.append("")
    
    try:
        if wave_data is None or len(wave_data) == 0:
            narrative_parts.append("**Data Status:** Insufficient data available for analysis.")
            return "\n".join(narrative_parts)
        
        # Calculate key metrics
        wave_data = wave_data.copy()
        
        if 'portfolio_return' not in wave_data.columns or 'benchmark_return' not in wave_data.columns:
            narrative_parts.append("**Data Status:** Required return data unavailable.")
            return "\n".join(narrative_parts)
        
        wave_data['alpha'] = wave_data['portfolio_return'] - wave_data['benchmark_return']
        
        # What Happened section
        narrative_parts.append("## What Happened")
        
        start_date = wave_data['date'].min().strftime("%Y-%m-%d")
        end_date = wave_data['date'].max().strftime("%Y-%m-%d")
        num_days = len(wave_data)
        
        narrative_parts.append(f"Over the period from {start_date} to {end_date} ({num_days} trading days), {wave_name} generated the following performance:")
        
        cumulative_return = wave_data['portfolio_return'].sum()
        cumulative_benchmark = wave_data['benchmark_return'].sum()
        cumulative_alpha = wave_data['alpha'].sum()
        
        narrative_parts.append(f"- Portfolio Return: {cumulative_return*100:.2f}%")
        narrative_parts.append(f"- Benchmark Return: {cumulative_benchmark*100:.2f}%")
        narrative_parts.append(f"- Alpha Generated: {cumulative_alpha*100:.2f}%")
        narrative_parts.append("")
        
        # Alpha Proof Section (NEW in v2)
        narrative_parts.append("## Alpha Proof - Observed Contributions")
        narrative_parts.append("")
        narrative_parts.append("Breaking down alpha into actionable components based on available data:")
        narrative_parts.append("")
        
        # Calculate alpha components
        alpha_components = calculate_alpha_components(wave_data, wave_name)
        
        if alpha_components:
            narrative_parts.append(f"**1. Selection Alpha:** {alpha_components['selection_alpha']*100:.2f}%")
            narrative_parts.append("   - Wave return vs benchmark return differential")
            narrative_parts.append("   - Reflects stock/asset selection effectiveness")
            narrative_parts.append("")
            
            narrative_parts.append(f"**2. Overlay Alpha:** {alpha_components['overlay_alpha']*100:.2f}%")
            narrative_parts.append("   - Impact of exposure scaling and VIX gates")
            
            # Check if exposure data is available
            if 'exposure' in wave_data.columns:
                avg_exposure = wave_data['exposure'].mean()
                narrative_parts.append(f"   - Average exposure: {avg_exposure*100:.1f}%")
            else:
                narrative_parts.append("   - Exposure data not available; contribution estimated")
            narrative_parts.append("")
            
            narrative_parts.append(f"**3. Cash/Risk-Off Contribution:** {alpha_components['cash_contribution']*100:.2f}%")
            narrative_parts.append("   - Contributions from moving capital into cash/risk-off positions")
            narrative_parts.append("   - Represents defensive positioning value")
            narrative_parts.append("")
            
            narrative_parts.append("*Note: These are observed contributions based on available data. ")
            narrative_parts.append("Components may not sum exactly to total alpha due to estimation when full exposure data is unavailable.*")
        else:
            narrative_parts.append("**Alpha decomposition unavailable** - insufficient data to separate components.")
            narrative_parts.append("")
            narrative_parts.append("*Total alpha observed: ")
            narrative_parts.append(f"{cumulative_alpha*100:.2f}% over the period, but component breakdown requires additional data fields.*")
        
        narrative_parts.append("")
        
        # Drivers of Alpha section
        narrative_parts.append("## Drivers of Alpha")
        
        avg_daily_alpha = wave_data['alpha'].mean()
        positive_days = len(wave_data[wave_data['alpha'] > 0])
        total_days = len(wave_data)
        win_rate = (positive_days / total_days * 100) if total_days > 0 else 0
        
        narrative_parts.append(f"The wave demonstrated an average daily alpha of {avg_daily_alpha*100:.4f}%, with positive alpha on {positive_days} of {total_days} trading days ({win_rate:.1f}% win rate).")
        
        # Identify best and worst days
        best_day = wave_data.loc[wave_data['alpha'].idxmax()]
        worst_day = wave_data.loc[wave_data['alpha'].idxmin()]
        
        narrative_parts.append(f"- Best performance: {best_day['alpha']*100:.2f}% alpha on {best_day['date'].strftime('%Y-%m-%d')}")
        narrative_parts.append(f"- Worst performance: {worst_day['alpha']*100:.2f}% alpha on {worst_day['date'].strftime('%Y-%m-%d')}")
        narrative_parts.append("")
        
        # Overall Risk Posture section
        narrative_parts.append("## Overall Risk Posture")
        
        volatility = wave_data['portfolio_return'].std()
        sharpe_ratio = (avg_daily_alpha / volatility * np.sqrt(252)) if volatility > 0 else 0
        
        narrative_parts.append(f"Daily return volatility: {volatility*100:.2f}%")
        narrative_parts.append(f"Annualized Sharpe Ratio (estimated): {sharpe_ratio:.2f}")
        
        # Calculate drawdown
        cumulative_returns = (1 + wave_data['portfolio_return']).cumprod()
        running_max = cumulative_returns.cummax()
        drawdown = (cumulative_returns - running_max) / running_max
        max_drawdown = drawdown.min()
        
        narrative_parts.append(f"Maximum drawdown: {max_drawdown*100:.2f}%")
        
        # Risk assessment
        if volatility < 0.01:
            risk_level = "LOW"
        elif volatility < 0.02:
            risk_level = "MODERATE"
        else:
            risk_level = "HIGH"
        
        narrative_parts.append(f"Risk Level: {risk_level}")
        narrative_parts.append("")
        
        # Recommended Action section
        narrative_parts.append("## Recommended Action Language")
        
        if cumulative_alpha > 0.05:  # >5% cumulative alpha
            if risk_level == "LOW":
                action = "STRONG BUY - Wave is generating significant alpha with controlled risk. Consider increasing allocation."
            elif risk_level == "MODERATE":
                action = "BUY - Wave is generating strong alpha but monitor volatility. Maintain or slightly increase position."
            else:
                action = "HOLD - Wave is generating alpha but with elevated volatility. Monitor closely before increasing exposure."
        elif cumulative_alpha > 0.01:  # >1% cumulative alpha
            action = "HOLD - Wave is generating positive but modest alpha. Maintain current allocation and monitor."
        elif cumulative_alpha > -0.01:  # Between -1% and 1%
            action = "NEUTRAL - Wave is performing in line with benchmark. Review strategy and consider alternatives."
        else:  # <-1% cumulative alpha
            action = "REDUCE - Wave is underperforming benchmark. Consider reducing allocation or investigating root causes."
        
        narrative_parts.append(action)
        narrative_parts.append("")
        
        # Data Quality Note
        narrative_parts.append("---")
        narrative_parts.append(f"*Analysis based on {num_days} days of data from {start_date} to {end_date}.*")
        narrative_parts.append("*Vector Explain v2 includes Alpha Proof component references and observed contributions.*")
        
    except Exception as e:
        narrative_parts.append(f"**Error generating narrative:** {str(e)}")
        narrative_parts.append("**Data Status:** Some required fields may be unavailable.")
    
    return "\n".join(narrative_parts)


def get_wave_comparison_data(wave1_name, wave2_name):
    """
    Retrieve comparison data for two waves.
    Returns a dictionary with comparison metrics or None if unavailable.
    """
    try:
        wave1_data = get_wave_data_filtered(wave_name=wave1_name, days=30)
        wave2_data = get_wave_data_filtered(wave_name=wave2_name, days=30)
        
        if wave1_data is None or wave2_data is None:
            return None
        
        # Calculate metrics for Wave 1
        wave1_metrics = calculate_wave_metrics(wave1_data)
        wave1_metrics['name'] = wave1_name
        
        # Calculate metrics for Wave 2
        wave2_metrics = calculate_wave_metrics(wave2_data)
        wave2_metrics['name'] = wave2_name
        
        # Calculate correlation
        correlation = calculate_wave_correlation(wave1_data, wave2_data)
        
        return {
            'wave1': wave1_metrics,
            'wave2': wave2_metrics,
            'correlation': correlation
        }
        
    except Exception:
        return None


# ============================================================================
# SECTION 6: REUSABLE UI COMPONENTS
# ============================================================================

def render_audit_trail_panel():
    """
    Render the Immutable Audit Trail Panel.
    Always accessible for auditing purposes.
    """
    st.markdown("### üìã Immutable Audit Trail")
    st.caption("Complete transparency log of all calculations performed")
    
    engine = get_attribution_engine()
    audit_trail = engine.get_audit_trail()
    
    if not audit_trail:
        st.info("üîç No audit trail entries yet. Perform an attribution calculation to generate audit logs.")
        return
    
    # Display latest entry prominently
    latest_entry = audit_trail[-1]
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric(
            "Latest Calculation",
            latest_entry.calculation_type,
            help=f"Wave: {latest_entry.wave_name}"
        )
    
    with col2:
        status_emoji = "‚úÖ" if latest_entry.success else "‚ùå"
        st.metric(
            "Status",
            f"{status_emoji} {'Success' if latest_entry.success else 'Failed'}",
            help=latest_entry.error_message if not latest_entry.success else "All calculations completed"
        )
    
    with col3:
        st.metric(
            "Timestamp",
            latest_entry.timestamp.strftime("%H:%M:%S"),
            help=latest_entry.timestamp.strftime("%Y-%m-%d %H:%M:%S")
        )
    
    st.divider()
    
    # Full audit trail table
    with st.expander("üìú View Full Audit Trail", expanded=False):
        # Convert audit trail to DataFrame
        audit_data = []
        for entry in reversed(audit_trail):  # Most recent first
            audit_data.append(entry.to_dict())
        
        if audit_data:
            audit_df = pd.DataFrame(audit_data)
            st.dataframe(audit_df, use_container_width=True, hide_index=True)
            
            # Download button
            csv = audit_df.to_csv(index=False)
            st.download_button(
                label="üì• Download Audit Trail (CSV)",
                data=csv,
                file_name=f"audit_trail_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
        else:
            st.info("No audit trail entries available")
    
    # System information
    with st.expander("‚ÑπÔ∏è System Information", expanded=False):
        sys_col1, sys_col2, sys_col3 = st.columns(3)
        
        with sys_col1:
            st.markdown("**Application Version**")
            st.code("v2.0-attribution")
        
        with sys_col2:
            st.markdown("**Git Commit**")
            st.code(get_git_commit_hash())
        
        with sys_col3:
            st.markdown("**Git Branch**")
            st.code(get_git_branch_name())


def render_confidence_band(
    wave_name: str,
    components: DecisionAttributionComponents,
    compact: bool = False
):
    """
    Render confidence band visualization for a wave based on attribution completeness.
    
    Args:
        wave_name: Name of the wave
        components: Attribution components
        compact: If True, render compact version
    """
    confidence_level = components.get_confidence_level()
    completeness = components.data_completeness
    
    # Color mapping
    color_map = {
        "High": "green",
        "Medium": "orange",
        "Low": "red",
        "Very Low": "darkred"
    }
    
    emoji_map = {
        "High": "üü¢",
        "Medium": "üü°",
        "Low": "üü†",
        "Very Low": "üî¥"
    }
    
    color = color_map.get(confidence_level, "gray")
    emoji = emoji_map.get(confidence_level, "‚ö™")
    
    if compact:
        # Compact version for inline display
        st.markdown(
            f"{emoji} **{confidence_level}** ({completeness*100:.0f}%)",
            help=f"Attribution confidence for {wave_name}"
        )
    else:
        # Full version with details
        st.markdown(f"#### {emoji} Confidence: {confidence_level}")
        
        # Progress bar
        st.progress(completeness, text=f"Data Completeness: {completeness*100:.0f}%")
        
        # Component availability
        st.markdown("**Available Components:**")
        
        components_status = [
            ("Selection Alpha", components.selection_available),
            ("Overlay Alpha", components.overlay_available),
            ("Risk-Off Alpha", components.risk_off_available),
            ("Residual Alpha", components.residual_available)
        ]
        
        for comp_name, available in components_status:
            status = "‚úÖ Observed" if available else "‚ùå Unavailable"
            st.markdown(f"- {comp_name}: {status}")
        
        # Warnings
        if components.warnings:
            st.warning(f"‚ö†Ô∏è {len(components.warnings)} warning(s)")
            with st.expander("View Warnings"):
                for warning in components.warnings:
                    st.markdown(f"- {warning}")


def render_decision_attribution_panel(wave_name: str, wave_data: pd.DataFrame):
    """
    Render the Decision Attribution Panel for a specific wave.
    Shows observable components decomposition with confidence bands.
    """
    st.markdown(f"### üéØ Decision Attribution - {wave_name}")
    st.caption("Observable performance decomposition with full transparency")
    
    # Check analytics readiness
    try:
        from waves_engine import get_wave_id_from_display_name
        from analytics_pipeline import compute_data_ready_status, MIN_COVERAGE_FOR_ANALYTICS, MIN_DAYS_FOR_ANALYTICS
        
        wave_id = get_wave_id_from_display_name(wave_name)
        if wave_id:
            diagnostics = compute_data_ready_status(wave_id)
            analytics_ready = diagnostics.get('analytics_ready', False)
            
            if not analytics_ready:
                # Show warning about limited analytics
                coverage_pct = diagnostics.get('coverage_pct', 0)
                history_days = diagnostics.get('history_days', 0)
                missing_tickers = diagnostics.get('missing_tickers', [])
                
                st.warning(
                    f"‚ö†Ô∏è **Analytics Limited**: Coverage {coverage_pct:.1f}% / History {history_days} days\n\n"
                    f"Analytics require ‚â•{MIN_COVERAGE_FOR_ANALYTICS*100:.0f}% coverage and ‚â•{MIN_DAYS_FOR_ANALYTICS} days history for reliable results."
                )
                
                if missing_tickers:
                    with st.expander("üìã Missing Tickers", expanded=False):
                        st.write(f"**{len(missing_tickers)} tickers missing:**")
                        st.write(', '.join(missing_tickers[:20]))
                        if len(missing_tickers) > 20:
                            st.write(f"... and {len(missing_tickers)-20} more")
                        
                        st.markdown("**üí° To enable full analytics:**")
                        st.markdown(f"- Add these tickers to `prices.csv`: `{', '.join(missing_tickers[:5])}`")
                        if wave_id:
                            st.markdown(f"- Run: `python analytics_pipeline.py --wave {wave_id}`")
                
                st.divider()
    except Exception:
        pass  # Silently continue if diagnostics fail
    
    try:
        # Compute attribution
        engine = get_attribution_engine()
        components = engine.compute_attribution(wave_data, wave_name)
        
        # Display confidence band
        conf_col1, conf_col2 = st.columns([2, 1])
        
        with conf_col1:
            st.markdown("#### Attribution Confidence")
            render_confidence_band(wave_name, components, compact=False)
        
        with conf_col2:
            st.markdown("#### Reconciliation")
            if components.reconciled:
                st.success("‚úÖ Reconciled")
                st.caption(f"Error: {components.reconciliation_error*100:.4f}%")
            else:
                st.warning("‚ö†Ô∏è Partial")
                if components.reconciliation_error is not None:
                    st.caption(f"Error: {components.reconciliation_error*100:.4f}%")
        
        st.divider()
        
        # Components breakdown
        st.markdown("#### üìä Observable Components")
        
        # Create visualization data
        component_names = []
        component_values = []
        component_status = []
        
        if components.selection_alpha is not None:
            component_names.append("Selection Alpha")
            component_values.append(components.selection_alpha * 100)
            component_status.append("Observed")
        else:
            component_names.append("Selection Alpha")
            component_values.append(0)
            component_status.append("Unavailable")
        
        if components.overlay_alpha is not None:
            component_names.append("Overlay Alpha\n(VIX Gates)")
            component_values.append(components.overlay_alpha * 100)
            component_status.append("Observed")
        else:
            component_names.append("Overlay Alpha\n(VIX Gates)")
            component_values.append(0)
            component_status.append("Unavailable")
        
        if components.risk_off_alpha is not None:
            component_names.append("Risk-Off Alpha\n(Cash)")
            component_values.append(components.risk_off_alpha * 100)
            component_status.append("Observed")
        else:
            component_names.append("Risk-Off Alpha\n(Cash)")
            component_values.append(0)
            component_status.append("Unavailable")
        
        if components.residual_alpha is not None:
            component_names.append("Residual Alpha")
            component_values.append(components.residual_alpha * 100)
            component_status.append("Observed")
        else:
            component_names.append("Residual Alpha")
            component_values.append(0)
            component_status.append("Unavailable")
        
        # Create bar chart
        colors = ['green' if status == 'Observed' else 'gray' for status in component_status]
        
        fig = go.Figure(data=[
            go.Bar(
                x=component_names,
                y=component_values,
                marker_color=colors,
                text=[f"{val:.2f}%" for val in component_values],
                textposition='outside',
                hovertemplate='<b>%{x}</b><br>Value: %{y:.2f}%<extra></extra>'
            )
        ])
        
        fig.update_layout(
            title="Attribution Components (% Alpha)",
            xaxis_title="Component",
            yaxis_title="Alpha Contribution (%)",
            showlegend=False,
            height=400
        )
        
        safe_plotly_chart(fig, use_container_width=True, key=f"decision_attr_{wave_name}")
        
        # Data table
        st.markdown("#### üìã Component Details")
        
        detail_data = {
            "Component": component_names,
            "Value": [f"{val:.4f}%" if status == "Observed" else "N/A" 
                     for val, status in zip(component_values, component_status)],
            "Status": component_status
        }
        
        detail_df = pd.DataFrame(detail_data)
        st.dataframe(detail_df, use_container_width=True, hide_index=True)
        
        # Total alpha
        if components.total_alpha is not None:
            st.metric(
                "Total Alpha",
                f"{components.total_alpha * 100:.4f}%",
                help="Sum of all observed components"
            )
        
    except Exception as e:
        st.error(f"‚ùå Error computing decision attribution: {str(e)}")
        st.info("üìã The application continues to function. Check audit trail for details.")


def compute_alpha_source_breakdown(df):
    """
    Compute alpha source breakdown using canonical portfolio alpha ledger.
    
    This function uses compute_portfolio_alpha_ledger as the single source of truth,
    ensuring consistency with the Portfolio Snapshot blue box.
    
    Uses 60D period from ledger with strict rolling window semantics.
    No fallback to inception - shows N/A with explicit reasons if unavailable.
    
    Returns dict with:
        - total_alpha: Cumulative alpha (Total) for 60D period
        - selection_alpha: Asset selection alpha for 60D period
        - overlay_alpha: VIX/SafeSmart overlay alpha for 60D period
        - residual: Reconciliation residual (should be near 0)
        - data_available: bool indicating if breakdown is possible
        - diagnostics: dict with diagnostic values for transparency
    """
    result = {
        'total_alpha': None,
        'selection_alpha': None,
        'overlay_alpha': None,
        'residual': None,
        'data_available': False,
        'diagnostics': {}
    }
    
    try:
        # Import the canonical ledger function (single source of truth)
        from helpers.wave_performance import (
            compute_portfolio_alpha_ledger,
            RESIDUAL_TOLERANCE
        )
        from helpers.price_book import get_price_book
        
        # Get PRICE_BOOK
        price_book = get_cached_price_book()
        
        if price_book is None or price_book.empty:
            return result
        
        # Compute portfolio alpha ledger (canonical implementation)
        # This is the SAME ledger used by the blue box
        ledger = compute_portfolio_alpha_ledger(
            price_book=price_book,
            mode=st.session_state.get('selected_mode', 'Standard'),
            periods=[1, 30, 60, 365],  # Match blue box periods
            benchmark_ticker='SPY',
            vix_exposure_enabled=True
        )
        
        if not ledger['success']:
            # Ledger computation failed
            result['diagnostics'] = {
                'period_used': 'N/A',
                'start_date': 'N/A',
                'end_date': 'N/A',
                'rows_used': 0,
                'reason': ledger.get('failure_reason', 'unknown'),
                'using_fallback_exposure': False,
                'exposure_series_found': False,
                'exposure_min': None,
                'exposure_max': None,
                'cum_realized': None,
                'cum_unoverlay': None,
                'cum_benchmark': None
            }
            return result
        
        # Store ledger in session state for consistency (only reached if successful due to early return above)
        st.session_state['portfolio_alpha_ledger'] = ledger
        st.session_state['portfolio_exposure_series'] = ledger.get('daily_exposure')
        
        # Extract 60D period data from ledger
        period_60d = ledger['period_results'].get('60D', {})
        
        # Check if 60D period is available with strict windowing
        if not period_60d.get('available', False):
            # Period unavailable - populate diagnostics with reason
            result['diagnostics'] = {
                'period_used': '60D',
                'start_date': 'N/A',
                'end_date': 'N/A',
                'rows_used': period_60d.get('rows_used', 0),
                'requested_period_days': 60,
                'reason': period_60d.get('reason', 'insufficient_aligned_rows'),
                'using_fallback_exposure': False,
                'exposure_series_found': ledger.get('overlay_available', False),
                'exposure_min': None,
                'exposure_max': None,
                'cum_realized': None,
                'cum_unoverlay': None,
                'cum_benchmark': None
            }
            result['data_available'] = False
            return result
        
        # 60D period is available - extract values
        result['total_alpha'] = period_60d['total_alpha']
        result['selection_alpha'] = period_60d['selection_alpha']
        result['overlay_alpha'] = period_60d['overlay_alpha']
        result['residual'] = period_60d['residual']
        result['data_available'] = True
        
        # Validate residual is within tolerance (using constant from helpers.wave_performance)
        if result['residual'] is not None and abs(result['residual']) > RESIDUAL_TOLERANCE:
            # Large residual - mark as decomposition error
            result['diagnostics'] = {
                'period_used': '60D',
                'start_date': period_60d.get('start_date', 'N/A'),
                'end_date': period_60d.get('end_date', 'N/A'),
                'rows_used': period_60d.get('rows_used', 60),
                'requested_period_days': 60,
                'reason': f'decomposition_error (residual={result["residual"]:.4%} > tolerance)',
                'using_fallback_exposure': not ledger.get('overlay_available', False),
                'exposure_series_found': ledger.get('overlay_available', False),
                'exposure_min': None,
                'exposure_max': None,
                'cum_realized': period_60d.get('cum_realized'),
                'cum_unoverlay': period_60d.get('cum_unoverlay'),
                'cum_benchmark': period_60d.get('cum_benchmark')
            }
            result['data_available'] = False
            return result
        
        # Extract diagnostic values from ledger daily series
        daily_exposure = ledger.get('daily_exposure')
        daily_realized = ledger.get('daily_realized_return')
        
        # Helper to check series validity
        def series_valid(series):
            """Check if series is valid (not None and has data)."""
            return series is not None and len(series) > 0
        
        # Build diagnostics using strict 60D window from ledger
        result['diagnostics'] = {
            'period_used': '60D',
            'start_date': period_60d.get('start_date', 'N/A'),
            'end_date': period_60d.get('end_date', 'N/A'),
            'rows_used': period_60d.get('rows_used', 60),
            'requested_period_days': 60,
            'reason': None,  # No error
            'using_fallback_exposure': not ledger.get('overlay_available', False),
            'exposure_series_found': ledger.get('overlay_available', False),
            'exposure_min': float(daily_exposure.min()) if series_valid(daily_exposure) else None,
            'exposure_max': float(daily_exposure.max()) if series_valid(daily_exposure) else None,
            'cum_realized': period_60d.get('cum_realized'),
            'cum_unoverlay': period_60d.get('cum_unoverlay'),
            'cum_benchmark': period_60d.get('cum_benchmark')
        }
        
    except Exception as e:
        # Log error but don't fail - graceful degradation
        import logging
        logging.warning(f"Error in compute_alpha_source_breakdown: {e}")
        result['diagnostics'] = {
            'period_used': 'N/A',
            'start_date': 'N/A',
            'end_date': 'N/A',
            'rows_used': 0,
            'reason': f'exception: {str(e)}',
            'using_fallback_exposure': False,
            'exposure_series_found': False,
            'exposure_min': None,
            'exposure_max': None,
            'cum_realized': None,
            'cum_unoverlay': None,
            'cum_benchmark': None
        }
    
    return result


def compute_exposure_adjusted_alpha(df, days=30):
    """
    Compute exposure-adjusted alpha if exposure series exists.
    Falls back to unadjusted alpha if no exposure series is available.
    
    Returns dict with:
        - exposure_adj_alpha_latest: latest exposure-adjusted alpha value
        - exposure_adj_alpha_30day: 30-day exposure-adjusted alpha data
        - data_available: bool
        - is_fallback: bool - True if using unadjusted alpha
    """
    result = {
        'exposure_adj_alpha_latest': None,
        'exposure_adj_alpha_30day': None,
        'data_available': False,
        'is_fallback': False
    }
    
    try:
        if df is None or len(df) == 0:
            return result
        
        # Check for required columns
        if 'portfolio_return' not in df.columns or 'benchmark_return' not in df.columns:
            return result
        
        # Compute total_alpha
        df_copy = df.copy()
        df_copy['total_alpha'] = df_copy['portfolio_return'] - df_copy['benchmark_return']
        
        # Check if exposure column exists
        has_exposure = 'exposure' in df_copy.columns
        
        if has_exposure:
            # Compute exposure-adjusted alpha
            df_copy['exposure_adj_alpha'] = df_copy['total_alpha'] / df_copy['exposure'].apply(lambda x: max(x, 0.05))
        else:
            # Fallback: use unadjusted alpha
            df_copy['exposure_adj_alpha'] = df_copy['total_alpha']
            result['is_fallback'] = True
        
        # Get latest value
        if len(df_copy) > 0:
            latest_date = df_copy['date'].max()
            latest_data = df_copy[df_copy['date'] == latest_date]
            if len(latest_data) > 0:
                result['exposure_adj_alpha_latest'] = latest_data['exposure_adj_alpha'].iloc[0]
        
        # Get last N days
        if len(df_copy) > 0:
            latest_date = df_copy['date'].max()
            days_ago = latest_date - timedelta(days=days)
            last_n_days = df_copy[df_copy['date'] >= days_ago]
            if len(last_n_days) > 0:
                result['exposure_adj_alpha_30day'] = last_n_days['exposure_adj_alpha'].sum()
        
        result['data_available'] = True
        
    except Exception:
        pass
    
    return result


def compute_capital_weighted_alpha(df):
    """
    Compute capital-weighted alpha at portfolio level.
    
    Returns dict with:
        - capital_weighted_alpha: weighted alpha value
        - weighting_method: 'capital' or 'equal-weight'
        - data_available: bool
    """
    result = {
        'capital_weighted_alpha': None,
        'weighting_method': 'equal-weight',
        'data_available': False
    }
    
    try:
        if df is None or len(df) == 0:
            return result
        
        # Check for required columns
        if 'portfolio_return' not in df.columns or 'benchmark_return' not in df.columns:
            return result
        
        df_copy = df.copy()
        df_copy['total_alpha'] = df_copy['portfolio_return'] - df_copy['benchmark_return']
        
        # Check if capital/AUM weight columns exist
        has_capital_weights = 'capital_weight' in df_copy.columns or 'aum_weight' in df_copy.columns
        
        if has_capital_weights:
            weight_col = 'capital_weight' if 'capital_weight' in df_copy.columns else 'aum_weight'
            # Compute weighted alpha
            df_copy['weighted_alpha'] = df_copy['total_alpha'] * df_copy[weight_col]
            result['capital_weighted_alpha'] = df_copy['weighted_alpha'].sum()
            result['weighting_method'] = 'capital'
        else:
            # Fallback to equal weighting
            # Group by wave if available
            if 'wave' in df_copy.columns:
                wave_alphas = df_copy.groupby('wave')['total_alpha'].sum()
                result['capital_weighted_alpha'] = wave_alphas.mean()
            else:
                result['capital_weighted_alpha'] = df_copy['total_alpha'].mean()
            result['weighting_method'] = 'equal-weight'
        
        result['data_available'] = True
        
    except Exception:
        pass
    
    return result


def compute_risk_regime_attribution(df, days_list=[30, 60, 90]):
    """
    Compute Risk-On vs Risk-Off attribution using VIX gate/regime labels.
    
    Returns dict with results for each time period:
        - risk_on_avg_alpha: average alpha on Risk-On days
        - risk_off_avg_alpha: average alpha on Risk-Off days
        - risk_on_contribution_pct: % of total alpha from Risk-On days
        - risk_off_contribution_pct: % of total alpha from Risk-Off days
    """
    result = {}
    
    try:
        if df is None or len(df) == 0:
            return result
        
        # Check for required columns
        if 'portfolio_return' not in df.columns or 'benchmark_return' not in df.columns:
            return result
        
        df_copy = df.copy()
        df_copy['total_alpha'] = df_copy['portfolio_return'] - df_copy['benchmark_return']
        
        # Determine regime column (could be vix_gate, regime, or we need to infer)
        regime_col = None
        if 'vix_gate_status' in df_copy.columns:
            regime_col = 'vix_gate_status'
        elif 'vix_gate' in df_copy.columns:
            regime_col = 'vix_gate'
        elif 'regime' in df_copy.columns:
            regime_col = 'regime'
        
        if regime_col is None:
            # Try to infer from volatility
            if 'portfolio_return' in df_copy.columns:
                # Compute rolling volatility
                df_copy['rolling_vol'] = df_copy['portfolio_return'].rolling(window=5).std() * np.sqrt(252) * 100
                # Define Risk-On as low vol (< 15%), Risk-Off as others
                df_copy['is_risk_on'] = df_copy['rolling_vol'] < 15
            else:
                return result
        else:
            # Map regime values to Risk-On/Risk-Off
            # Risk-On: GREEN, Low Vol, uptrend
            # Risk-Off: RED, YELLOW, High Vol, Med Vol, panic, downtrend
            df_copy['is_risk_on'] = df_copy[regime_col].astype(str).str.contains('GREEN|Low|uptrend', case=False, na=False)
        
        # Compute attribution for each time period
        latest_date = df_copy['date'].max()
        
        for days in days_list:
            days_ago = latest_date - timedelta(days=days)
            period_df = df_copy[df_copy['date'] >= days_ago]
            
            if len(period_df) == 0:
                continue
            
            risk_on_df = period_df[period_df['is_risk_on'] == True]
            risk_off_df = period_df[period_df['is_risk_on'] == False]
            
            period_result = {
                'risk_on_avg_alpha': None,
                'risk_off_avg_alpha': None,
                'risk_on_contribution_pct': None,
                'risk_off_contribution_pct': None,
                'risk_on_days': len(risk_on_df),
                'risk_off_days': len(risk_off_df)
            }
            
            if len(risk_on_df) > 0:
                period_result['risk_on_avg_alpha'] = risk_on_df['total_alpha'].mean()
                risk_on_total = risk_on_df['total_alpha'].sum()
            else:
                risk_on_total = 0
            
            if len(risk_off_df) > 0:
                period_result['risk_off_avg_alpha'] = risk_off_df['total_alpha'].mean()
                risk_off_total = risk_off_df['total_alpha'].sum()
            else:
                risk_off_total = 0
            
            # Compute contribution percentages
            total_alpha_period = period_df['total_alpha'].sum()
            if abs(total_alpha_period) > 0.0001:
                period_result['risk_on_contribution_pct'] = (risk_on_total / total_alpha_period) * 100
                period_result['risk_off_contribution_pct'] = (risk_off_total / total_alpha_period) * 100
            
            result[f'{days}day'] = period_result
        
    except Exception:
        pass
    
    return result


def render_reality_panel():
    """
    Render the Reality Panel - Single Source of Truth Diagnostics
    
    This panel displays the actual PRICE_BOOK metadata used by the entire system.
    It ensures transparency and prevents "two truths" by showing exactly what
    data is being used for readiness, health, execution, and diagnostics.
    
    Displayed at the top of the Overview tab (minimum) or app-wide.
    """
    st.markdown("### üìã Reality Panel - Price Book Status")
    st.caption("Single source of truth for all price data, readiness, health, and execution")
    
    try:
        from helpers.price_book import (
            get_price_book,
            get_price_book_meta,
            get_active_required_tickers,
            get_required_tickers_active_waves,
            compute_missing_and_extra_tickers,
            compute_system_health,
            CANONICAL_CACHE_PATH,
            PRICE_FETCH_ENABLED,
            ALLOW_NETWORK_FETCH
        )
        
        # Load the PRICE_BOOK (this is the actual object used by execution)
        active_tickers = get_active_required_tickers()
        price_book = get_cached_price_book()  # Load all cached tickers
        
        # Get metadata from the actual PRICE_BOOK
        meta = get_price_book_meta(price_book)
        
        # Compute missing/extra tickers
        ticker_analysis = compute_missing_and_extra_tickers(price_book)
        
        # Get git commit info (if available)
        git_commit = "N/A"
        try:
            result = subprocess.run(
                ['git', 'rev-parse', '--short', 'HEAD'],
                capture_output=True,
                text=True,
                timeout=2,
                shell=False  # Explicitly set for security clarity
            )
            if result.returncode == 0:
                git_commit = result.stdout.strip()
        except:
            pass
        
        # Display in columns
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("**üìÅ Price Cache**")
            st.text(f"Path: {meta['cache_path']}")
            st.text(f"Shape: {meta['rows']} rows √ó {meta['cols']} cols")
            
            if meta['is_empty']:
                st.error("‚ö†Ô∏è PRICE_BOOK is EMPTY")
            else:
                st.text(f"Date Range:")
                st.text(f"  Min: {meta['date_min']}")
                st.text(f"  Max: {meta['date_max']}")
        
        with col2:
            st.markdown("**üéØ Ticker Coverage**")
            st.text(f"Active Required: {ticker_analysis['required_count']}")
            st.text(f"In PRICE_BOOK: {ticker_analysis['cached_count']}")
            
            # Guardrail: Check for active_required underreporting
            # Count active waves from registry
            try:
                import pandas as pd
                wave_registry_path = os.path.join('data', 'wave_registry.csv')
                if os.path.exists(wave_registry_path):
                    registry_df = pd.read_csv(wave_registry_path)
                    active_wave_count = registry_df['active'].sum()
                    
                    # Show warning if we have many active waves but suspiciously few required tickers
                    if active_wave_count >= 20 and ticker_analysis['required_count'] < 20:
                        st.error("üö® BUG: active_required too small ‚Äî registry/ticker collection failed.")
                        st.text(f"Active waves in registry: {active_wave_count}")
            except Exception as e:
                pass  # Don't fail if we can't load registry
            
            missing_count = ticker_analysis['missing_count']
            extra_count = ticker_analysis['extra_count']
            
            if missing_count > 0:
                st.error(f"‚ö†Ô∏è Missing: {missing_count} tickers")
            else:
                st.success("‚úÖ Missing: 0 tickers")
            
            if extra_count > 0:
                st.info(f"‚ÑπÔ∏è Extra: {extra_count} tickers (harmless)")
            else:
                st.text(f"Extra: 0 tickers")
        
        with col3:
            st.markdown("**üè• System Health**")
            
            # Compute unified health status
            health = compute_system_health(price_book)
            
            st.metric(
                "Health Status",
                f"{health['health_emoji']} {health['health_status']}",
                help=health['details']
            )
            st.caption(health['details'])
            
            # Show fetch status
            if ALLOW_NETWORK_FETCH:
                st.warning("üü° Network Fetch: ENABLED")
            else:
                st.success("üü¢ Network Fetch: DISABLED")
        
        # Show detailed missing/extra tickers in expander
        if ticker_analysis['missing_count'] > 0 or ticker_analysis['extra_count'] > 0:
            with st.expander("üìä Detailed Ticker Analysis", expanded=False):
                if ticker_analysis['missing_count'] > 0:
                    st.markdown("**Missing Tickers (Required but not in cache):**")
                    missing_display = ", ".join(ticker_analysis['missing_tickers'])
                    if len(missing_display) > 500:
                        missing_display = missing_display[:500] + "..."
                    st.text(missing_display)
                    st.caption(f"Total missing: {ticker_analysis['missing_count']}")
                
                if ticker_analysis['extra_count'] > 0:
                    st.markdown("**Extra Tickers (In cache but not required):**")
                    extra_display = ", ".join(ticker_analysis['extra_tickers'][:20])
                    if ticker_analysis['extra_count'] > 20:
                        extra_display += f" ... and {ticker_analysis['extra_count'] - 20} more"
                    st.text(extra_display)
                    st.caption(f"Total extra: {ticker_analysis['extra_count']} (these are harmless)")
        
    except Exception as e:
        st.error(f"‚ùå Error loading Reality Panel: {str(e)}")
        st.code(traceback.format_exc(), language="python")


def render_snapshot_authority_banner():
    """
    Render Snapshot Authority Banner - Data Freshness & Staleness Warning
    
    This banner enforces data freshness by displaying:
    - Snapshot date from live_snapshot.csv
    - SPY last trading date from prices_cache_meta.json
    - Cache max date from prices_cache_meta.json
    - STALE DATA warning if snapshot_date < last_trading_date
    
    This prevents the app from silently rendering stale data and provides
    visibility into the data pipeline status.
    """
    try:
        # Load snapshot date from live_snapshot.csv
        snapshot_date_str = "N/A"
        snapshot_rows = "N/A"
        snapshot_exists = False
        
        snapshot_path = "data/live_snapshot.csv"
        if os.path.exists(snapshot_path):
            try:
                snapshot_df = pd.read_csv(snapshot_path)
                snapshot_exists = True
                snapshot_rows = len(snapshot_df)
                if "Date" in snapshot_df.columns and not snapshot_df.empty:
                    snapshot_date_str = snapshot_df["Date"].iloc[0]
                else:
                    snapshot_date_str = "Missing Date column"
            except Exception as e:
                snapshot_date_str = f"Error: {e}"
        
        # Load SPY trading date and cache max date from prices_cache_meta.json
        spy_max_date_str = "N/A"
        cache_max_date_str = "N/A"
        cache_generated_at = "N/A"
        
        cache_meta_path = "data/cache/prices_cache_meta.json"
        if os.path.exists(cache_meta_path):
            try:
                with open(cache_meta_path, 'r') as f:
                    cache_meta = json.load(f)
                
                spy_max_date_str = cache_meta.get("spy_max_date", "N/A")
                cache_max_date_str = cache_meta.get("max_price_date", "N/A")
                cache_generated_at = cache_meta.get("generated_at_utc", "N/A")
            except Exception as e:
                spy_max_date_str = f"Error: {e}"
                cache_max_date_str = f"Error: {e}"
        
        # Determine if data is stale
        is_stale = False
        stale_message = ""
        
        if snapshot_date_str != "N/A" and spy_max_date_str != "N/A":
            try:
                snapshot_date = pd.to_datetime(snapshot_date_str).date()
                spy_trading_date = pd.to_datetime(spy_max_date_str).date()
                
                if snapshot_date < spy_trading_date:
                    is_stale = True
                    days_behind = (spy_trading_date - snapshot_date).days
                    stale_message = f"STALE DATA: Snapshot is {days_behind} trading day(s) behind!"
            except Exception as e:
                stale_message = f"Date comparison error: {e}"
        
        # Render banner
        if is_stale:
            # STALE DATA - Red alert banner
            st.error(f"üö® {stale_message}")
            st.markdown(f"""
            <div style="
                background: linear-gradient(135deg, #8B0000 0%, #DC143C 50%, #FF6347 100%);
                border: 4px solid #FF0000;
                border-radius: 12px;
                padding: 20px 30px;
                margin: 20px 0;
                box-shadow: 0 6px 12px rgba(255, 0, 0, 0.5);
            ">
                <h3 style="color: #FFFFFF; margin: 0 0 15px 0; text-align: center;">
                    ‚ö†Ô∏è SNAPSHOT AUTHORITY BANNER - STALE DATA DETECTED ‚ö†Ô∏è
                </h3>
                <div style="color: #FFFFFF; font-size: 16px; line-height: 1.8;">
                    <div style="display: grid; grid-template-columns: 200px auto; gap: 10px;">
                        <div><strong>üìÖ Snapshot Date:</strong></div>
                        <div>{snapshot_date_str} <span style="color: #FFD700; font-weight: bold;">‚ö†Ô∏è BEHIND</span></div>
                        
                        <div><strong>üìä SPY Last Trading Day:</strong></div>
                        <div>{spy_max_date_str} <span style="color: #00FF00; font-weight: bold;">‚úì CURRENT</span></div>
                        
                        <div><strong>üíæ Cache Max Date:</strong></div>
                        <div>{cache_max_date_str}</div>
                        
                        <div><strong>üì¶ Snapshot Rows:</strong></div>
                        <div>{snapshot_rows}</div>
                    </div>
                    <div style="margin-top: 20px; padding: 15px; background: rgba(0, 0, 0, 0.3); border-radius: 8px;">
                        <strong>‚ö†Ô∏è ACTION REQUIRED:</strong><br/>
                        1. Run "Update Price Cache" workflow<br/>
                        2. Run "Build Wave History" workflow<br/>
                        3. Run "Rebuild Snapshot" workflow<br/>
                        <br/>
                        <strong>The app is displaying STALE data. Do not make decisions based on this data.</strong>
                    </div>
                </div>
            </div>
            """, unsafe_allow_html=True)
        else:
            # FRESH DATA - Green success banner
            st.markdown(f"""
            <div style="
                background: linear-gradient(135deg, #1a472a 0%, #2d5f3d 50%, #4a7c59 100%);
                border: 3px solid #00FF88;
                border-radius: 12px;
                padding: 15px 25px;
                margin: 15px 0;
                box-shadow: 0 4px 8px rgba(0, 255, 136, 0.3);
            ">
                <h4 style="color: #00FF88; margin: 0 0 10px 0; text-align: center;">
                    ‚úÖ Snapshot Authority - Data Status
                </h4>
                <div style="color: #FFFFFF; font-size: 14px; display: grid; grid-template-columns: 180px auto; gap: 8px;">
                    <div><strong>üìÖ Snapshot Date:</strong></div>
                    <div>{snapshot_date_str}</div>
                    
                    <div><strong>üìä SPY Last Trading Day:</strong></div>
                    <div>{spy_max_date_str}</div>
                    
                    <div><strong>üíæ Cache Max Date:</strong></div>
                    <div>{cache_max_date_str}</div>
                    
                    <div><strong>üì¶ Snapshot Rows:</strong></div>
                    <div>{snapshot_rows}</div>
                </div>
            </div>
            """, unsafe_allow_html=True)
        
    except Exception as e:
        st.error(f"‚ö†Ô∏è Failed to render Snapshot Authority Banner: {e}")


def render_mission_control():
    """
    Render the Mission Control summary strip at the top of the page.
    Enhanced with additional system metrics and visual indicators.
    Enhanced with Alpha Attribution + Diagnostics features (Exec Layer v2).
    """
    st.markdown("### üéØ Mission Control - Executive Layer v2")
    
    # RUN COUNTER + Timestamp Indicator (ALWAYS VISIBLE - Production requirement)
    # Note: run_id is incremented in main() function at line ~19076
    current_time = datetime.now().strftime("%H:%M:%S")
    run_id = st.session_state.get("run_id", 0)
    auto_refresh_enabled = st.session_state.get("auto_refresh_enabled", False)
    rebuilding = st.session_state.get("rebuilding_price_book", False)
    
    # Prominent banner showing RUN COUNTER and status
    st.info(
        f"üîÑ **RUN COUNTER:** {run_id} | üïê **Timestamp:** {current_time} | "
        f"üîÑ **Auto-Refresh:** {'üü¢ ON' if auto_refresh_enabled else 'üî¥ OFF'} | "
        f"{'üî® **Rebuild:** IN PROGRESS' if rebuilding else ''}"
    )
    
    mc_data = get_mission_control_data()
    
    # Top row: Primary metrics (5 columns)
    col1, col2, col3, col4, col5 = st.columns(5)
    
    with col1:
        regime_value = mc_data['market_regime']
        # Add emoji indicators
        if 'Risk-On' in regime_value:
            regime_display = f"üìà {regime_value}"
        elif 'Risk-Off' in regime_value:
            regime_display = f"üìâ {regime_value}"
        else:
            regime_display = f"‚ûñ {regime_value}"
        
        st.metric(
            label="Market Regime",
            value=regime_display,
            help="Current market regime based on recent portfolio performance and volatility"
        )
    
    with col2:
        vix_value = mc_data['vix_gate_status']
        # Add color indicators
        if 'GREEN' in vix_value:
            vix_display = f"üü¢ {vix_value}"
        elif 'YELLOW' in vix_value:
            vix_display = f"üü° {vix_value}"
        elif 'RED' in vix_value:
            vix_display = f"üî¥ {vix_value}"
        elif vix_value in ['Pending', 'Initializing']:
            vix_display = vix_value
        else:
            vix_display = vix_value
        
        st.metric(
            label="VIX Gate Status",
            value=vix_display,
            help="VIX-based volatility gate (requires VIX data)"
        )
        
        # Add caption for pending state
        if vix_value in ['Pending', 'Initializing']:
            st.caption("Awaiting VIX data")
    
    with col3:
        st.markdown("**Alpha Captured**")
        alpha_today_str = mc_data['alpha_today']
        alpha_30day_str = mc_data['alpha_30day']
        
        # Add color coding if possible
        try:
            if alpha_today_str not in ['Pending', 'Initializing'] and '%' in alpha_today_str:
                alpha_today_val = float(alpha_today_str.replace('%', ''))
                if alpha_today_val > 0:
                    alpha_today_str = f"üü¢ {alpha_today_str}"
                elif alpha_today_val < 0:
                    alpha_today_str = f"üî¥ {alpha_today_str}"
        except:
            pass
        
        # Display alpha metrics
        st.write(f"Latest: {alpha_today_str}")
        st.write(f"30-Day: {alpha_30day_str}")
    
    with col4:
        st.markdown("**WaveScore Leader**")
        if mc_data['wavescore_leader'] not in ['Pending', 'Initializing']:
            # Truncate long wave names
            wave_name_display = mc_data['wavescore_leader']
            if len(wave_name_display) > 20:
                wave_name_display = wave_name_display[:20] + "..."
            st.write(f"üèÜ {wave_name_display}")
            st.write(f"Score: {mc_data['wavescore_leader_score']}")
        else:
            st.write("Pending (data verified)")
            st.caption("Awaiting sufficient data")
    
    with col5:
        system_status = mc_data['system_status']
        freshness_value = mc_data['data_freshness']
        
        # Add status indicator
        if system_status == 'Excellent':
            status_display = f"‚úÖ {system_status}"
        elif system_status == 'Good':
            status_display = f"üü¢ {system_status}"
        elif system_status == 'Fair':
            status_display = f"üü° {system_status}"
        else:
            status_display = f"üî¥ {system_status}"
        
        # Check for degraded data state (price cache failures)
        price_failures = st.session_state.get("global_price_failures", {})
        has_failures = len(price_failures) > 0
        
        st.metric(
            label="System Health",
            value=status_display,
            help=f"Data freshness: {freshness_value}"
        )
        
        # Show data quality badge if there are failures
        if has_failures:
            failure_count = len(price_failures)
            st.caption(f"‚ö†Ô∏è Data: Degraded ({failure_count} tickers failed)")
        else:
            st.caption(f"Data: {freshness_value}")
    
    # Bottom row: Secondary metrics + Auto-Refresh Indicators (6 columns)
    st.markdown("---")
    
    sec_col1, sec_col2, sec_col3, sec_col4, sec_col5, sec_col6 = st.columns(6)
    
    with sec_col1:
        st.metric(
            label="Universe",
            value=mc_data.get('universe_count', 0),
            help="Total waves in the canonical wave registry"
        )
    
    with sec_col2:
        st.metric(
            label="Active Waves",
            value=mc_data.get('active_waves', 0),
            help="Waves with enabled=True (not dependent on data availability)"
        )
    
    with sec_col3:
        waves_live = mc_data.get('waves_live_count', 0)
        universe = mc_data.get('universe_count', 0)
        # Display as fraction (defensive: handle edge case of universe=0)
        display_value = f"{waves_live}/{universe}" if universe > 0 else "0/0"
        st.metric(
            label="Waves Live",
            value=display_value,
            help="Waves with valid PRICE_BOOK data / Total universe"
        )
        
    with sec_col4:
        data_age = mc_data.get('data_age_days')
        if data_age is not None:
            age_display = f"{data_age} day{'s' if data_age != 1 else ''}"
            if data_age == 0:
                age_display = "Today"
            # Option B: Three-tier staleness display using canonical price_book thresholds
            # UI uses price_book as source of truth to prevent divergence
            # OK: ‚â§PRICE_CACHE_OK_DAYS (14), DEGRADED: PRICE_CACHE_OK_DAYS+1 to PRICE_CACHE_DEGRADED_DAYS (15-30), STALE: >PRICE_CACHE_DEGRADED_DAYS (>30)
            elif isinstance(data_age, (int, float)) and data_age > STALE_DAYS_THRESHOLD:
                # STALE: >PRICE_CACHE_DEGRADED_DAYS
                age_display = f"‚ùå {data_age} days (STALE)"
            elif isinstance(data_age, (int, float)) and data_age > DEGRADED_DAYS_THRESHOLD:
                # DEGRADED: PRICE_CACHE_OK_DAYS+1 to PRICE_CACHE_DEGRADED_DAYS
                age_display = f"‚ö†Ô∏è {data_age} days (DEGRADED)"
            # else: OK: ‚â§PRICE_CACHE_OK_DAYS (no special indicator)
        else:
            age_display = "Unknown"
        
        # Build help text for data age metric
        # UI uses price_book as source of truth to prevent divergence
        help_text = (
            f"Time since last data update (UTC). Three-tier staleness system: "
            f"OK (‚â§{PRICE_CACHE_OK_DAYS} days), "
            f"DEGRADED ({PRICE_CACHE_OK_DAYS + 1}-{PRICE_CACHE_DEGRADED_DAYS} days), "
            f"STALE (>{PRICE_CACHE_DEGRADED_DAYS} days). "
            f"Thresholds are configurable via PRICE_CACHE_OK_DAYS and PRICE_CACHE_DEGRADED_DAYS environment variables."
        )
        
        st.metric(
            label="Data Age",
            value=age_display,
            help=help_text
        )
    
    with sec_col5:
        # Last Price Date (UTC) - shows the actual latest date in PRICE_BOOK
        last_price_date = mc_data.get('last_price_date', 'Unknown')
        st.metric(
            label="Last Price Date",
            value=last_price_date,
            help="Latest price date in PRICE_BOOK (UTC)"
        )
    
    with sec_col6:
        # Auto-Refresh Status Indicator (Enhanced)
        auto_refresh_enabled = st.session_state.get("auto_refresh_enabled", DEFAULT_AUTO_REFRESH_ENABLED)
        auto_refresh_paused = st.session_state.get("auto_refresh_paused", False)
        auto_refresh_interval_ms = st.session_state.get("auto_refresh_interval_ms", DEFAULT_REFRESH_INTERVAL_MS)
        
        # Determine status display
        if auto_refresh_paused:
            refresh_display = STATUS_FORMAT.get("paused", "üü° PAUSED")
        elif auto_refresh_enabled:
            # Get interval in human-readable format
            if AUTO_REFRESH_CONFIG_AVAILABLE:
                interval_name = get_interval_display_name(auto_refresh_interval_ms)
            else:
                interval_sec = auto_refresh_interval_ms / 1000
                interval_name = f"{interval_sec:.0f}s"
            refresh_display = f"{STATUS_FORMAT.get('enabled', 'üü¢ ON')} ({interval_name})"
        else:
            refresh_display = STATUS_FORMAT.get("disabled", "üî¥ OFF")
        
        st.metric(
            label="Auto-Refresh",
            value=refresh_display,
            help="App automatically refreshes at configured interval. Default: 1 minute"
        )
        
        # Show last successful refresh time
        last_successful_refresh = st.session_state.get("last_successful_refresh_time", datetime.now())
        st.caption(f"Last: {last_successful_refresh.strftime('%H:%M:%S')}")
    
    # ========================================================================
    # WARNING: Cache Stale + Network Fetch Disabled
    # ========================================================================
    # UI uses price_book as source of truth to prevent divergence
    data_age = mc_data.get('data_age_days')
    try:
        from helpers.price_book import ALLOW_NETWORK_FETCH
        
        # Show warning if cache is STALE (>PRICE_CACHE_DEGRADED_DAYS) AND network fetch is disabled (with type safety)
        if isinstance(data_age, (int, float)) and data_age > STALE_DAYS_THRESHOLD and not ALLOW_NETWORK_FETCH:
            st.warning(
                f"‚ö†Ô∏è **STALE DATA WARNING**\n\n"
                f"Data is {data_age} days old (>{PRICE_CACHE_DEGRADED_DAYS} days). Network fetching is disabled (safe_mode), "
                f"but you can still manually refresh using the 'Rebuild PRICE_BOOK Cache' button below."
            )
        elif isinstance(data_age, (int, float)) and data_age > DEGRADED_DAYS_THRESHOLD and not ALLOW_NETWORK_FETCH:
            # Info message for DEGRADED (PRICE_CACHE_OK_DAYS+1 to PRICE_CACHE_DEGRADED_DAYS)
            st.info(
                f"‚ÑπÔ∏è **DEGRADED DATA NOTICE**\n\n"
                f"Data is {data_age} days old ({PRICE_CACHE_OK_DAYS + 1}-{PRICE_CACHE_DEGRADED_DAYS} days). "
                f"Consider refreshing using the 'Rebuild PRICE_BOOK Cache' button below for fresher data."
            )
    except Exception:
        pass
    
    # ========================================================================
    # REBUILD CACHE BUTTON
    # ========================================================================
    st.markdown("---")
    
    col_btn1, col_btn2, col_btn3 = st.columns([1, 1, 1])
    
    with col_btn1:
        if st.button(
            "üî® Rebuild PRICE_BOOK Cache",
            key="rebuild_price_cache_mission_control",
            use_container_width=True,
            help="Rebuild the canonical price cache with fresh market data. Works even when safe_mode is ON (safe_mode only blocks implicit fetches, not explicit user actions)."
        ):
            # Prevent double-trigger by checking if rebuild is already in progress
            if st.session_state.get("rebuilding_price_book", False):
                st.warning("‚è≥ Rebuild already in progress...")
            else:
                try:
                    # Set flag to prevent double-trigger
                    st.session_state.rebuilding_price_book = True
                    
                    # Show progress indicator
                    with st.spinner("Rebuilding price cache... This may take a few minutes."):
                        # Import the rebuild function
                        from helpers.price_book import rebuild_price_cache, ALLOW_NETWORK_FETCH
                        
                        # Call rebuild with force_user_initiated=True to allow manual fetching
                        # even when safe_mode_no_fetch=True (safe_mode only blocks IMPLICIT fetches)
                        result = rebuild_price_cache(active_only=True, force_user_initiated=True)
                        
                        # Check if fetching is allowed
                        if not result['allowed']:
                            st.error(
                                "‚ùå Price fetching failed\n\n"
                                f"{result.get('message', 'Unable to fetch prices.')}"
                            )
                        elif result['success']:
                            st.success(
                                f"‚úÖ PRICE_BOOK rebuilt. Latest price date now: {result['date_max']}\n\n"
                                f"üìä {result['tickers_fetched']}/{result['tickers_requested']} tickers fetched"
                            )
                            
                            # Show failed tickers if any
                            if result['tickers_failed'] > 0 and result['failures']:
                                with st.expander("‚ö†Ô∏è Failed Tickers", expanded=False):
                                    # Show first 10 failed tickers efficiently using islice
                                    for ticker in itertools.islice(result['failures'].keys(), 10):
                                        st.text(f"‚Ä¢ {ticker}")
                                    if len(result['failures']) > 10:
                                        st.text(f"... and {len(result['failures']) - 10} more")
                            
                            # Clear Streamlit caches to reflect new data
                            st.cache_data.clear()
                            
                            # Mark user interaction
                            st.session_state.user_interaction_detected = True
                            
                            # Note: Flag will be cleared in finally block after rerun
                            # This prevents race conditions
                            
                            # Trigger rerun
                            trigger_rerun("rebuild_price_cache_mission_control")
                        else:
                            st.error("‚ùå Failed to rebuild price cache")
                        
                except Exception as e:
                    st.error(f"Error rebuilding cache: {str(e)}")
                    import traceback
                    st.code(traceback.format_exc(), language="python")
                finally:
                    # Always clear the flag when done
                    st.session_state.rebuilding_price_book = False
    
    # ========================================================================
    # NEW FEATURES: Exec Layer v2 - Alpha Attribution + Diagnostics
    # ========================================================================
    
    st.markdown("---")
    st.markdown("#### üìä Alpha Attribution & Analytics")
    
    # Load wave history data for analytics
    try:
        df = safe_load_wave_history()
        
        if df is not None and len(df) > 0:
            # ================================================================
            # 1. Alpha Source Breakdown Panel
            # ================================================================
            st.markdown("##### üîç Alpha Source Breakdown (Portfolio-Level)")
            st.caption("Portfolio-level alpha attribution with transparent methodology")
            
            alpha_breakdown = compute_alpha_source_breakdown(df)
            
            # Attribution Diagnostics Expander - always show (even if data unavailable)
            with st.expander("üî¨ Attribution Diagnostics", expanded=False):
                st.caption("Detailed diagnostic values for transparency and validation (60D period from ledger)")
                
                diagnostics = alpha_breakdown.get('diagnostics', {})
                
                # Check if period is available
                period_available = alpha_breakdown.get('data_available', False)
                period_reason = diagnostics.get('reason')
                
                if not period_available and period_reason:
                    # Show unavailable reason prominently
                    st.warning(f"‚ö†Ô∏è 60D Period Unavailable: {period_reason}")
                    st.caption(f"Rows available: {diagnostics.get('rows_used', 0)}, Required: {diagnostics.get('requested_period_days', 60)}")
                
                # Create diagnostic display in two columns
                diag_col1, diag_col2 = st.columns(2)
                
                with diag_col1:
                    st.markdown("**Period & Date Range (Strict Rolling Window):**")
                    st.text(f"Period Used: {diagnostics.get('period_used', 'N/A')}")
                    st.text(f"Requested Period Days: {diagnostics.get('requested_period_days', 'N/A')}")
                    st.text(f"Rows Used: {diagnostics.get('rows_used', 'N/A')}")
                    st.text(f"Start Date: {diagnostics.get('start_date', 'N/A')}")
                    st.text(f"End Date: {diagnostics.get('end_date', 'N/A')}")
                    
                    if period_reason:
                        st.markdown("")
                        st.markdown("**Unavailability Reason:**")
                        st.text(f"{period_reason}")
                    
                    st.markdown("")
                    st.markdown("**Exposure Series:**")
                    st.text(f"Using Fallback Exposure: {diagnostics.get('using_fallback_exposure', 'N/A')}")
                    st.text(f"Exposure Series Found: {diagnostics.get('exposure_series_found', 'N/A')}")
                    
                    exposure_min = diagnostics.get('exposure_min')
                    exposure_max = diagnostics.get('exposure_max')
                    st.text(f"Exposure Min: {exposure_min:.4f}" if exposure_min is not None else "Exposure Min: N/A")
                    st.text(f"Exposure Max: {exposure_max:.4f}" if exposure_max is not None else "Exposure Max: N/A")
                
                with diag_col2:
                    st.markdown("**Cumulative Returns (Compounded):**")
                    
                    cum_realized = diagnostics.get('cum_realized')
                    cum_unoverlay = diagnostics.get('cum_unoverlay')
                    cum_benchmark = diagnostics.get('cum_benchmark')
                    
                    st.text(f"Cum Realized: {cum_realized:+.4%}" if cum_realized is not None else "Cum Realized: N/A")
                    st.text(f"Cum Unoverlay: {cum_unoverlay:+.4%}" if cum_unoverlay is not None else "Cum Unoverlay: N/A")
                    st.text(f"Cum Benchmark: {cum_benchmark:+.4%}" if cum_benchmark is not None else "Cum Benchmark: N/A")
                    
                    st.markdown("")
                    st.caption("All cumulative returns computed using compounded math: (1 + daily_returns).prod() - 1")
            
            if alpha_breakdown['data_available']:
                # Display as table and KPI tiles
                col_table, col_kpi1, col_kpi2, col_kpi3 = st.columns([2, 1, 1, 1])
                
                with col_table:
                    # Create breakdown table with numeric values
                    breakdown_data = []
                    breakdown_data.append(['Cumulative Alpha (Total)', _fmt_pct_or_status(alpha_breakdown['total_alpha'], 'N/A')])
                    breakdown_data.append(['Selection Alpha', _fmt_pct_or_status(alpha_breakdown['selection_alpha'], 'N/A')])
                    breakdown_data.append(['Overlay Alpha (VIX/SafeSmart)', _fmt_pct_or_status(alpha_breakdown['overlay_alpha'], 'N/A')])
                    breakdown_data.append(['Residual', _fmt_pct_or_status(alpha_breakdown['residual'], 'N/A')])
                    
                    if breakdown_data:
                        breakdown_df = pd.DataFrame(breakdown_data, columns=['Component', 'Value'])
                        st.dataframe(breakdown_df, hide_index=True, use_container_width=True)
                
                # Display Cumulative Alpha (Pre-Decomposition) headline with caption
                st.markdown("**Cumulative Portfolio Alpha (60D from Ledger)**")
                if alpha_breakdown['total_alpha'] is not None:
                    st.markdown(f"**{_fmt_pct_or_status(alpha_breakdown['total_alpha'], 'Pending')}**")
                else:
                    st.markdown("**Pending**")
                st.caption("Benchmark-relative ¬∑ Portfolio-level ¬∑ 60D rolling window")
                
                # KPI tiles
                with col_kpi1:
                    st.metric(
                        "Selection Œ±",
                        _fmt_pct_or_status(alpha_breakdown['selection_alpha'], 'Pending'),
                        help="Security-selection attribution from canonical ledger (60D period)"
                    )
                
                with col_kpi2:
                    st.metric(
                        "Overlay Œ±",
                        _fmt_pct_or_status(alpha_breakdown['overlay_alpha'], 'Derived'),
                        help="Overlay alpha from VIX exposure management (60D period)"
                    )
                
                with col_kpi3:
                    residual_val = alpha_breakdown['residual']
                    # Color code residual based on tolerance (matching blue box)
                    residual_pct = abs(residual_val) * 100 if residual_val is not None else 0
                    residual_color = "üü¢" if residual_pct < 0.10 else "üü°" if residual_pct < 0.5 else "üî¥"
                    st.metric(
                        f"{residual_color} Residual",
                        _fmt_pct_or_status(residual_val, 'Reserved'),
                        help="Residual should be near 0% (tolerance: 0.10%)"
                    )
                    
                    # Warning banner if residual exceeds strict tolerance
                    if residual_val is not None and abs(residual_val) > 0.0010:  # 0.10% strict tolerance
                        st.warning(f"‚ö†Ô∏è Residual exceeds tolerance ({residual_val*100:.4f}% > 0.10%)")
                
                # Governance footer
                st.markdown("*Attribution from canonical ledger (60D rolling window) - same source as Portfolio Snapshot blue box*")
            else:
                # Show N/A with explicit reason
                diagnostics = alpha_breakdown.get('diagnostics', {})
                reason = diagnostics.get('reason', 'unknown')
                rows_used = diagnostics.get('rows_used', 0)
                requested = diagnostics.get('requested_period_days', 60)
                
                st.info(f"üìã 60D Alpha breakdown unavailable: {reason}")
                st.caption(f"Rows available: {rows_used}, Required: {requested}")
                st.caption("No fallback to inception - strict rolling window enforcement")
            
            # ================================================================
            # 2. Exposure-Adjusted Alpha
            # ================================================================
            st.markdown("##### üìà Exposure-Adjusted Alpha")
            
            exposure_alpha = compute_exposure_adjusted_alpha(df, days=30)
            
            if exposure_alpha['data_available']:
                exp_col1, exp_col2 = st.columns(2)
                
                with exp_col1:
                    # Clarify whether adjusted or unadjusted in the label
                    if exposure_alpha['is_fallback']:
                        label = "Latest Alpha (Unadjusted)"
                        help_text = "Showing raw alpha - exposure data not available for adjustment"
                    else:
                        label = "Latest Exposure-Adj Alpha"
                        help_text = "Alpha adjusted for beta and dynamic exposure changes"
                    
                    st.metric(
                        label,
                        _fmt_pct_or_status(exposure_alpha['exposure_adj_alpha_latest'], 'Pending'),
                        help=help_text
                    )
                
                with exp_col2:
                    # Clarify whether adjusted or unadjusted in the label
                    if exposure_alpha['is_fallback']:
                        label = "30-Day Alpha (Unadjusted)"
                        help_text = "Showing raw alpha - exposure data not available for adjustment"
                    else:
                        label = "30-Day Exposure-Adj Alpha"
                        help_text = "Alpha adjusted for beta and dynamic exposure changes"
                    
                    st.metric(
                        label,
                        _fmt_pct_or_status(exposure_alpha['exposure_adj_alpha_30day'], 'Pending'),
                        help=help_text
                    )
                
                # Show methodology indicator - only show when actually missing exposure data
                # Suppress warning if using expected fallback behavior (exposure=1.0)
                if exposure_alpha['is_fallback']:
                    # Check if this is the expected fallback (no overlay data) vs actual missing data
                    # If portfolio_alpha_attribution also indicates fallback, this is expected behavior
                    portfolio_attrib = st.session_state.get('portfolio_alpha_attribution', {})
                    using_fallback = portfolio_attrib.get('using_fallback_exposure', False)
                    
                    if not using_fallback:
                        # Actual missing data - show warning
                        st.info("‚ÑπÔ∏è Showing unadjusted alpha - exposure series not found in data")
                        st.caption("Exposure-adjusted calculation requires exposure time series data")
                    # else: Fallback is expected, no warning needed
                else:
                    st.success("‚úÖ Using exposure-adjusted methodology")
                    st.caption("Normalized for dynamic exposure and volatility regime")
            else:
                st.info("üìã Exposure-adjusted alpha not available. Required: portfolio_return, benchmark_return.")
            
            # ================================================================
            # 3. Capital-Weighted Alpha (Portfolio-level)
            # ================================================================
            st.markdown("##### üíº Capital-Weighted Alpha (Portfolio-Level)")
            st.caption("Portfolio-level alpha with transparent weighting methodology")
            
            capital_alpha = compute_capital_weighted_alpha(df)
            
            if capital_alpha['data_available']:
                cap_col1, cap_col2 = st.columns(2)
                
                with cap_col1:
                    method = capital_alpha['weighting_method']
                    
                    # Check if using fallback (equal-weight) due to missing capital inputs
                    if method == 'equal-weight':
                        # Display N/A instead of computed value when capital inputs are missing
                        st.metric(
                            "Capital-Weighted Alpha",
                            "N/A",
                            help="Capital inputs required for capital-weighted alpha calculation"
                        )
                    else:
                        # Display actual capital-weighted value
                        if capital_alpha['capital_weighted_alpha'] is not None:
                            label = f"Capital-Weighted Alpha ({method.title()})"
                            st.metric(
                                label,
                                f"{capital_alpha['capital_weighted_alpha']*100:.4f}%",
                                help=f"Alpha calculated using {method} weighting methodology"
                            )
                        else:
                            st.metric("Capital-Weighted Alpha", "N/A")
                
                with cap_col2:
                    method_label = capital_alpha['weighting_method']
                    if method_label == 'equal-weight':
                        st.info("‚ÑπÔ∏è Add capital inputs to enable capital-weighted alpha.")
                        st.caption("Equal-weight methodology is currently in use.")
                    else:
                        st.success(f"‚úÖ Using {method_label} weighting")
                        st.caption("Based on available capital allocation data")
            else:
                st.info("üìã Capital-weighted alpha not available. Required: portfolio returns.")
            
            # ================================================================
            # 4. Risk-On vs Risk-Off Attribution
            # ================================================================
            st.markdown("##### üéØ Risk-On vs Risk-Off Attribution")
            st.caption("‚ö†Ô∏è Note: Percentages shown are cumulative exposure contributions, NOT allocation weights")
            
            risk_attrib = compute_risk_regime_attribution(df, days_list=[30, 60, 90])
            
            if risk_attrib:
                # Create tabs for different time periods
                period_tabs = st.tabs(["30 Days", "60 Days", "90 Days"])
                
                for idx, days in enumerate([30, 60, 90]):
                    with period_tabs[idx]:
                        period_key = f'{days}day'
                        if period_key in risk_attrib:
                            period_data = risk_attrib[period_key]
                            
                            risk_col1, risk_col2, risk_col3, risk_col4 = st.columns(4)
                            
                            with risk_col1:
                                if period_data['risk_on_avg_alpha'] is not None:
                                    st.metric(
                                        "Risk-On Avg Œ±",
                                        f"{period_data['risk_on_avg_alpha']*100:.4f}%",
                                        help=f"Average alpha on Risk-On days ({period_data['risk_on_days']} days)"
                                    )
                                else:
                                    st.metric("Risk-On Avg Œ±", "N/A")
                            
                            with risk_col2:
                                if period_data['risk_off_avg_alpha'] is not None:
                                    st.metric(
                                        "Risk-Off Avg Œ±",
                                        f"{period_data['risk_off_avg_alpha']*100:.4f}%",
                                        help=f"Average alpha on Risk-Off days ({period_data['risk_off_days']} days)"
                                    )
                                else:
                                    st.metric("Risk-Off Avg Œ±", "N/A")
                            
                            with risk_col3:
                                if period_data['risk_on_contribution_pct'] is not None:
                                    st.metric(
                                        "Risk-On %",
                                        f"{period_data['risk_on_contribution_pct']:.1f}%",
                                        help="% of total alpha from Risk-On days"
                                    )
                                else:
                                    st.metric("Risk-On %", "N/A")
                            
                            with risk_col4:
                                if period_data['risk_off_contribution_pct'] is not None:
                                    st.metric(
                                        "Risk-Off %",
                                        f"{period_data['risk_off_contribution_pct']:.1f}%",
                                        help="% of total alpha from Risk-Off days"
                                    )
                                else:
                                    st.metric("Risk-Off %", "N/A")
                        else:
                            st.info(f"No data available for {days}-day period")
            else:
                st.info("üìã Risk regime attribution not available. Required: portfolio and benchmark returns.")
            
            # ================================================================
            # 5. 'Show Your Work' Expander (Diagnostics Table)
            # ================================================================
            st.markdown("##### üî¨ Diagnostics & Validation")
            
            with st.expander("üìä Show Your Work - Per-Day Diagnostics", expanded=False):
                st.markdown("**Per-day diagnostic data showing VIX, Regime, Exposure, and Returns**")
                
                # Try to use get_wave_diagnostics if available
                if VIX_DIAGNOSTICS_AVAILABLE:
                    # Get first wave from data to show diagnostics
                    if 'wave' in df.columns:
                        waves = df['wave'].unique()
                        if len(waves) > 0:
                            selected_wave = st.selectbox(
                                "Select Wave for Diagnostics",
                                waves,
                                key=k("Diagnostics", "wave_selector")
                            )
                            
                            try:
                                diag_df = get_wave_diagnostics(
                                    wave_name=selected_wave,
                                    mode="Standard",
                                    days=90
                                )
                                
                                if diag_df is not None and not diag_df.empty:
                                    # Display diagnostic table
                                    st.dataframe(
                                        diag_df,
                                        use_container_width=True,
                                        height=400
                                    )
                                    
                                    # Download button
                                    csv = diag_df.to_csv(index=True)
                                    st.download_button(
                                        label="üì• Download Diagnostics CSV",
                                        data=csv,
                                        file_name=f"diagnostics_{selected_wave}_{datetime.now().strftime('%Y%m%d')}.csv",
                                        mime="text/csv"
                                    )
                                else:
                                    st.warning("No diagnostic data available for selected wave")
                            except Exception as e:
                                st.error(f"Error loading diagnostics: {str(e)}")
                        else:
                            st.info("No waves found in data")
                    else:
                        st.info("Wave column not found in data")
                else:
                    # Fallback: create basic diagnostics from available data
                    st.info("VIX diagnostics module not available. Showing basic per-day data:")
                    
                    # Create basic diagnostic view
                    diag_cols = ['date']
                    if 'wave' in df.columns:
                        diag_cols.append('wave')
                    if 'portfolio_return' in df.columns:
                        diag_cols.append('portfolio_return')
                    if 'benchmark_return' in df.columns:
                        diag_cols.append('benchmark_return')
                    
                    # Add alpha column
                    if 'portfolio_return' in df.columns and 'benchmark_return' in df.columns:
                        df_diag = df[diag_cols].copy()
                        df_diag['alpha'] = df['portfolio_return'] - df['benchmark_return']
                    else:
                        df_diag = df[diag_cols].copy()
                    
                    # Show last 90 days
                    latest_date = df_diag['date'].max()
                    days_ago = latest_date - timedelta(days=90)
                    df_diag = df_diag[df_diag['date'] >= days_ago]
                    
                    st.dataframe(df_diag, use_container_width=True, height=400)
        
        else:
            st.info("üìã No wave history data available for analytics. Please load wave history data to view attribution and diagnostics.")
    
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Error loading analytics features: {str(e)}")
        st.info("üìã The application continues to function. Core features remain available.")
    
    st.divider()


def render_sidebar_info():
    """
    Render sidebar information including build info and menu.
    
    NOTE: Wave selection changes will trigger ONE rerun to update the page context.
    This is expected Streamlit behavior and not an infinite loop.
    The loop detection mechanism will prevent multiple consecutive reruns.
    """
    
    # ========================================================================
    # WAVE SELECTION CONTROL - Always Visible
    # ========================================================================
    st.sidebar.markdown("### üåä Wave Selection")
    
    # Get all active waves from the registry with wave_id mapping
    try:
        # Check if required modules are available
        if not WAVE_REGISTRY_MANAGER_AVAILABLE or not WAVES_ENGINE_AVAILABLE:
            st.sidebar.warning("‚ö†Ô∏è Wave selection unavailable - required modules not loaded")
            # Ensure consistent state - default to portfolio view
            # UPDATED: Use selected_wave_id as authoritative state key
            if "selected_wave_id" not in st.session_state:
                st.session_state.selected_wave_id = None
            return
        
        # Load active waves with wave_id and display_name
        active_waves_df = get_active_wave_registry()
        
        # Build list of wave objects with both wave_id and display_name
        active_waves = []
        for _, row in active_waves_df.iterrows():
            active_waves.append({
                'wave_id': row['wave_id'],
                'display_name': row['wave_name']
            })
        
        # Build options list for selectbox (display names only)
        wave_display_names = [wave['display_name'] for wave in active_waves]
        wave_options = [PORTFOLIO_VIEW_TITLE] + sorted(wave_display_names)
        
        # Create mapping dictionary: display_name -> wave_id
        name_to_id = {wave['display_name']: wave['wave_id'] for wave in active_waves}
        
        # Get current selection from session state (UPDATED: use selected_wave_id)
        current_wave_id = st.session_state.get("selected_wave_id")
        
        # Determine the index for the selectbox based on stored wave_id
        if current_wave_id is None:
            # Portfolio mode
            default_index = 0
        else:
            # Convert wave_id back to display_name to find index
            try:
                current_display_name = get_display_name_from_wave_id(current_wave_id)
                if current_display_name in wave_options:
                    default_index = wave_options.index(current_display_name)
                else:
                    # Invalid wave_id, default to portfolio
                    default_index = 0
            except (KeyError, ValueError, AttributeError):
                # Error converting wave_id, default to portfolio
                default_index = 0
        
        # Render wave selector with stable key
        # UPDATED: Use selected_wave_id as the key for persistence
        selected_option = st.sidebar.selectbox(
            "Select Context",
            options=wave_options,
            index=default_index,
            key="selected_wave_id_display",
            help="Choose Portfolio for all-waves view, or select an individual wave for wave-specific metrics"
        )
        
        # Map the selected display name to wave_id and store in session state
        # This ensures the selection persists across reruns
        if selected_option == PORTFOLIO_VIEW_TITLE:
            # Portfolio mode selected
            st.session_state.selected_wave_id = None
        else:
            # Individual wave selected - map display_name to wave_id
            new_wave_id = name_to_id.get(selected_option)
            if new_wave_id is not None:
                st.session_state.selected_wave_id = new_wave_id
            else:
                # Fallback if mapping fails
                st.session_state.selected_wave_id = None
        
        # Display current context using canonical resolver
        ctx = resolve_app_context()
        if ctx["selected_wave_id"] is None:
            st.sidebar.info("‚ÑπÔ∏è Portfolio view")
        else:
            st.sidebar.info("üìà Wave view")
        
        # Debug caption to prove persistence (only in debug mode)
        if st.session_state.get("debug_mode", False):
            wave_id = ctx["selected_wave_id"]
            if wave_id:
                st.sidebar.caption(f"selected_wave_id: {wave_id} ‚Üí {ctx['selected_wave_name']}")
                st.sidebar.caption(f"context_key: {ctx['context_key']}")
            else:
                st.sidebar.caption(f"selected_wave_id: None (Portfolio)")
                st.sidebar.caption(f"context_key: {ctx['context_key']}")
    
    except (ImportError, KeyError, ValueError, AttributeError) as e:
        # Fallback if wave loading fails
        st.sidebar.warning("‚ö†Ô∏è Could not load wave list")
        if st.session_state.get("debug_mode", False):
            st.sidebar.error(f"Error: {str(e)}")
    
    st.sidebar.markdown("---")
    
    # ========================================================================
    # CLIENT MODE - Read-Only Health Information
    # ========================================================================
    st.sidebar.markdown("### üìä System Health")
    
    # Display read-only health information
    try:
        # Universe count - with freshness indicator
        wave_universe_version = st.session_state.get("wave_universe_version", 1)
        universe = get_canonical_wave_universe(force_reload=False, _wave_universe_version=wave_universe_version)
        all_waves = universe.get("waves", [])
        active_wave_count = len(all_waves)
        
        # Get timestamp if available
        universe_timestamp = universe.get("timestamp", "Unknown")
        
        st.sidebar.metric("Active Waves", active_wave_count)
        if universe_timestamp != "Unknown":
            st.sidebar.caption(f"Updated: {universe_timestamp}")
        
        # Data Age
        if "global_price_asof" in st.session_state and st.session_state.global_price_asof:
            asof_time = st.session_state.global_price_asof
            time_diff = datetime.now(timezone.utc) - asof_time
            minutes_ago = int(time_diff.total_seconds() / 60)
            
            if minutes_ago < 60:
                age_str = f"{minutes_ago} min"
            else:
                hours_ago = minutes_ago // 60
                age_str = f"{hours_ago}h {minutes_ago % 60}m"
            
            st.sidebar.metric("Data Age", age_str)
        
        # Last Price Date
        data_timestamp = get_latest_data_timestamp()
        st.sidebar.metric("Last Price Date", data_timestamp)
        
    except Exception as e:
        st.sidebar.warning("‚ö†Ô∏è Health info unavailable")
        if st.session_state.get("debug_mode", False):
            st.sidebar.error(f"Error: {str(e)}")
    
    st.sidebar.markdown("---")
    
    # ========================================================================
    # OPERATOR CONTROLS - Always Visible
    # ========================================================================
    st.sidebar.markdown("### üõ† Operator Controls")
    
    # Define safe logger for operator controls
    logger = logging.getLogger(__name__)
    if not logger.handlers:
        logging.basicConfig(level=logging.INFO)
    
    # Initialize last operator action in session state
    if "last_operator_action" not in st.session_state:
        st.session_state.last_operator_action = None
        st.session_state.last_operator_time = None
    
    # Clear Cache Button
    if st.sidebar.button("üóëÔ∏è Clear Cache", use_container_width=True, help="Clear Streamlit cache to force fresh computations"):
        try:
            # Clear both cache types safely
            st.cache_data.clear()
            try:
                st.cache_resource.clear()
            except AttributeError:
                # cache_resource may not be available in older Streamlit versions
                pass
            
            action_time = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')
            st.session_state.last_operator_action = "Clear Cache"
            st.session_state.last_operator_time = action_time
            
            # Log the action (fail-safe)
            try:
                logger.info(f"Operator action: Clear Cache at {action_time}")
            except Exception:
                pass  # Logging errors should not block button execution
            
            st.sidebar.success("‚úÖ Cache cleared")
            
            # Trigger rerun to refresh the app with cleared caches
            st.rerun()
        except Exception as e:
            st.sidebar.error(f"‚ùå Cache clear failed: {str(e)}")
    
    # Force Recompute Button
    if st.sidebar.button("‚ôªÔ∏è Force Recompute", use_container_width=True, help="Clear session state keys to trigger fresh computations"):
        try:
            # Safely delete/clear specific session state keys
            keys_to_clear = [
                'portfolio_alpha_ledger',
                'portfolio_snapshot_debug',
                'portfolio_exposure_series',
                'wave_data_cache',
                'price_book_cache',
                'compute_lock'
            ]
            
            cleared_count = 0
            for key in keys_to_clear:
                if key in st.session_state:
                    del st.session_state[key]
                    cleared_count += 1
            
            # Also clear Streamlit caches
            st.cache_data.clear()
            st.cache_resource.clear()
            
            action_time = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')
            st.session_state.last_operator_action = "Force Recompute"
            st.session_state.last_operator_time = action_time
            
            # Log the action (fail-safe)
            try:
                logger.info(f"Operator action: Force Recompute at {action_time} (cleared {cleared_count} keys)")
            except Exception:
                pass  # Logging errors should not block button execution
            
            st.sidebar.success(f"‚úÖ Cleared {cleared_count} keys")
            
            # Trigger rerun to refresh the app with recomputed data
            st.rerun()
        except Exception as e:
            st.sidebar.error(f"‚ùå Recompute failed: {str(e)}")
    
    # Hard Rerun Button
    if st.sidebar.button("üîÑ Hard Rerun", use_container_width=True, help="Trigger a full app rerun immediately"):
        try:
            action_time = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')
            st.session_state.last_operator_action = "Hard Rerun"
            st.session_state.last_operator_time = action_time
            
            # Log the action (fail-safe)
            try:
                logger.info(f"Operator action: Hard Rerun at {action_time}")
            except Exception:
                pass  # Logging errors should not block button execution
            
            st.rerun()
        except Exception as e:
            st.sidebar.error(f"‚ùå Rerun failed: {str(e)}")
    
    # Reload Price Book Button
    if st.sidebar.button("üìö Reload Price Book", use_container_width=True, help="Reload price_book from disk cache"):
        try:
            # Clear price_book_cache from session state to force reload
            if 'price_book_cache' in st.session_state:
                del st.session_state['price_book_cache']
            
            # Clear Streamlit caches for price book
            st.cache_data.clear()
            st.cache_resource.clear()
            
            action_time = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')
            st.session_state.last_operator_action = "Reload Price Book"
            st.session_state.last_operator_time = action_time
            
            # Log the action (fail-safe)
            try:
                logger.info(f"Operator action: Reload Price Book at {action_time}")
            except Exception:
                pass  # Logging errors should not block button execution
            
            st.sidebar.success("‚úÖ Price book reloaded")
            
            # Trigger rerun to refresh the app with reloaded data
            st.rerun()
        except Exception as e:
            st.sidebar.error(f"‚ùå Price book reload failed: {str(e)}")
    
    # Force Ledger Recompute Button
    if st.sidebar.button("üìä Force Ledger Recompute", use_container_width=True, help="Force re-computation of canonical Return Ledger from fresh price_book cache"):
        try:
            if OPERATOR_TOOLBOX_AVAILABLE and force_ledger_recompute:
                # Use the new comprehensive recompute function with diagnostic wrapper
                with st.spinner("Reloading price_book and rebuilding wave_history..."):
                    success, message, details = force_ledger_recompute()
                
                if success:
                    # Clear ledger-related session state keys to trigger fresh computation
                    ledger_keys = [
                        'portfolio_alpha_ledger',
                        'portfolio_snapshot_debug',
                        'portfolio_exposure_series',
                        'canonical_return_ledger',
                        'wave_data_cache',
                        'price_book_cache'
                    ]
                    
                    cleared_count = 0
                    for key in ledger_keys:
                        if key in st.session_state:
                            del st.session_state[key]
                            cleared_count += 1
                    
                    action_time = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')
                    st.session_state.last_operator_action = "Force Ledger Recompute"
                    st.session_state.last_operator_time = action_time
                    
                    # Log the action (fail-safe)
                    try:
                        logger.info(f"Operator action: Force Ledger Recompute at {action_time} (cleared {cleared_count} keys)")
                    except Exception:
                        pass  # Logging errors should not block button execution
                    
                    st.sidebar.success(message)
                    st.rerun()
                else:
                    # Display error message with diagnostics
                    st.sidebar.error(message)
                    
                    # If there's a traceback in details, show it
                    if 'traceback' in details:
                        st.sidebar.markdown("**üìç Stack Trace for Debugging:**")
                        st.sidebar.code(details['traceback'], language="python")
            else:
                # Fallback to old behavior if operator_toolbox not available
                ledger_keys = [
                    'portfolio_alpha_ledger',
                    'portfolio_snapshot_debug',
                    'portfolio_exposure_series',
                    'canonical_return_ledger'
                ]
                
                cleared_count = 0
                for key in ledger_keys:
                    if key in st.session_state:
                        del st.session_state[key]
                        cleared_count += 1
                
                action_time = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')
                st.session_state.last_operator_action = "Force Ledger Recompute"
                st.session_state.last_operator_time = action_time
                
                # Log the action (fail-safe)
                try:
                    logger.info(f"Operator action: Force Ledger Recompute at {action_time} (cleared {cleared_count} ledger keys)")
                except Exception:
                    pass  # Logging errors should not block button execution
                
                st.sidebar.success(f"‚úÖ Ledger recompute triggered ({cleared_count} keys cleared)")
        except Exception as e:
            # Diagnostic: Capture full stack trace for any error
            full_traceback = traceback.format_exc()
            st.sidebar.error(f"‚ùå Ledger recompute failed: {str(e)}")
            st.sidebar.markdown("**üìç Full Stack Trace:**")
            st.sidebar.code(full_traceback, language="python")
    
    st.sidebar.markdown("---")
    
    # ========================================================================
    # OPERATOR DIAGNOSTICS - Read-Only Info
    # ========================================================================
    st.sidebar.markdown("### üìã Diagnostics")
    
    # Build Marker
    try:
        build_info = get_build_info()
        build_marker = f"{build_info.get('sha', 'unknown')[:8]}"
        st.sidebar.caption(f"**Build marker:** `{build_marker}`")
    except Exception:
        st.sidebar.caption("**Build marker:** `SHA unavailable`")
    
    # Price Cache Max Date
    try:
        if get_price_book is not None and PRICE_BOOK_CONSTANTS_AVAILABLE:
            price_book = get_cached_price_book()
            if price_book is not None and not price_book.empty:
                max_date = price_book.index.max()
                max_date_str = max_date.strftime('%Y-%m-%d') if hasattr(max_date, 'strftime') else str(max_date)
                st.sidebar.caption(f"**Price cache max date:** `{max_date_str}`")
            else:
                st.sidebar.caption("**Price cache max date:** `N/A`")
        else:
            st.sidebar.caption("**Price cache max date:** `N/A`")
    except Exception as e:
        st.sidebar.caption(f"**Price cache max date:** `Error: {str(e)[:30]}`")
    
    # Ledger Max Date - Read from metadata or artifact if available
    try:
        ledger_max_date_display = None
        
        # First try to get from get_data_health_metadata if available
        if OPERATOR_TOOLBOX_AVAILABLE and get_data_health_metadata:
            try:
                health_metadata = get_data_health_metadata()
                ledger_max_date_display = health_metadata.get('ledger_max_date')
            except Exception:
                pass  # Fall back to session state check
        
        # If not found in metadata, check session state
        if not ledger_max_date_display and 'portfolio_alpha_ledger' in st.session_state:
            ledger = st.session_state['portfolio_alpha_ledger']
            if ledger and isinstance(ledger, dict) and ledger.get('success'):
                ledger_df = ledger.get('ledger')
                if ledger_df is not None and not ledger_df.empty:
                    max_date = ledger_df.index.max()
                    ledger_max_date_display = max_date.strftime('%Y-%m-%d') if hasattr(max_date, 'strftime') else str(max_date)
        
        # Display result
        if ledger_max_date_display:
            st.sidebar.caption(f"**Ledger max date:** `{ledger_max_date_display}`")
        else:
            st.sidebar.caption("**Ledger max date:** `N/A`")
    except Exception as e:
        st.sidebar.caption(f"**Ledger max date:** `Error: {str(e)[:30]}`")
    
    # Wave History Max Date
    try:
        wave_history_path = os.path.join(os.path.dirname(__file__), 'wave_history.csv')
        if os.path.exists(wave_history_path):
            import pandas as pd
            wave_history = pd.read_csv(wave_history_path)
            if 'date' in wave_history.columns and not wave_history.empty:
                wave_history['date'] = pd.to_datetime(wave_history['date'])
                max_date = wave_history['date'].max()
                max_date_str = max_date.strftime('%Y-%m-%d') if hasattr(max_date, 'strftime') else str(max_date)
                st.sidebar.caption(f"**Wave history max date:** `{max_date_str}`")
            else:
                st.sidebar.caption("**Wave history max date:** `N/A`")
        else:
            st.sidebar.caption("**Wave history max date:** `Not found`")
    except Exception as e:
        st.sidebar.caption(f"**Wave history max date:** `Error: {str(e)[:30]}`")
    
    # Display last operator action
    if st.session_state.last_operator_action:
        st.sidebar.caption(
            f"Last operator action: **{st.session_state.last_operator_action}** at {st.session_state.last_operator_time}"
        )
    
    st.sidebar.markdown("---")
    
    # ========================================================================
    # OPERATOR TOOLBOX - Always Visible
    # Low-risk, in-app toolbox for debugging and rebuilding
    # ========================================================================
    with st.sidebar.expander("üß∞ Operator Toolbox", expanded=False):
        if not OPERATOR_TOOLBOX_AVAILABLE:
            st.warning("‚ö†Ô∏è Operator Toolbox unavailable - helper module not loaded")
        else:
            st.markdown("### Data Health Panel")
            
            # Get data health metadata
            try:
                health_metadata = get_data_health_metadata()
                
                # Display key metrics
                col1, col2 = st.columns(2)
                
                with col1:
                    st.metric("Last Trading Day", health_metadata.get('last_trading_day') or 'N/A')
                    st.metric("Price Book Max", health_metadata.get('price_book_max_date') or 'N/A')
                
                with col2:
                    st.metric("Wave History Max", health_metadata.get('wave_history_max_date') or 'N/A')
                    
                    # Count missing tickers
                    missing_count = len(health_metadata.get('missing_tickers', []))
                    st.metric("Missing Tickers", missing_count)
                
                # Required symbols presence checks
                st.markdown("#### Required Symbols")
                
                benchmarks = health_metadata.get('required_symbols_present', {}).get('benchmarks', {})
                vix_any = health_metadata.get('required_symbols_present', {}).get('vix_any', False)
                tbill_any = health_metadata.get('required_symbols_present', {}).get('tbill_any', False)
                
                # Benchmarks (ALL required)
                spy_status = "‚úÖ" if benchmarks.get('SPY') else "‚ùå"
                qqq_status = "‚úÖ" if benchmarks.get('QQQ') else "‚ùå"
                iwm_status = "‚úÖ" if benchmarks.get('IWM') else "‚ùå"
                st.text(f"Benchmarks: SPY {spy_status} QQQ {qqq_status} IWM {iwm_status}")
                
                # VIX variants (ANY required)
                vix_status = "‚úÖ" if vix_any else "‚ùå"
                st.text(f"VIX (any): {vix_status}")
                
                # T-bill variants (ANY required)
                tbill_status = "‚úÖ" if tbill_any else "‚ùå"
                st.text(f"T-bill (any): {tbill_status}")
                
                # Show missing tickers if any
                if missing_count > 0:
                    with st.expander(f"Missing Tickers ({missing_count})"):
                        missing_tickers = health_metadata.get('missing_tickers', [])
                        st.text(", ".join(missing_tickers[:20]))
                        if missing_count > 20:
                            st.caption(f"... and {missing_count - 20} more")
                
                # Show errors if any
                errors = health_metadata.get('errors', [])
                if errors:
                    with st.expander(f"‚ö†Ô∏è Errors ({len(errors)})"):
                        for error in errors:
                            st.error(error)
                
            except Exception as e:
                st.error(f"‚ùå Error loading health metadata: {str(e)}")
            
            st.markdown("---")
            st.markdown("### Interactive Actions")
            
            # Clear Streamlit Cache button
            if st.button("üóëÔ∏è Clear Streamlit Cache", key="toolbox_clear_cache", use_container_width=True):
                try:
                    st.cache_data.clear()
                    try:
                        st.cache_resource.clear()
                    except AttributeError:
                        pass
                    st.success("‚úÖ Cache cleared")
                except Exception as e:
                    st.error(f"‚ùå Error: {str(e)}")
            
            # Clear Session State (Soft Reset) button
            if st.button("‚ôªÔ∏è Clear Session State (Soft Reset)", key="toolbox_soft_reset", use_container_width=True):
                try:
                    # Clear non-critical session keys
                    keys_to_clear = [
                        'portfolio_alpha_ledger',
                        'portfolio_snapshot_debug',
                        'portfolio_exposure_series',
                        'wave_data_cache',
                        'price_book_cache'
                    ]
                    
                    cleared_count = 0
                    for key in keys_to_clear:
                        if key in st.session_state:
                            del st.session_state[key]
                            cleared_count += 1
                    
                    st.success(f"‚úÖ Cleared {cleared_count} keys")
                    st.rerun()
                except Exception as e:
                    st.error(f"‚ùå Error: {str(e)}")
            
            # Rebuild Price Cache button
            if st.button("üî® Rebuild Price Cache (price_book)", key="toolbox_rebuild_price_cache", use_container_width=True):
                try:
                    with st.spinner("Rebuilding price cache... (this may take a few minutes)"):
                        success, message = rebuild_price_cache_toolbox()
                    
                    if success:
                        st.success(message)
                        st.rerun()
                    else:
                        st.error(message)
                except Exception as e:
                    st.error(f"‚ùå Error: {str(e)}")
            
            # Rebuild wave_history button
            if st.button("üìä Rebuild wave_history from price_book", key="toolbox_rebuild_wave_history", use_container_width=True):
                try:
                    with st.spinner("Rebuilding wave_history... (this may take a few minutes)"):
                        success, message = rebuild_wave_history()
                    
                    if success:
                        st.success(message)
                        st.rerun()
                    else:
                        st.error(message)
                except Exception as e:
                    st.error(f"‚ùå Error: {str(e)}")
            
            # Force Ledger Recompute button (comprehensive)
            if st.button("üîÑ Force Ledger Recompute (Full Pipeline)", key="toolbox_force_ledger_recompute", use_container_width=True):
                try:
                    with st.spinner("Reloading price_book, rebuilding wave_history, and clearing ledger cache..."):
                        success, message, details = force_ledger_recompute()
                    
                    if success:
                        # Clear ledger-related session state
                        ledger_keys = [
                            'portfolio_alpha_ledger',
                            'portfolio_snapshot_debug',
                            'portfolio_exposure_series',
                            'canonical_return_ledger',
                            'wave_data_cache',
                            'price_book_cache'
                        ]
                        
                        cleared_count = 0
                        for key in ledger_keys:
                            if key in st.session_state:
                                del st.session_state[key]
                                cleared_count += 1
                        
                        st.success(f"{message}\n\n‚úÖ Cleared {cleared_count} cached keys")
                        st.rerun()
                    else:
                        # Display error message with diagnostics
                        st.error(message)
                        
                        # If there's a traceback in details, show it
                        if 'traceback' in details:
                            st.markdown("**üìç Stack Trace for Debugging:**")
                            st.code(details['traceback'], language="python")
                except Exception as e:
                    # Diagnostic: Capture full stack trace for any error during unpacking or processing
                    full_traceback = traceback.format_exc()
                    st.error(f"‚ùå Error: {str(e)}")
                    st.markdown("**üìç Full Stack Trace:**")
                    st.code(full_traceback, language="python")
            
            # Run Self-Test button
            if st.button("üîç Run Self-Test", key="toolbox_self_test", use_container_width=True):
                try:
                    with st.spinner("Running self-test suite..."):
                        test_results = run_self_test()
                    
                    # Display overall status
                    if test_results['overall_status'] == 'PASS':
                        st.success(f"‚úÖ PASS - {test_results['summary']}")
                    else:
                        st.error(f"‚ùå FAIL - {test_results['summary']}")
                    
                    # Display individual test results
                    st.markdown("#### Test Results")
                    for test in test_results['tests']:
                        status_icon = "‚úÖ" if test['status'] == 'PASS' else "‚ùå"
                        with st.expander(f"{status_icon} {test['name']} - {test['status']}"):
                            st.text(test['message'])
                    
                    st.caption(f"Test run timestamp: {test_results['timestamp']}")
                    
                except Exception as e:
                    st.error(f"‚ùå Error running self-test: {str(e)}")
    
    st.sidebar.markdown("---")
    
    # ========================================================================
    # OPERATOR MODE (Admin-Gated)
    # Operator Mode hidden by default; enable via OPERATOR_MODE secret.
    # Set OPERATOR_MODE = true in .streamlit/secrets.toml to enable.
    # ========================================================================
    operator_mode_allowed = False
    try:
        operator_mode_allowed = st.secrets.get('OPERATOR_MODE', False)
    except Exception:
        # Secrets not available, OPERATOR_MODE not set, or secrets parsing error
        pass
    
    if operator_mode_allowed:
        with st.sidebar.expander("‚öôÔ∏è Operator Controls (Admin)", expanded=False):
            operator_mode_enabled = st.checkbox(
                "Enable Operator Mode",
                value=st.session_state.get("operator_mode_enabled", False),
                key="operator_mode_toggle",
                help="Enable advanced operator controls and tools"
            )
            st.session_state["operator_mode_enabled"] = operator_mode_enabled
            
            if operator_mode_enabled:
                st.info("üîì Operator Mode Active")
                
                # ========================================================================
                # OPERATOR CONTROLS - Safe Mode
                # ========================================================================
                st.markdown("#### üõ°Ô∏è Safe Mode")
                
                # Safe Mode (No Fetch / No Compute) - Default ON
                safe_mode_no_fetch = st.checkbox(
                    "Safe Mode (No Fetch / No Compute)",
                    value=st.session_state.get("safe_mode_no_fetch", True),
                    key="safe_mode_no_fetch_toggle",
                    help="When ON: Prevents all network calls (yfinance, Alpaca, Coinbase) and snapshot builds. Loads pre-existing snapshots only."
                )
                st.session_state["safe_mode_no_fetch"] = safe_mode_no_fetch
                
                if safe_mode_no_fetch:
                    st.info("üõ°Ô∏è SAFE MODE ACTIVE - No external data calls")
                
                st.markdown("---")
                
                # ========================================================================
                # OPERATOR CONTROLS - Debug Mode
                # ========================================================================
                st.markdown("#### üêõ Debug Mode")
                
                # Allow Continuous Reruns checkbox (default OFF)
                allow_continuous_reruns = st.checkbox(
                    "Allow Continuous Reruns (Debug)",
                    value=st.session_state.get("allow_continuous_reruns", False),
                    key="allow_continuous_reruns_toggle",
                    help="When OFF: App stops after 2 runs to prevent infinite loops. Enable this for debugging or when continuous auto-refresh is needed."
                )
                st.session_state["allow_continuous_reruns"] = allow_continuous_reruns
                
                if not allow_continuous_reruns:
                    st.warning("‚ö†Ô∏è Loop Trap Active - Max 2 runs")
                else:
                    st.info("üîÑ Continuous reruns enabled")
                
                # Reset Compute Lock button (for diagnostics)
                if st.button(
                    "Reset Compute Lock",
                    key="reset_compute_lock_button",
                    use_container_width=True,
                    help="Reset the global compute lock to allow heavy computations again. Use this if computations appear stuck."
                ):
                    st.session_state["compute_lock"] = False
                    st.session_state["compute_lock_reason"] = None
                    st.success("‚úÖ Compute lock reset")
                
                st.markdown("---")
                
                # ========================================================================
                # OPERATOR CONTROLS - Manual Snapshot Rebuild Buttons
                # ========================================================================
                st.markdown("#### üîß Manual Snapshot Rebuild")
                
                # Main Snapshot Rebuild Button
                if st.button(
                    "Rebuild Snapshot Now (Manual)",
                    key="manual_rebuild_snapshot_button",
                    use_container_width=True,
                    disabled=safe_mode_no_fetch,
                    help="Manually rebuild the main snapshot. Safe Mode must be OFF."
                ):
                    try:
                        from analytics_pipeline import generate_live_snapshot
                        
                        st.info("‚è≥ Building snapshot...")
                        
                        # Temporarily allow the build by creating a temporary session state
                        temp_session_state = dict(st.session_state)
                        temp_session_state["safe_mode_no_fetch"] = False
                        
                        
                        snapshot_df = generate_live_snapshot(
                            output_path="data/live_snapshot.csv",
                            session_state=temp_session_state
                        )
                        
                        if snapshot_df is not None and not snapshot_df.empty:
                            st.success(f"‚úÖ Snapshot rebuilt: {len(snapshot_df)} rows")
                            # Reset run guard counter on successful rebuild
                            st.session_state.run_count = 0
                            st.session_state.loop_detected = False
                            # Mark user interaction
                            st.session_state.user_interaction_detected = True
                            trigger_rerun("rebuild_snapshot_manual")
                        else:
                            st.warning("‚ö†Ô∏è Snapshot rebuild returned empty data")
                    except Exception as e:
                        st.error(f"‚ùå Snapshot rebuild failed: {str(e)}")
                
                # Proxy Snapshot Rebuild Button
                if st.button(
                    "Rebuild Proxy Snapshot Now (Manual)",
                    key="manual_rebuild_proxy_snapshot_button",
                    use_container_width=True,
                    disabled=safe_mode_no_fetch,
                    help="Manually rebuild the proxy snapshot. Safe Mode must be OFF."
                ):
                    try:
                        from planb_proxy_pipeline import build_proxy_snapshot
                        
                        st.info("‚è≥ Building proxy snapshot...")
                        
                        # Temporarily allow the build by creating a temporary session state
                        temp_session_state = dict(st.session_state)
                        temp_session_state["safe_mode_no_fetch"] = False  # Temporarily disable for manual build
                        
                        # Build proxy snapshot with explicit button click flag
                        proxy_df = build_proxy_snapshot(
                            days=365,
                            session_state=temp_session_state,
                            explicit_button_click=True
                        )
                        
                        if proxy_df is not None and not proxy_df.empty:
                            st.success(f"‚úÖ Proxy snapshot rebuilt: {len(proxy_df)} rows")
                            # Reset run guard counter on successful rebuild
                            st.session_state.run_count = 0
                            st.session_state.loop_detected = False
                            # Mark user interaction
                            st.session_state.user_interaction_detected = True
                            trigger_rerun("rebuild_proxy_snapshot_manual")
                        else:
                            st.warning("‚ö†Ô∏è Proxy snapshot rebuild returned empty data")
                    except Exception as e:
                        st.error(f"‚ùå Proxy snapshot rebuild failed: {str(e)}")
                
                st.markdown("---")
                
                # ========================================================================
                # OPERATOR CONTROLS - Feature Toggles
                # ========================================================================
                st.markdown("#### ‚öôÔ∏è Feature Settings")
                
                # SAFE_MODE toggle (environment variable can override, but sidebar provides UI control)
                safe_mode_default = os.environ.get("SAFE_MODE", "False").lower() == "true"
                safe_mode_ui = st.checkbox(
                    "Enable Safe Mode (Wave IC)",
                    value=safe_mode_default,
                    key="safe_mode_ui_toggle",
                    help="When enabled, catches errors in Wave Intelligence Center and provides graceful fallback"
                )
                
                # Update global SAFE_MODE based on UI toggle (allows runtime control)
                # Note: This doesn't actually update the constant, but we can use session_state
                st.session_state["safe_mode_enabled"] = safe_mode_ui
                
                # RENDER_RICH_HTML toggle
                render_rich_html_default = os.environ.get("RENDER_RICH_HTML", "True").lower() == "true"
                render_rich_html_ui = st.checkbox(
                    "Enable Rich HTML Rendering",
                    value=render_rich_html_default,
                    key="render_rich_html_ui_toggle",
                    help="Use st.components.v1.html for rich HTML blocks (disable for simpler rendering)"
                )
                st.session_state["render_rich_html_enabled"] = render_rich_html_ui
                
                # Debug Mode toggle (default OFF per requirements)
                debug_mode_ui = st.checkbox(
                    "üêõ Debug Mode",
                    value=st.session_state.get("debug_mode", False),
                    key="debug_mode_ui_toggle",
                    help="Show detailed error messages and diagnostics when components fail (default: OFF)"
                )
                st.session_state["debug_mode"] = debug_mode_ui
                
                st.markdown("---")
                
                # ========================================================================
                # OPERATOR CONTROLS - Quick Actions
                # ========================================================================
                st.markdown("#### ‚ö° Quick Actions")
                
                if st.button(
                    "üîÑ Force Reload Wave Universe",
                    key="force_reload_universe_top_button",
                    use_container_width=True,
                    type="primary"
                ):
                    try:
                        # Increment wave universe version
                        if "wave_universe_version" not in st.session_state:
                            st.session_state.wave_universe_version = 1
                        st.session_state.wave_universe_version += 1
                        
                        # Clear wave universe cache from session state using constant
                        for key in WAVE_UNIVERSE_CACHE_KEYS:
                            if key in st.session_state:
                                del st.session_state[key]
                        
                        # Clear Streamlit caches
                        st.cache_data.clear()
                        st.cache_resource.clear()
                        
                        # Set force reload flag
                        st.session_state["force_reload_universe"] = True
                        
                        # Mark user interaction
                        st.session_state.user_interaction_detected = True
                        
                        # Trigger immediate rerun
                        trigger_rerun("force_reload_waves")
                    except Exception as e:
                        st.warning(f"Force reload unavailable: {str(e)}")
                
                # Force Reload Data Button (Clear All Caches) - Add confirmation
                clear_cache_confirm = st.checkbox(
                    "Confirm Clear Cache",
                    key="clear_cache_confirm_checkbox",
                    help="Check this box to enable the Clear Cache button"
                )
                
                if st.button(
                    "üßπ Force Reload Data (Clear Cache)",
                    key="force_reload_data_button",
                    use_container_width=True,
                    disabled=not clear_cache_confirm,
                    help="Clear all cached data and reload from files. Requires confirmation."
                ):
                    try:
                        # Clear all Streamlit caches
                        st.cache_data.clear()
                        st.cache_resource.clear()
                        
                        # Clear wave/history-related session state keys
                        keys_to_clear = [
                            "wave_universe",
                            "waves_list",
                            "universe_cache",
                            "wave_history_cache",
                            "last_compute_ts",
                            "alpha_proof_result",
                            "alpha_proof_wave",
                            "attrib_result"
                        ]
                        for key in keys_to_clear:
                            if key in st.session_state:
                                del st.session_state[key]
                        
                        # Increment wave universe version to force reload
                        if "wave_universe_version" not in st.session_state:
                            st.session_state.wave_universe_version = 1
                        st.session_state.wave_universe_version += 1
                        
                        # Show success message
                        st.success("‚úÖ Cache cleared successfully!")
                        
                        # Mark user interaction
                        st.session_state.user_interaction_detected = True
                        
                        # Trigger rerun
                        trigger_rerun("force_reload_data_clear_cache")
                    except Exception as e:
                        st.error(f"Error clearing cache: {str(e)}")
                
                # Rebuild Price Cache Button
                if st.button(
                    "üí∞ Rebuild Price Cache (Active Tickers Only)",
                    key="rebuild_price_cache_button",
                    use_container_width=True,
                    help="Explicitly rebuild the canonical price cache with active wave tickers only. Requires ALLOW_NETWORK_FETCH=true (PRICE_FETCH_ENABLED)."
                ):
                    try:
                        # Show progress indicator
                        with st.spinner("Rebuilding price cache... This may take a few minutes."):
                            # Import the rebuild function
                            from helpers.price_book import rebuild_price_cache
                            
                            # Call rebuild (this checks PRICE_FETCH_ENABLED internally)
                            result = rebuild_price_cache(active_only=True)
                            
                            # Check if fetching is allowed
                            if not result['allowed']:
                                st.warning(
                                    "‚ö†Ô∏è Price fetching is DISABLED (ALLOW_NETWORK_FETCH=False)\n\n"
                                    f"{result.get('message', 'Set PRICE_FETCH_ENABLED=true or ALLOW_NETWORK_FETCH=true to enable fetching.')}"
                                )
                            elif result['success']:
                                st.success(
                                    f"‚úÖ Price cache rebuilt!\n\n"
                                    f"üìä {result['tickers_fetched']}/{result['tickers_requested']} tickers fetched\n"
                                    f"üìÖ Latest Date: {result['date_max']}"
                                )
                                
                                # Show failed tickers if any
                                if result['tickers_failed'] > 0 and result['failures']:
                                    st.warning(
                                        f"‚ö†Ô∏è {result['tickers_failed']} tickers failed\n\n"
                                        f"See data/cache/failed_tickers.csv for details"
                                    )
                            else:
                                st.error("‚ùå Failed to rebuild price cache")
                            
                            # Mark user interaction
                            st.session_state.user_interaction_detected = True
                            
                            # Clear Streamlit caches to reflect new data
                            st.cache_data.clear()
                            
                            # Trigger rerun
                            trigger_rerun("rebuild_price_cache")
                    except Exception as e:
                        st.error(f"Error rebuilding cache: {str(e)}")
                        import traceback
                        st.code(traceback.format_exc())
                
                # ========================================================================
                # OPERATOR CONTROLS - Force Build Data for All Waves Button
                # ========================================================================
                if st.button(
                    "üî® Force Build Data for All Waves",
                    key="force_build_all_waves_button",
                    use_container_width=True,
                    help="Trigger price prefetch, update readiness statuses, and generate diagnostics for all waves"
                ):
                    try:
                        # Show progress indicator
                        with st.spinner("Prefetching prices for all waves..."):
                            # Clear diagnostics tracker
                            try:
                                from helpers.ticker_diagnostics import get_diagnostics_tracker
                                tracker = get_diagnostics_tracker()
                                tracker.clear()
                            except ImportError:
                                pass
                            
                            # Set force rebuild flag
                            st.session_state.force_price_cache_rebuild = True
                            
                            # Clear any previous rate-limited flags
                            if "rate_limited_waves" in st.session_state:
                                del st.session_state["rate_limited_waves"]
                            
                            # Force refresh the canonical price cache (PRICE_BOOK)
                            from helpers.price_loader import refresh_price_cache
                            cache_result = refresh_price_cache(active_only=True)
                            
                            # Show results
                            success_count = cache_result.get("tickers_fetched", 0)
                            failure_count = cache_result.get("tickers_failed", 0)
                            
                            st.success(f"‚úÖ Prefetch complete: {success_count} tickers succeeded, {failure_count} failed")
                            
                            # Generate and export diagnostics report if there are failures
                            if failure_count > 0:
                                try:
                                    from helpers.ticker_diagnostics import get_diagnostics_tracker
                                    tracker = get_diagnostics_tracker()
                                    report_path = tracker.export_to_csv()
                                    st.info(f"üìä Diagnostics report saved to: {report_path}")
                                except Exception:
                                    pass
                            
                            # Force reload diagnostics
                            st.cache_data.clear()
                            # Mark user interaction
                            st.session_state.user_interaction_detected = True
                            trigger_rerun("force_build_all_waves")
                    except Exception as e:
                        st.error(f"Error building data: {str(e)}")
                
                st.markdown("---")
                
                # ========================================================================
                # OPERATOR CONTROLS - Rebuild Wave CSV
                # ========================================================================
                if st.button(
                    "üìã Rebuild Wave CSV + Clear Cache",
                    key="rebuild_wave_csv_button",
                    use_container_width=True,
                    help="Rebuild wave registry CSV from canonical source and clear all cached data"
                ):
                    try:
                        # Show progress indicator
                        with st.spinner("Rebuilding wave registry CSV and clearing cache..."):
                            # Import wave registry manager
                            from wave_registry_manager import rebuild_wave_registry_csv
                            
                            # Rebuild the CSV
                            rebuild_result = rebuild_wave_registry_csv(force=True)
                            
                            if rebuild_result['success']:
                                st.success(f"‚úÖ Wave CSV rebuilt with {rebuild_result['waves_written']} waves")
                                
                                # Clear all cached data
                                st.cache_data.clear()
                                st.cache_resource.clear()
                                
                                # Clear wave-related session state
                                keys_to_clear = [
                                    "wave_universe",
                                    "waves_list",
                                    "universe_cache",
                                    "wave_history_cache",
                                    "last_compute_ts",
                                    "alpha_proof_result",
                                    "alpha_proof_wave",
                                    "attrib_result",
                                    "global_price_df",
                                    "global_price_failures",
                                    "global_price_asof"
                                ]
                                for key in keys_to_clear:
                                    if key in st.session_state:
                                        del st.session_state[key]
                                
                                # Force rebuild price cache
                                st.session_state.force_price_cache_rebuild = True
                                
                                # Rebuild canonical price cache (PRICE_BOOK)
                                try:
                                    from helpers.price_loader import refresh_price_cache
                                    cache_result = refresh_price_cache(active_only=True)
                                    
                                    # Get summary statistics
                                    waves_total = rebuild_result['waves_written']
                                    success_count = cache_result.get("tickers_fetched", 0)
                                    failures = cache_result.get("failures", {})
                                    tickers_failed_count = cache_result.get("tickers_failed", 0)
                                    
                                    # Display summary
                                    st.info(f"""
**Rebuild Summary:**
- Total Waves: {waves_total}
- Waves Loaded: {waves_total}
- Tickers Success: {success_count}
- Tickers Failed: {tickers_failed_count}
                                    """)
                                    
                                    # Show failed tickers if any
                                    if tickers_failed_count > 0 and failures:
                                        st.warning(f"‚ö†Ô∏è {tickers_failed_count} tickers failed to load")
                                        
                                        # Build failed ticker table (limited display)
                                        failed_ticker_data = []
                                        for ticker, error in list(failures.items())[:20]:  # Show first 20
                                            failed_ticker_data.append({
                                                'Ticker': ticker,
                                                'Error': str(error)[:50],  # Truncate long errors
                                            })
                                        
                                        if failed_ticker_data:
                                            failed_df = pd.DataFrame(failed_ticker_data)
                                            st.dataframe(failed_df, use_container_width=True, height=min(200, len(failed_df) * 35 + 35))
                                except Exception as cache_error:
                                    st.warning(f"‚ö†Ô∏è Could not rebuild cache: {str(cache_error)}")
                                
                                # Increment version to force reload
                                if "wave_universe_version" not in st.session_state:
                                    st.session_state.wave_universe_version = 1
                                st.session_state.wave_universe_version += 1
                                
                                # Trigger rerun
                                # Mark user interaction
                                st.session_state.user_interaction_detected = True
                                trigger_rerun("rebuild_wave_csv")
                            else:
                                st.error(f"‚ùå Failed to rebuild wave CSV: {rebuild_result['errors']}")
                    except Exception as e:
                        st.error(f"‚ùå Error rebuilding wave CSV: {str(e)}")
                
                st.markdown("---")
                
                # ========================================================================
                # OPERATOR CONTROLS - Data Refresh TTL Selector
                # ========================================================================
                st.markdown("#### üïê Data Refresh Settings")
                
                ttl_options = {
                    "1 hour": 3600,
                    "2 hours": 7200,
                    "4 hours": 14400,
                    "8 hours": 28800,
                    "12 hours": 43200,
                    "24 hours": 86400
                }
                
                # Get current TTL from session state
                current_ttl = st.session_state.get("price_cache_ttl_seconds", 7200)
                
                # Find the label for current TTL
                current_label = "2 hours"  # default
                for label, value in ttl_options.items():
                    if value == current_ttl:
                        current_label = label
                        break
                
                selected_ttl_label = st.selectbox(
                    "Data Refresh TTL",
                    options=list(ttl_options.keys()),
                    index=list(ttl_options.keys()).index(current_label),
                    key="ttl_selector",
                    help="How long to cache price data before refreshing"
                )
                
                # Update session state
                st.session_state.price_cache_ttl_seconds = ttl_options[selected_ttl_label]
                
                # Show cache status if available
                if "global_price_asof" in st.session_state and st.session_state.global_price_asof:
                    asof_time = st.session_state.global_price_asof
                    time_diff = datetime.now(timezone.utc) - asof_time
                    minutes_ago = int(time_diff.total_seconds() / 60)
                    
                    if minutes_ago < 60:
                        age_str = f"{minutes_ago} min ago"
                    else:
                        hours_ago = minutes_ago // 60
                        age_str = f"{hours_ago}h {minutes_ago % 60}m ago"
                    
                    success_count = st.session_state.get("global_price_success_count", 0)
                    ticker_count = st.session_state.get("global_price_ticker_count", 0)
                    
                    st.caption(f"üìä Cache: {success_count}/{ticker_count} tickers ({age_str})")
                
                st.markdown("---")
                
                # ========================================================================
                # OPERATOR CONTROLS - Activate All Waves Button
                # ========================================================================
                if st.button(
                    "‚úÖ Activate All Waves",
                    key="activate_all_waves_button",
                    use_container_width=True,
                    help="Enable all waves in the universe (set enabled=True for all)"
                ):
                    try:
                        # Get current universe
                        wave_universe_version = st.session_state.get("wave_universe_version", 1)
                        universe = get_canonical_wave_universe(force_reload=False, _wave_universe_version=wave_universe_version)
                        
                        # Create enabled flags dictionary with all waves set to True
                        enabled_flags = {wave: True for wave in universe.get("waves", [])}
                        
                        # Store in session state
                        st.session_state["wave_enabled_flags"] = enabled_flags
                        
                        # Force reload universe to pick up new flags
                        st.session_state.wave_universe_version = wave_universe_version + 1
                        st.cache_data.clear()
                        
                        st.success(f"‚úÖ Activated all {len(enabled_flags)} waves!")
                        # Mark user interaction
                        st.session_state.user_interaction_detected = True
                        trigger_rerun("activate_all_waves")
                    except Exception as e:
                        st.error(f"Error activating waves: {str(e)}")
                
                # ========================================================================
                # OPERATOR CONTROLS - Warm Cache Button
                # ========================================================================
                if st.button(
                    "üî• Warm Cache",
                    key="warm_cache_button",
                    use_container_width=True,
                    help="Prefetch and cache price data to ensure fast startup (uses canonical cache)"
                ):
                    # Debug trace marker
                    if st.session_state.get("debug_mode", False):
                        st.caption("üîç Trace: Entering warm cache")
                    
                    try:
                        with st.spinner("Warming cache with price data..."):
                            # Use canonical price cache refresh
                            from helpers.price_loader import refresh_price_cache, check_cache_readiness
                            
                            result = refresh_price_cache(active_only=True)
                            
                            # Show results
                            if result['success']:
                                st.success(
                                    f"‚úÖ Cache warmed!\n\n"
                                    f"üìä {result['tickers_fetched']}/{result['tickers_requested']} tickers fetched"
                                )
                                
                                if result['tickers_failed'] > 0:
                                    st.warning(
                                        f"‚ö†Ô∏è {result['tickers_failed']} tickers failed\n\n"
                                        f"See data/cache/failed_tickers.csv for details"
                                    )
                            else:
                                st.error("‚ùå Failed to warm cache")
                            
                            # Check readiness after refresh
                            readiness = check_cache_readiness(active_only=True)
                            if readiness['ready']:
                                st.success(f"‚úÖ Cache is READY")
                            else:
                                st.warning(f"‚ö†Ô∏è Cache status: {readiness['status_code']}")
                                    
                    except Exception as e:
                        st.error(f"‚ùå Error warming cache: {str(e)}")
                        # Don't crash - let user continue working
                
                st.markdown("---")
                
                # ========================================================================
                # OPERATOR CONTROLS - Auto-Refresh Control
                # ========================================================================
                st.markdown("#### üîÑ Auto-Refresh Control")
                
                # Check if st_autorefresh or st.autorefresh is available
                autorefresh_available = False
                try:
                    # Try importing st_autorefresh from streamlit-autorefresh
                    from streamlit_autorefresh import st_autorefresh
                    autorefresh_available = True
                except ImportError:
                    # Check if built-in autorefresh is available (newer Streamlit versions)
                    if hasattr(st, 'autorefresh'):
                        autorefresh_available = True
                
                if autorefresh_available:
                    # Initialize auto-refresh setting (using DEFAULT from config)
                    if "auto_refresh_enabled" not in st.session_state:
                        st.session_state.auto_refresh_enabled = DEFAULT_AUTO_REFRESH_ENABLED
                    
                    # Initialize auto-refresh interval
                    if "auto_refresh_interval_ms" not in st.session_state:
                        st.session_state.auto_refresh_interval_ms = DEFAULT_REFRESH_INTERVAL_MS
                    
                    # Toggle switch
                    auto_refresh_enabled = st.checkbox(
                        "Enable Auto-Refresh",
                        value=st.session_state.auto_refresh_enabled and not st.session_state.get("auto_refresh_paused", False),
                        key="auto_refresh_toggle",
                        help="Automatically refresh live analytics, overlays, attribution, and diagnostics at configured interval"
                    )
                    
                    # Update session state
                    st.session_state.auto_refresh_enabled = auto_refresh_enabled
                    
                    # If re-enabled, reset paused state
                    if auto_refresh_enabled and st.session_state.get("auto_refresh_paused", False):
                        st.session_state.auto_refresh_paused = False
                        st.session_state.auto_refresh_error_count = 0
                        st.session_state.auto_refresh_error_message = None
                    
                    # Interval selector (only show when enabled)
                    if auto_refresh_enabled:
                        # Get interval options
                        interval_options = list(REFRESH_INTERVAL_OPTIONS.keys())
                        current_interval_ms = st.session_state.get("auto_refresh_interval_ms", DEFAULT_REFRESH_INTERVAL_MS)
                        
                        # Find current selection index
                        current_index = 0
                        for i, (name, value) in enumerate(REFRESH_INTERVAL_OPTIONS.items()):
                            if value == current_interval_ms:
                                current_index = i
                                break
                        
                        selected_interval_name = st.selectbox(
                            "Refresh Interval",
                            options=interval_options,
                            index=current_index,
                            key="auto_refresh_interval_selector",
                            help="How frequently to refresh live data. Default: 1 minute"
                        )
                        
                        # Update interval in session state
                        st.session_state.auto_refresh_interval_ms = REFRESH_INTERVAL_OPTIONS[selected_interval_name]
                    
                    # Show status
                    auto_refresh_paused = st.session_state.get("auto_refresh_paused", False)
                    
                    if auto_refresh_paused:
                        st.error("‚ö†Ô∏è Auto-refresh PAUSED due to errors")
                        error_msg = st.session_state.get("auto_refresh_error_message", "Unknown error")
                        st.caption(f"Error: {error_msg}")
                        st.caption("Re-enable the checkbox above to resume")
                    elif auto_refresh_enabled:
                        interval_name = REFRESH_INTERVAL_OPTIONS.get(
                            st.session_state.get("auto_refresh_interval_ms", DEFAULT_REFRESH_INTERVAL_MS),
                            "1 minute"
                        )
                        if AUTO_REFRESH_CONFIG_AVAILABLE:
                            interval_name = get_interval_display_name(st.session_state.get("auto_refresh_interval_ms", DEFAULT_REFRESH_INTERVAL_MS))
                        
                        st.success(f"üü¢ Auto-refresh is ON")
                        st.caption(f"Refreshes every {interval_name}")
                        
                        # Show last successful refresh
                        last_successful = st.session_state.get("last_successful_refresh_time", datetime.now())
                        st.caption(f"Last update: {last_successful.strftime('%H:%M:%S')}")
                        
                        # Show scope information
                        with st.expander("‚ÑπÔ∏è What gets refreshed?"):
                            st.caption("**Included in auto-refresh:**")
                            st.caption("‚Ä¢ Live analytics & metrics")
                            st.caption("‚Ä¢ VIX overlays & regime detection")
                            st.caption("‚Ä¢ Alpha attribution")
                            st.caption("‚Ä¢ System diagnostics")
                            st.caption("‚Ä¢ Summary statistics")
                            st.caption("")
                            st.caption("**Excluded (cached):**")
                            st.caption("‚Ä¢ Historical backtests")
                            st.caption("‚Ä¢ Heavy simulations")
                            st.caption("‚Ä¢ Report generation")
                    else:
                        st.info("üî¥ Auto-refresh is OFF")
                        st.caption("Enable above for live updates")
                else:
                    # Auto-refresh not available
                    st.warning("‚ö†Ô∏è Auto-refresh unavailable")
                    st.caption("Install streamlit-autorefresh:")
                    st.code("pip install streamlit-autorefresh", language="bash")
                    # Ensure auto-refresh is disabled
                    st.session_state.auto_refresh_enabled = False
                
                st.markdown("---")
                
                # ========================================================================
                # OPERATOR CONTROLS - Bottom Ticker Bar Control
                # ========================================================================
                st.markdown("#### üìä Bottom Ticker Bar")
                
                # Initialize ticker setting if not present
                if "show_bottom_ticker" not in st.session_state:
                    st.session_state.show_bottom_ticker = True  # Default: ON
                
                # Checkbox control
                show_ticker = st.checkbox(
                    "Show bottom ticker",
                    value=st.session_state.show_bottom_ticker,
                    key="show_ticker_toggle",
                    help="Display scrolling ticker bar at the bottom with portfolio info, earnings dates, and Fed data"
                )
                
                # Update session state
                st.session_state.show_bottom_ticker = show_ticker
                
                # Show status
                if show_ticker:
                    st.success("üü¢ Ticker bar is visible")
                    st.caption("Displays portfolio tickers, earnings, and Fed data")
                else:
                    st.info("üî¥ Ticker bar is hidden")
                
                st.markdown("---")
                
                # ========================================================================
                # OPERATOR CONTROLS - Ops Controls (Admin)
                # ========================================================================
                st.markdown("#### üõ†Ô∏è Ops Controls")
                
                # Confirmation checkbox
                ops_confirmation = st.checkbox(
                    "I understand this will reset cached data.",
                    key="ops_confirmation"
                )
                
                # Clear Streamlit Cache Button with confirmation
                if st.button(
                    "Clear Streamlit Cache",
                    disabled=not ops_confirmation,
                    key="clear_cache_button",
                    use_container_width=True
                ):
                    try:
                        st.cache_data.clear()
                        st.cache_resource.clear()
                        st.success("Cache cleared.")
                    except Exception:
                        st.warning("Cache clear unavailable.")
                
                # Reset Session State Button
                if st.button(
                    "Reset Session State",
                    disabled=not ops_confirmation,
                    key="reset_session_button",
                    use_container_width=True
                ):
                    # Preserve system keys (navigation-related)
                    system_keys = [key for key in st.session_state.keys() if key.startswith('_')]
                    
                    # Clear all non-system keys
                    keys_to_delete = [key for key in st.session_state.keys() if key not in system_keys]
                    for key in keys_to_delete:
                        del st.session_state[key]
                    
                    st.success("Session state reset.")
                
                # Force Reload Wave Universe Button
                if st.button(
                    "Force Reload Wave Universe",
                    disabled=not ops_confirmation,
                    key="force_reload_universe_ops_button",
                    use_container_width=True
                ):
                    try:
                        # Increment wave universe version
                        if "wave_universe_version" not in st.session_state:
                            st.session_state.wave_universe_version = 1
                        st.session_state.wave_universe_version += 1
                        
                        # Clear wave universe cache from session state
                        cache_keys = ["wave_universe", "waves_list", "universe_cache"]
                        for key in cache_keys:
                            if key in st.session_state:
                                del st.session_state[key]
                        
                        # Clear Streamlit caches
                        try:
                            st.cache_data.clear()
                        except Exception:
                            pass
                        
                        try:
                            st.cache_resource.clear()
                        except Exception:
                            pass
                        
                        # Set force reload flag
                        st.session_state["force_reload_universe"] = True
                        
                        st.success("Wave universe cache cleared. Reloading...")
                        # Mark user interaction
                        st.session_state.user_interaction_detected = True
                        trigger_rerun("clear_wave_universe_cache")
                    except Exception as e:
                        st.warning(f"Force reload unavailable: {str(e)}")
                
                # Hard Rerun App Button
                if st.button(
                    "Hard Rerun App",
                    disabled=not ops_confirmation,
                    key="hard_rerun_button",
                    use_container_width=True
                ):
                    trigger_rerun("hard_rerun_button")
                
                # Force Reload + Clear Cache + Rerun Button (Enhanced)
                if st.button(
                    "Force Reload + Clear Cache + Rerun",
                    disabled=not ops_confirmation,
                    key="force_reload_clear_rerun_button",
                    use_container_width=True,
                    type="primary"
                ):
                    try:
                        # Clear wave-related session cache keys
                        cache_keys = ["wave_universe", "waves_list", "universe_cache", "force_reload_universe"]
                        for key in cache_keys:
                            if key in st.session_state:
                                del st.session_state[key]
                        
                        # Clear Streamlit caches
                        try:
                            st.cache_data.clear()
                        except Exception:
                            pass
                        
                        try:
                            st.cache_resource.clear()
                        except Exception:
                            pass
                        
                        # Trigger canonical wave universe reload with force_reload=True
                        get_canonical_wave_universe(force_reload=True)
                        
                        st.success("‚úÖ Cache cleared and wave universe reloaded. Rerunning app...")
                        # Mark user interaction
                        st.session_state.user_interaction_detected = True
                        trigger_rerun("force_clear_wave_reload")
                    except Exception as e:
                        st.warning(f"‚ö†Ô∏è Force reload failed: {str(e)}")
    
    st.sidebar.markdown("---")
    
    # ========================================================================
    # Data Health Panel
    # ========================================================================
    with st.sidebar.expander("üìä Data Health Status", expanded=False):
        try:
            from helpers.data_health_panel import render_data_health_panel
            render_data_health_panel()
        except ImportError:
            st.warning("‚ö†Ô∏è Data health panel not available")
        except Exception as e:
            st.error(f"‚ùå Error loading health panel: {str(e)}")
    
    st.sidebar.markdown("---")
    
    # ========================================================================
    # Wave Universe Truth Panel (Collapsible)
    # ========================================================================
    with st.sidebar.expander("üî¨ Wave Universe Truth Panel", expanded=False):
        render_wave_universe_truth_panel()
    
    st.sidebar.markdown("---")
    
    # ========================================================================
    # Sidebar Information
    # ========================================================================
    
    st.sidebar.title("Risk Lab")
    st.sidebar.write("Advanced risk analytics and monitoring tools for institutional portfolio management.")
    
    st.sidebar.title("Correlation Matrix")
    st.sidebar.write("Cross-asset correlation analysis for portfolio diversification insights.")
    
    st.sidebar.title("Rolling Alpha / Volatility")
    st.sidebar.write("Time-series analysis of alpha generation and volatility patterns.")
    
    st.sidebar.title("Drawdown Monitor")
    st.sidebar.write("Real-time tracking of portfolio drawdowns and recovery metrics.")
    
    # Debug Expander - Wave List Verification
    st.sidebar.markdown("---")
    with st.sidebar.expander("üîç Wave List Debug (Engine Source)"):
        try:
            all_waves = get_all_wave_names()
            
            if all_waves:
                st.write(f"**Total Waves Available:** {len(all_waves)}")
                st.write("")
                st.write("**First 25 Waves:**")
                
                # Display first 25 waves
                display_waves = all_waves[:25]
                for i, wave in enumerate(display_waves, 1):
                    st.text(f"{i}. {wave}")
                
                if len(all_waves) > 25:
                    st.write("")
                    st.caption(f"... and {len(all_waves) - 25} more waves")
                
                st.write("")
                st.caption("‚úÖ Sourced from WAVE_WEIGHTS in waves_engine.py")
            else:
                st.warning("‚ö†Ô∏è No waves available from engine")
                st.caption("Check waves_engine.py WAVE_WEIGHTS")
        except Exception as e:
            st.error(f"‚ùå Error loading waves: {str(e)}")
    
    # Build Information
    st.sidebar.markdown("---")
    st.sidebar.markdown("### Build Information")
    
    version_label = "Console v1.0"
    commit_hash = get_git_commit_hash()
    branch_name = get_git_branch_name()
    deploy_time = get_deploy_timestamp()
    data_timestamp = get_latest_data_timestamp()
    
    st.sidebar.text(f"Version: {version_label}")
    st.sidebar.text(f"Commit: {commit_hash}")
    st.sidebar.text(f"Branch: {branch_name}")
    st.sidebar.text(f"Deployed: {deploy_time}")
    st.sidebar.text(f"Data as of: {data_timestamp}")
    
    # Debug Display - Wave Universe Info
    st.sidebar.markdown("---")
    with st.sidebar.expander("üîç Wave Universe Debug Info"):
        try:
            universe = st.session_state.get("wave_universe", {})
            if universe:
                waves = universe.get("waves", [])
                removed_duplicates = universe.get("removed_duplicates", [])
                source = universe.get("source", "unknown")
                timestamp = universe.get("timestamp", "N/A")
                
                st.write(f"**Total Waves:** {len(waves)}")
                st.write(f"**Duplicates Removed:** {len(removed_duplicates)}")
                st.write(f"**Source:** {source}")
                st.write(f"**Last Updated:** {timestamp}")
                
                # Show first 10 waves for verification
                if waves:
                    st.write("**First 10 Waves:**")
                    preview_waves = waves[:10]
                    for i, wave in enumerate(preview_waves, 1):
                        st.text(f"{i}. {wave}")
                else:
                    st.warning("No waves loaded")
            else:
                st.info("Wave universe not yet initialized")
        except Exception as e:
            st.error(f"Debug display error: {str(e)}")
    
    # ========================================================================
    # DIAGNOSTICS DEBUG PANEL - Collapsible
    # ========================================================================
    st.sidebar.markdown("---")
    with st.sidebar.expander("üîç Diagnostics Debug Panel", expanded=False):
        st.markdown("**Diagnostics & Visibility Panel**")
        
        # Initialize exception storage in session state
        if "data_load_exceptions" not in st.session_state:
            st.session_state.data_load_exceptions = []
        
        try:
            # Display selected_wave_id
            selected_wave_id = st.session_state.get("selected_wave_id", "None")
            st.text(f"selected_wave_id: {selected_wave_id}")
            
            # Display selectbox key being used
            st.text(f"Selectbox key: selected_wave_id_display")
            
            # Check wave registry status
            try:
                if WAVE_REGISTRY_MANAGER_AVAILABLE:
                    active_waves_df = get_active_wave_registry()
                    wave_count = len(active_waves_df)
                    st.text(f"Wave Registry: Loaded ({wave_count} waves)")
                else:
                    st.text("Wave Registry: Module unavailable")
            except Exception as e:
                st.text(f"Wave Registry: Error - {str(e)}")
                st.session_state.data_load_exceptions.append({
                    "component": "Wave Registry",
                    "error": str(e),
                    "traceback": traceback.format_exc()
                })
            
            # Check portfolio snapshot status
            try:
                snapshot_path = "data/live_snapshot.csv"
                if os.path.exists(snapshot_path):
                    snapshot_df = pd.read_csv(snapshot_path)
                    row_count = len(snapshot_df)
                    st.text(f"Portfolio Snapshot: Loaded ({row_count} rows)")
                else:
                    st.text("Portfolio Snapshot: File not found")
            except Exception as e:
                st.text(f"Portfolio Snapshot: Error - {str(e)}")
                st.session_state.data_load_exceptions.append({
                    "component": "Portfolio Snapshot",
                    "error": str(e),
                    "traceback": traceback.format_exc()
                })
            
            # Check price cache status
            try:
                price_cache_path = CANONICAL_CACHE_PATH  # Use canonical path
                cache_exists = os.path.exists(price_cache_path)
                st.text(f"Price Cache Path: {price_cache_path}")
                st.text(f"Price Cache Exists: {cache_exists}")
                
                if cache_exists:
                    # Get file size
                    file_size = os.path.getsize(price_cache_path) / (1024 * 1024)  # Convert to MB
                    st.text(f"Price Cache Size: {file_size:.2f} MB")
            except Exception as e:
                st.text(f"Price Cache: Error - {str(e)}")
                st.session_state.data_load_exceptions.append({
                    "component": "Price Cache",
                    "error": str(e),
                    "traceback": traceback.format_exc()
                })
            
            # Portfolio Snapshot Debug Section
            st.markdown("---")
            st.markdown("**üìä Portfolio Snapshot Debug (last run)**")
            try:
                if "portfolio_snapshot_debug" in st.session_state:
                    debug_info = st.session_state.portfolio_snapshot_debug
                    
                    # Enhanced display with structured sections
                    if debug_info.get('reason_if_failure'):
                        st.error(f"**Failure Reason:** {debug_info['reason_if_failure']}")
                    
                    # Show exception details if available
                    if debug_info.get('exception_message'):
                        with st.expander("üîç Exception Details", expanded=True):
                            st.markdown("**Error Message:**")
                            st.code(debug_info['exception_message'])
                            
                            if debug_info.get('exception_traceback'):
                                st.markdown("**Traceback:**")
                                st.code(debug_info['exception_traceback'], language='python')
                    
                    # Show input summaries
                    with st.expander("üìä Input Summary", expanded=False):
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("Price Book Shape", debug_info.get('price_book_shape', 'N/A'))
                            st.metric("Active Waves", debug_info.get('active_waves_count', 'N/A'))
                            st.metric("Tickers Requested", debug_info.get('tickers_requested_count', 'N/A'))
                        with col2:
                            st.metric("Date Min", debug_info.get('price_book_index_min', 'N/A'))
                            st.metric("Date Max", debug_info.get('price_book_index_max', 'N/A'))
                            st.metric("Tickers Found", debug_info.get('tickers_intersection_count', 'N/A'))
                        
                        # Show missing tickers if any
                        missing = debug_info.get('tickers_missing_sample', [])
                        if missing:
                            st.markdown("**Missing Tickers (sample):**")
                            st.caption(', '.join(missing))
                    
                    # Show full JSON for power users
                    with st.expander("üîß Full Debug JSON", expanded=False):
                        import json
                        st.json(debug_info)
                else:
                    st.text("No portfolio snapshot debug info available yet")
                    st.caption("Navigate to Portfolio View to generate debug data")
            except Exception as e:
                st.error(f"Portfolio Snapshot Debug error: {str(e)}")
            
            # Display all captured exceptions
            if st.session_state.data_load_exceptions:
                st.markdown("---")
                st.markdown("**üö® Captured Exceptions:**")
                for idx, exc in enumerate(st.session_state.data_load_exceptions, 1):
                    with st.expander(f"Exception {idx}: {exc['component']}", expanded=False):
                        st.error(f"**Error:** {exc['error']}")
                        st.code(exc['traceback'], language="python")
                        
        except Exception as e:
            st.error(f"Debug panel error: {str(e)}")
            import traceback
            st.code(traceback.format_exc())


# ============================================================================
# SECTION 7: TAB RENDER FUNCTIONS
# ============================================================================


def render_wave_identity_card(selected_wave: str, mode: str):
    """
    Wave Identity Card must always render as visual UI. Raw HTML rendering is forbidden.
    
    Renders a professional Wave Identity Card showing:
    - Wave Name
    - Mode Pill
    - Key Stats: 1D, 30D, 60D, 365D Returns, Alpha, Beta, Exposure, Cash
    
    Uses st.components.v1.html() for rich rendering with automatic fallback to
    Streamlit-native layout if the HTML component fails.
    
    Args:
        selected_wave: Name of the selected wave
        mode: Mode (Standard, Alpha-Minus-Beta, or Private Logic)
    """
    try:
        # Get wave data for different time periods
        data_1d = get_wave_data_filtered(wave_name=selected_wave, days=1)
        data_30d = get_wave_data_filtered(wave_name=selected_wave, days=30)
        data_60d = get_wave_data_filtered(wave_name=selected_wave, days=60)
        data_365d = get_wave_data_filtered(wave_name=selected_wave, days=365)
        
        # Calculate returns for each period
        def calc_return(data):
            if data is not None and len(data) > 0 and 'portfolio_return' in data.columns:
                return data['portfolio_return'].sum()
            return None
        
        return_1d = calc_return(data_1d)
        return_30d = calc_return(data_30d)
        return_60d = calc_return(data_60d)
        return_365d = calc_return(data_365d)
        
        # Calculate Alpha (30D)
        alpha_30d = None
        if data_30d is not None and len(data_30d) > 0:
            if 'portfolio_return' in data_30d.columns and 'benchmark_return' in data_30d.columns:
                alpha_30d = (data_30d['portfolio_return'] - data_30d['benchmark_return']).sum()
        
        # Calculate Beta (30D) - simplified correlation-based beta
        beta_30d = None
        if data_30d is not None and len(data_30d) > 0:
            if 'portfolio_return' in data_30d.columns and 'benchmark_return' in data_30d.columns:
                try:
                    port_returns = data_30d['portfolio_return']
                    bench_returns = data_30d['benchmark_return']
                    covariance = np.cov(port_returns, bench_returns)[0, 1]
                    bench_variance = np.var(bench_returns)
                    if bench_variance > 0:
                        beta_30d = covariance / bench_variance
                except:
                    pass
        
        # Get exposure and cash from latest data point
        exposure = None
        cash = None
        if data_1d is not None and len(data_1d) > 0:
            if 'exposure' in data_1d.columns:
                exposure = data_1d['exposure'].iloc[-1]
            if 'cash' in data_1d.columns:
                cash = data_1d['cash'].iloc[-1]
        
        # Format values for display
        def fmt_pct(val):
            return f"{val*100:.2f}%" if val is not None else "N/A"
        
        def fmt_num(val, decimals=2):
            return f"{val:.{decimals}f}" if val is not None else "N/A"
        
        # Determine mode color
        if mode == "Standard":
            mode_color = "#00ff88"
        elif mode == "Alpha-Minus-Beta":
            mode_color = "#ffd700"
        elif mode == "Aggressive":
            mode_color = "#ff6b6b"
        else:
            mode_color = "#888"
        
        # Try to use st.components.v1.html() for rich rendering
        try:
            import streamlit.components.v1 as components
            
            # Create HTML content for the identity card
            html_content = f"""
            <!DOCTYPE html>
            <html>
            <head>
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <style>
                    body {{
                        margin: 0;
                        padding: 0;
                        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                    }}
                    .wave-identity-card {{
                        background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
                        border: 2px solid #00ff88;
                        border-radius: 12px;
                        padding: 24px;
                        margin-bottom: 20px;
                        box-shadow: 0 4px 12px rgba(0, 255, 136, 0.2);
                    }}
                    .wave-name {{
                        color: #ffffff;
                        font-size: 28px;
                        font-weight: bold;
                        margin: 0 0 12px 0;
                    }}
                    .mode-pill {{
                        display: inline-block;
                        background: {mode_color};
                        color: #1a1a2e;
                        padding: 8px 20px;
                        border-radius: 20px;
                        font-weight: bold;
                        font-size: 14px;
                        margin-bottom: 20px;
                    }}
                    .stats-grid {{
                        display: grid;
                        grid-template-columns: repeat(auto-fit, minmax(140px, 1fr));
                        gap: 16px;
                        margin-top: 16px;
                    }}
                    .stat-tile {{
                        background: rgba(255, 255, 255, 0.05);
                        border: 1px solid rgba(0, 255, 136, 0.3);
                        border-radius: 8px;
                        padding: 12px;
                        text-align: center;
                    }}
                    .stat-label {{
                        color: #aaa;
                        font-size: 12px;
                        margin-bottom: 4px;
                        text-transform: uppercase;
                        letter-spacing: 0.5px;
                    }}
                    .stat-value {{
                        color: #ffffff;
                        font-size: 18px;
                        font-weight: bold;
                    }}
                    .stat-value.positive {{
                        color: #00ff88;
                    }}
                    .stat-value.negative {{
                        color: #ff6b6b;
                    }}
                    @media only screen and (max-width: 768px) {{
                        .wave-identity-card {{
                            padding: 16px;
                            margin-bottom: 16px;
                        }}
                        .wave-name {{
                            font-size: 22px;
                            margin-bottom: 10px;
                        }}
                        .mode-pill {{
                            padding: 6px 16px;
                            font-size: 12px;
                            margin-bottom: 16px;
                        }}
                        .stats-grid {{
                            grid-template-columns: repeat(2, 1fr);
                            gap: 12px;
                        }}
                        .stat-tile {{
                            padding: 10px;
                        }}
                        .stat-label {{
                            font-size: 11px;
                        }}
                        .stat-value {{
                            font-size: 16px;
                        }}
                    }}
                </style>
            </head>
            <body>
                <div class="wave-identity-card">
                    <h1 class="wave-name">üåä {selected_wave}</h1>
                    <div class="mode-pill">MODE: {mode}</div>
                    
                    <div class="stats-grid">
                        <div class="stat-tile">
                            <div class="stat-label">1D Return</div>
                            <div class="stat-value {'positive' if return_1d and return_1d > 0 else 'negative' if return_1d and return_1d < 0 else ''}">{fmt_pct(return_1d)}</div>
                        </div>
                        <div class="stat-tile">
                            <div class="stat-label">30D Return</div>
                            <div class="stat-value {'positive' if return_30d and return_30d > 0 else 'negative' if return_30d and return_30d < 0 else ''}">{fmt_pct(return_30d)}</div>
                        </div>
                        <div class="stat-tile">
                            <div class="stat-label">60D Return</div>
                            <div class="stat-value {'positive' if return_60d and return_60d > 0 else 'negative' if return_60d and return_60d < 0 else ''}">{fmt_pct(return_60d)}</div>
                        </div>
                        <div class="stat-tile">
                            <div class="stat-label">365D Return</div>
                            <div class="stat-value {'positive' if return_365d and return_365d > 0 else 'negative' if return_365d and return_365d < 0 else ''}">{fmt_pct(return_365d)}</div>
                        </div>
                        <div class="stat-tile">
                            <div class="stat-label">Alpha (30D)</div>
                            <div class="stat-value {'positive' if alpha_30d and alpha_30d > 0 else 'negative' if alpha_30d and alpha_30d < 0 else ''}">{fmt_pct(alpha_30d)}</div>
                        </div>
                        <div class="stat-tile">
                            <div class="stat-label">Beta</div>
                            <div class="stat-value">{fmt_num(beta_30d)}</div>
                        </div>
                        <div class="stat-tile">
                            <div class="stat-label">Exposure</div>
                            <div class="stat-value">{fmt_pct(exposure)}</div>
                        </div>
                        <div class="stat-tile">
                            <div class="stat-label">Cash</div>
                            <div class="stat-value">{fmt_pct(cash)}</div>
                        </div>
                    </div>
                </div>
            </body>
            </html>
            """
            
            # Render using st.components.v1.html()
            # Increased height to 600px to prevent clipping on mobile devices
            # Enabled scrolling to ensure all content is accessible
            components.html(html_content, height=600, scrolling=True)
            
        except Exception as component_error:
            # Fallback to Streamlit-native layout
            _render_wave_identity_card_fallback(
                selected_wave, mode, mode_color,
                return_1d, return_30d, return_60d, return_365d,
                alpha_30d, beta_30d, exposure, cash
            )
    
    except Exception as e:
        # Ultimate fallback - show minimal info
        st.error(f"‚ö†Ô∏è Wave Identity Card unavailable: {str(e)}")
        st.info(f"**Selected Wave:** {selected_wave} | **Mode:** {mode}")


def _render_wave_identity_card_fallback(
    selected_wave, mode, mode_color,
    return_1d, return_30d, return_60d, return_365d,
    alpha_30d, beta_30d, exposure, cash
):
    """
    Fallback rendering for Wave Identity Card using native Streamlit components only.
    No HTML, no unsafe_allow_html - pure Streamlit UI.
    """
    # Format values
    def fmt_pct(val):
        return f"{val*100:.2f}%" if val is not None else "N/A"
    
    def fmt_num(val, decimals=2):
        return f"{val:.{decimals}f}" if val is not None else "N/A"
    
    # Create container for the card
    with st.container():
        # Wave name and mode
        st.subheader(f"üåä {selected_wave}")
        
        # Mode indicator using colored text (no HTML)
        mode_emoji = "üü¢" if mode == "Standard" else "üü°" if mode == "Alpha-Minus-Beta" else "üî¥"
        st.markdown(f"**{mode_emoji} MODE:** {mode}")
        
        st.markdown("---")
        
        # Stats in a grid layout using columns
        st.markdown("**Key Statistics**")
        
        # Row 1: Returns
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            delta_1d = None if return_1d is None else return_1d
            st.metric("1D Return", fmt_pct(return_1d), delta=fmt_pct(delta_1d) if delta_1d else None)
        with col2:
            st.metric("30D Return", fmt_pct(return_30d))
        with col3:
            st.metric("60D Return", fmt_pct(return_60d))
        with col4:
            st.metric("365D Return", fmt_pct(return_365d))
        
        # Row 2: Alpha, Beta, Exposure, Cash
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Alpha (30D)", fmt_pct(alpha_30d))
        with col2:
            st.metric("Beta", fmt_num(beta_30d))
        with col3:
            st.metric("Exposure", fmt_pct(exposure))
        with col4:
            st.metric("Cash", fmt_pct(cash))
        
        # Add bottom padding for sticky bar compatibility
        st.markdown("<br>", unsafe_allow_html=True)


def render_executive_brief_tab():
    """
    Render the Executive Brief tab - Top-level actionable insights for decision-makers.
    
    This is the FIRST tab (Overview) and provides:
    - Mission Control Header: "WAVES Intelligence‚Ñ¢"
    - Executive Summary: Highlights of top performers and waves requiring attention
    - Comprehensive Performance Table: All waves with Current, 30D, 60D, 365D returns and alphas
    - Market Snapshot: Market Regime, VIX Gate Status, Rates, SPY/QQQ, Liquidity
    - Wave System Snapshot: System Return, Alpha, Win Rate, Risk State
    - What's Strong/What's Weak: Top/Bottom 5 Waves by Alpha (30D)
    - Why: Compact narrative paragraph on the current regime
    - What to Do: Action panel (e.g., today's actions, watchlist)
    
    NO DIAGNOSTICS CONTENT - All diagnostics moved to Diagnostics tab.
    """
    # ========================================================================
    # RESOLVE APP CONTEXT - Get selected wave for conditional rendering
    # ========================================================================
    ctx = resolve_app_context()
    selected_wave = ctx["selected_wave_name"]
    is_portfolio_view = is_portfolio_context(selected_wave)
    
    # ========================================================================
    # MISSION CONTROL HEADER
    # ========================================================================
    st.markdown("""
    <div style="
        background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
        border: 2px solid #00d9ff;
        border-radius: 12px;
        padding: 20px;
        margin-bottom: 20px;
        text-align: center;
    ">
        <h1 style="color: #00d9ff; margin: 0; font-size: 32px;">
            üåä WAVES Intelligence‚Ñ¢
        </h1>
        <p style="color: #ffffff; margin: 5px 0 0 0; font-size: 16px;">
            Market + Wave Health Dashboard
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # ========================================================================
    # WAVE RENDERING GUARANTEE BANNER (28/28 Always Visible)
    # ========================================================================
    st.markdown("""
    <div style="
        background: linear-gradient(90deg, #1a4d2e 0%, #2d6a4f 100%);
        border: 2px solid #40916c;
        border-radius: 8px;
        padding: 15px;
        margin-bottom: 15px;
        text-align: center;
    ">
        <h3 style="color: #52b788; margin: 0; font-size: 18px;">
            ‚úì 28/28 Waves Rendering Guarantee
        </h3>
        <p style="color: #b7e4c7; margin: 5px 0 0 0; font-size: 14px;">
            All waves always visible | No blockers | Graceful degradation enabled
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    try:
        # ========================================================================
        # PORTFOLIO SNAPSHOT FROM SESSION STATE - Use pre-loaded snapshot
        # ========================================================================
        # UPDATED: Use portfolio_snapshot from session state instead of reloading/recomputing
        # The snapshot is already loaded at app startup and stored in st.session_state["portfolio_snapshot"]
        snapshot_df = st.session_state.get("portfolio_snapshot")
        snapshot_metadata = None
        
        if snapshot_df is None or snapshot_df.empty:
            # Fallback: Try loading from TruthFrame if session state is empty
            try:
                from analytics_truth import get_truth_frame
                from truth_frame_helpers import convert_truthframe_to_snapshot_format
                from snapshot_ledger import get_snapshot_metadata
                
                # Debug trace marker
                if st.session_state.get("debug_mode", False):
                    st.caption("üîç Trace: Fallback to TruthFrame (portfolio_snapshot not in session state)")
                
                # Get TruthFrame (respects Safe Mode)
                safe_mode = st.session_state.get("safe_mode_enabled", False)
                truth_df = get_truth_frame(safe_mode=safe_mode)
                
                # Convert to snapshot_df format for backward compatibility
                snapshot_df = convert_truthframe_to_snapshot_format(truth_df)
                
                snapshot_metadata = get_snapshot_metadata()
            except ImportError:
                st.warning("‚ö†Ô∏è TruthFrame module not available and portfolio_snapshot not in session state.")
            except Exception as e:
                st.warning(f"‚ö†Ô∏è TruthFrame fallback error: {str(e)}")
        else:
            # Successfully loaded from session state
            if st.session_state.get("debug_mode", False):
                st.caption(f"‚úì Loaded portfolio_snapshot from session state ({len(snapshot_df)} rows)")
            
            # Try to get metadata
            try:
                from snapshot_ledger import get_snapshot_metadata
                snapshot_metadata = get_snapshot_metadata()
            except:
                pass
        
        # ========================================================================
        # WAVE STATUS FILTERING - Add UI toggle for staging waves
        # ========================================================================
        include_staging = st.checkbox(
            "Include Staging Waves",
            value=False,
            key="include_staging_waves_executive",
            help="Enable to include waves with STAGING status in aggregations and summaries"
        )
        
        # Filter snapshot_df based on wave_status if the column exists
        if snapshot_df is not None and not snapshot_df.empty and 'wave_status' in snapshot_df.columns:
            # Store original count
            total_waves_all = len(snapshot_df)
            
            # Filter by wave_status
            if not include_staging:
                snapshot_df = snapshot_df[snapshot_df['wave_status'] == 'ACTIVE'].copy()
                staging_count = total_waves_all - len(snapshot_df)
                if staging_count > 0:
                    st.caption(f"‚ÑπÔ∏è Filtered out {staging_count} STAGING wave(s). Showing {len(snapshot_df)} ACTIVE waves.")
            else:
                active_count = (snapshot_df['wave_status'] == 'ACTIVE').sum()
                staging_count = (snapshot_df['wave_status'] == 'STAGING').sum()
                st.caption(f"‚ÑπÔ∏è Showing all waves: {active_count} ACTIVE, {staging_count} STAGING")
        
        # ========================================================================
        # SNAPSHOT CONTROLS - Last Snapshot Timestamp + Force Refresh Button
        # ========================================================================
        col1, col2, col3 = st.columns([3, 1, 1])
        
        with col1:
            st.markdown("### üìä Live TruthFrame Overview")
            st.caption("28/28 Waves with live metrics - Canonical single source of truth")
        
        with col2:
            # Show snapshot metadata
            if snapshot_metadata and snapshot_metadata.get("exists"):
                age_hours = snapshot_metadata.get('age_hours', 0)
                age_str = f"{age_hours:.1f}h ago" if age_hours is not None else "N/A"
                is_stale = snapshot_metadata.get('is_stale', True)
                status_icon = "üü¢" if not is_stale else "üü°"
                st.metric("Last Snapshot", age_str, delta=status_icon, help="Snapshot freshness indicator")
            else:
                st.metric("Last Snapshot", "Not Generated", help="Click Force Refresh to generate")
        
        with col3:
            # Force refresh button with runtime guard
            if st.button("üîÑ Force Refresh", help="Regenerate TruthFrame (max 5 min)", use_container_width=True, key=k("ExecutiveBrief", "force_refresh_truthframe")):
                with st.spinner("Regenerating TruthFrame..."):
                    try:
                        safe_mode = st.session_state.get("safe_mode_enabled", False)
                        
                        if safe_mode:
                            st.warning("‚ö†Ô∏è Cannot refresh in Safe Mode. Disable Safe Mode to regenerate.")
                        else:
                            # Get global price cache for faster generation
                            price_df = st.session_state.get("global_price_df")
                            from analytics_truth import get_truth_frame
                            truth_df = get_truth_frame(safe_mode=False, force_refresh=True, price_df=price_df)
                            st.success(f"‚úì TruthFrame refreshed: {len(truth_df)} waves")
                            # Mark user interaction
                            st.session_state.user_interaction_detected = True
                            trigger_rerun("truthframe_refresh")
                    except Exception as e:
                        st.error(f"TruthFrame refresh failed: {str(e)}")
        
        st.divider()
        
        # ========================================================================
        # SECTION 0.5: QUICK DIAGNOSTICS PANEL (28/28 Status from Snapshot)
        # ========================================================================
        with st.expander("üîç Quick Diagnostics - Wave Rendering Status", expanded=False):
            try:
                # Use snapshot data if available, otherwise fall back to diagnostics
                if snapshot_df is not None and not snapshot_df.empty:
                    total_waves = len(snapshot_df)
                    
                    # Count by Data_Regime_Tag from snapshot
                    status_breakdown = {
                        "Full": (snapshot_df["Data_Regime_Tag"] == "Full").sum(),
                        "Partial": (snapshot_df["Data_Regime_Tag"] == "Partial").sum(),
                        "Operational": (snapshot_df["Data_Regime_Tag"] == "Operational").sum(),
                        "Degraded": 0,  # Snapshot uses "Operational" instead
                        "Unavailable": (snapshot_df["Data_Regime_Tag"] == "Unavailable").sum()
                    }
                    
                    # Count populated vs degraded waves (populated = non-NaN returns)
                    populated_count = snapshot_df["Return_30D"].notna().sum()
                    degraded_count = total_waves - populated_count
                    
                    # Count broken tickers
                    broken_tickers_count = 0
                    try:
                        from helpers.ticker_diagnostics import get_diagnostics_tracker
                        tracker = get_diagnostics_tracker()
                        if tracker:
                            broken_tickers_count = len(tracker.get_all_failed_tickers())
                    except (ImportError, AttributeError):
                        pass
                else:
                    # Fallback to diagnostics
                    diagnostics = compute_wave_universe_diagnostics()
                    wave_statuses = diagnostics.get('wave_statuses', {})
                    total_waves = diagnostics.get('universe_count', 28)
                    
                    # Count by readiness status
                    status_breakdown = {
                        "Full": 0,
                        "Partial": 0,
                        "Operational": 0,
                        "Degraded": 0,
                        "Unavailable": 0
                    }
                    
                    for wave, info in wave_statuses.items():
                        status = info['status']
                        if status in ["Full", "Ready"]:
                            status_breakdown["Full"] += 1
                        elif status == "Partial":
                            status_breakdown["Partial"] += 1
                        elif status == "Operational":
                            status_breakdown["Operational"] += 1
                        elif status in ["Degraded", "Degraded (Rate Limited)", "Degraded (Partial Data)"]:
                            status_breakdown["Degraded"] += 1
                        else:
                            status_breakdown["Unavailable"] += 1
                    
                    populated_count = status_breakdown["Full"] + status_breakdown["Partial"]
                    degraded_count = status_breakdown["Degraded"] + status_breakdown["Unavailable"]
                    
                    broken_tickers_count = 0
                    try:
                        from helpers.ticker_diagnostics import get_diagnostics_tracker
                        tracker = get_diagnostics_tracker()
                        if tracker:
                            broken_tickers_count = len(tracker.get_all_failed_tickers())
                    except (ImportError, AttributeError):
                        pass
                
                # Display metrics - 4 key metrics as per requirements
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric(
                        label="Total Waves",
                        value=f"{total_waves}",
                        help="All waves in registry (guaranteed 28)"
                    )
                
                with col2:
                    st.metric(
                        label="Populated Waves",
                        value=populated_count,
                        delta=f"{populated_count/total_waves*100:.0f}%" if total_waves > 0 else "N/A",
                        help="Waves with valid performance data"
                    )
                
                with col3:
                    st.metric(
                        label="Degraded Waves",
                        value=degraded_count,
                        delta=f"{degraded_count/total_waves*100:.0f}%" if total_waves > 0 else "N/A",
                        help="Waves with limited or no data"
                    )
                
                with col4:
                    st.metric(
                        label="Broken Tickers",
                        value=broken_tickers_count,
                        help="Tickers that failed to download"
                    )
                
                # Show detailed breakdown
                st.divider()
                st.markdown("**Data Quality Breakdown:**")
                col1, col2, col3, col4, col5 = st.columns(5)
                
                with col1:
                    st.metric(
                        label="üü¢ Full",
                        value=status_breakdown["Full"],
                        help="All analytics available"
                    )
                
                with col2:
                    st.metric(
                        label="üîµ Partial",
                        value=status_breakdown["Partial"],
                        help="Basic analytics available"
                    )
                
                with col3:
                    st.metric(
                        label="üü° Operational",
                        value=status_breakdown["Operational"],
                        help="Current pricing only"
                    )
                
                with col4:
                    st.metric(
                        label="üü† Degraded",
                        value=status_breakdown["Degraded"],
                        help="Limited data, still visible"
                    )
                
                with col5:
                    st.metric(
                        label="üî¥ Unavailable",
                        value=status_breakdown["Unavailable"],
                        help="No data, fallback mode"
                    )
                
                # Show timestamp
                timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                st.caption(f"üìÖ Last checked: {timestamp}")
                
                # Show ticker failure summary if available
                if broken_tickers_count > 0:
                    st.warning(f"‚ö†Ô∏è {broken_tickers_count} ticker(s) failed to download - waves still visible with available data")
                
                # ========================================================================
                # PORTFOLIO SNAPSHOT DIAGNOSTICS
                # ========================================================================
                st.divider()
                st.markdown("**Portfolio Snapshot Diagnostics:**")
                
                try:
                    from helpers.wave_performance import validate_portfolio_diagnostics
                    from helpers.price_book import get_price_book
                    
                    price_book = get_cached_price_book()
                    diagnostics = validate_portfolio_diagnostics(price_book, mode='Standard')
                    
                    # Display key diagnostics
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        st.metric(
                            label="Latest Date",
                            value=diagnostics['latest_date'] if diagnostics['latest_date'] else "N/A",
                            help="Latest data date in PRICE_BOOK"
                        )
                    
                    with col2:
                        data_age = diagnostics['data_age_days']
                        quality = diagnostics['data_quality']
                        if data_age is not None:
                            if quality == 'OK':
                                delta_icon = "üü¢"
                            elif quality == 'DEGRADED':
                                delta_icon = "üü°"
                            else:
                                delta_icon = "üî¥"
                            st.metric(
                                label="Data Age",
                                value=f"{data_age}d",
                                delta=delta_icon,
                                help=f"Data quality: {quality}"
                            )
                        else:
                            st.metric("Data Age", "N/A")
                    
                    with col3:
                        st.metric(
                            label="Portfolio Waves",
                            value=diagnostics['wave_count'],
                            help="Number of waves in portfolio"
                        )
                    
                    with col4:
                        st.metric(
                            label="History Days",
                            value=diagnostics['min_history_days'],
                            help="Total trading days available"
                        )
                    
                    # Show series status
                    st.markdown("**Required Series Status:**")
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        icon = "‚úÖ" if diagnostics['has_portfolio_returns_series'] else "‚ùå"
                        st.markdown(f"{icon} Portfolio Returns")
                    
                    with col2:
                        icon = "‚úÖ" if diagnostics['has_portfolio_benchmark_series'] else "‚ùå"
                        st.markdown(f"{icon} Benchmark Series")
                    
                    with col3:
                        icon = "‚úÖ" if diagnostics['has_overlay_alpha_series'] else "‚ö†Ô∏è"
                        st.markdown(f"{icon} Overlay Alpha")
                    
                    # Show issues if any
                    if diagnostics['issues']:
                        st.markdown("**Issues:**")
                        for issue in diagnostics['issues']:
                            # Skip expected issues
                            if "Overlay alpha component series missing (expected" not in issue:
                                st.caption(f"‚ö†Ô∏è {issue}")
                    
                except Exception as e:
                    st.warning(f"Portfolio diagnostics unavailable: {str(e)}")
                    
            except Exception as e:
                st.warning(f"Diagnostics panel temporarily unavailable: {str(e)}")
    
        # ========================================================================
        # SECTION 1: EXECUTIVE SUMMARY (FROM SNAPSHOT)
        # ========================================================================
        st.markdown("### üìã Executive Summary")
        
        try:
            # Use snapshot data if available, otherwise fallback to original method
            if snapshot_df is not None and not snapshot_df.empty:
                # Sort by 30D Alpha to identify top and bottom performers
                sorted_df = snapshot_df.sort_values('Alpha_30D', ascending=False, na_position='last')
                sorted_df_clean = sorted_df.dropna(subset=['Alpha_30D'])
                
                # Identify top performers (top 3 with valid data)
                top_performers = sorted_df_clean.head(3)
                
                # Identify waves requiring attention (bottom 3 with valid data)
                bottom_performers = sorted_df_clean.tail(3)
                
                # Calculate system statistics from snapshot
                valid_alpha_30d = snapshot_df['Alpha_30D'].dropna()
                valid_return_30d = snapshot_df['Return_30D'].dropna()
                
                if len(valid_alpha_30d) > 0:
                    system_alpha_30d = valid_alpha_30d.mean()
                    system_return_30d = valid_return_30d.mean() if len(valid_return_30d) > 0 else 0.0
                    win_rate_30d = (valid_alpha_30d > 0).sum() / len(valid_alpha_30d) * 100
                    
                    # Get market context
                    mc_data = get_mission_control_data()
                    
                    # Build executive summary narrative
                    summary_parts = []
                    
                    # Market context
                    market_regime = mc_data.get('market_regime', 'Unknown') if mc_data else 'Unknown'
                    vix_gate = mc_data.get('vix_gate_status', 'Unknown') if mc_data else 'Unknown'
                    
                    if market_regime != 'Unknown':
                        summary_parts.append(f"**Market Regime:** {market_regime}")
                    if vix_gate != 'Unknown':
                        summary_parts.append(f"**VIX Status:** {vix_gate}")
                    
                    # Top performers section
                    if len(top_performers) > 0:
                        top_text = "**Top Performers:** "
                        top_names = []
                        for idx, row in top_performers.iterrows():
                            wave_name = row['Wave']
                            alpha_30d = row.get('Alpha_30D')
                            if pd.notna(alpha_30d):
                                top_names.append(f"{wave_name} ({alpha_30d:+.2%})")
                        if top_names:
                            top_text += ", ".join(top_names)
                            summary_parts.append(top_text)
                    
                    # Waves requiring attention section
                    if len(bottom_performers) > 0:
                        bottom_text = "**Requiring Attention:** "
                        bottom_names = []
                        for idx, row in bottom_performers.iterrows():
                            wave_name = row['Wave']
                            alpha_30d = row.get('Alpha_30D')
                            if pd.notna(alpha_30d):
                                bottom_names.append(f"{wave_name} ({alpha_30d:+.2%})")
                        if bottom_names:
                            bottom_text += ", ".join(bottom_names)
                            summary_parts.append(bottom_text)
                    
                    # System-level insights
                    if win_rate_30d >= 70 and system_alpha_30d > 0.005:
                        insight = "**Overall Impact:** Strong performance across the system with broad-based positive momentum."
                    elif win_rate_30d >= 55:
                        insight = "**Overall Impact:** Mixed performance with selective opportunities in leading waves."
                    elif win_rate_30d >= 40:
                        insight = "**Overall Impact:** Divergent performance across waves; focus on differentiated positioning."
                    else:
                        insight = "**Overall Impact:** Defensive positioning warranted given weak breadth and negative alpha trends."
                    
                    summary_parts.append(insight)
                    
                    # Display summary
                    if summary_parts:
                        summary_text = " | ".join(summary_parts)
                        st.info(summary_text)
                    else:
                        st.info("Executive summary unavailable - insufficient data.")
                else:
                    st.info("Executive summary unavailable - no valid alpha data in snapshot.")
            else:
                # Fallback to original method
                timeframes = [1, 30, 60, 365]
                all_waves_df = get_all_waves_multi_timeframe_data(timeframes=timeframes)
                
                if not all_waves_df.empty and '30D_Alpha' in all_waves_df.columns:
                    # Sort by 30D Alpha to identify top and bottom performers
                    sorted_df = all_waves_df.sort_values('30D_Alpha', ascending=False)
                    sorted_df_clean = sorted_df.dropna(subset=['30D_Alpha'])
                    
                    # Identify top performers (top 3 with valid data)
                    top_performers = sorted_df_clean.head(3)
                    
                    # Identify waves requiring attention (bottom 3 with valid data)
                    bottom_performers = sorted_df_clean.tail(3)
                    
                    # Get system statistics
                    stats_30d = get_system_statistics(timeframe_days=30)
                    mc_data = get_mission_control_data()
                    
                    # Build executive summary narrative
                    summary_parts = []
                    
                    # Market context
                    market_regime = mc_data.get('market_regime', 'Unknown') if mc_data else 'Unknown'
                    vix_gate = mc_data.get('vix_gate_status', 'Unknown') if mc_data else 'Unknown'
                    
                    if market_regime != 'Unknown':
                        summary_parts.append(f"**Market Regime:** {market_regime}")
                    if vix_gate != 'Unknown':
                        summary_parts.append(f"**VIX Status:** {vix_gate}")
                    
                    # Top performers section
                    if len(top_performers) > 0:
                        top_text = "**Top Performers:** "
                        top_names = []
                        for idx, row in top_performers.iterrows():
                            wave_name = row['Wave']
                            alpha_30d = row.get('30D_Alpha')
                            if pd.notna(alpha_30d):
                                top_names.append(f"{wave_name} ({alpha_30d:+.2%})")
                        if top_names:
                            top_text += ", ".join(top_names)
                            summary_parts.append(top_text)
                    
                    # Waves requiring attention section
                    if len(bottom_performers) > 0:
                        bottom_text = "**Requiring Attention:** "
                        bottom_names = []
                        for idx, row in bottom_performers.iterrows():
                            wave_name = row['Wave']
                            alpha_30d = row.get('30D_Alpha')
                            if pd.notna(alpha_30d):
                                bottom_names.append(f"{wave_name} ({alpha_30d:+.2%})")
                        if bottom_names:
                            bottom_text += ", ".join(bottom_names)
                            summary_parts.append(bottom_text)
                    
                    # System-level insights
                    if stats_30d:
                        avg_alpha = stats_30d.get('avg_alpha', 0.0)
                        win_rate = stats_30d.get('pct_positive_alpha', 0.0)
                        
                        if win_rate >= 70 and avg_alpha > 0.005:
                            insight = "**Overall Impact:** Strong performance across the system with broad-based positive momentum."
                        elif win_rate >= 55:
                            insight = "**Overall Impact:** Mixed performance with selective opportunities in leading waves."
                        elif win_rate >= 40:
                            insight = "**Overall Impact:** Divergent performance across waves; focus on differentiated positioning."
                        else:
                            insight = "**Overall Impact:** Defensive positioning warranted given weak breadth and negative alpha trends."
                        
                        summary_parts.append(insight)
                    
                    # Display summary
                    if summary_parts:
                        summary_text = " | ".join(summary_parts)
                        st.info(summary_text)
                    else:
                        st.info("Executive summary unavailable - insufficient data.")
                else:
                    st.info("Executive summary unavailable - no wave data available.")
        
        except Exception as e:
            st.info("Executive summary unavailable - data loading error.")
        
        st.divider()
        
        # ========================================================================
        # SECTION 1.5: PORTFOLIO SNAPSHOT (BLUE BOX)
        # Only show for Portfolio View (all waves), not individual wave view
        # ========================================================================
        if is_portfolio_view:
            st.markdown("### üíº Portfolio Snapshot")
            st.caption("Equal-weight portfolio across all active waves - Multi-window returns and alpha")
            
            try:
                # ================================================================
                # INLINE COMPUTATION FROM PRICE_BOOK
                # All portfolio metrics computed directly in render path
                # No dependencies on snapshots, caches, or external helpers
                # ================================================================
                
                # Fetch PRICE_BOOK (already loaded in app memory)
                PRICE_BOOK = get_cached_price_book()
                
                if PRICE_BOOK is None or PRICE_BOOK.empty:
                    st.error("‚ö†Ô∏è PRICE_BOOK is empty - cannot compute portfolio metrics")
                    st.caption("Portfolio Snapshot requires PRICE_BOOK data")
                else:
                    # Compute returns inline using pct_change
                    returns_df = PRICE_BOOK.pct_change().dropna()
                    
                    # Compute equal-weighted portfolio returns (mean across all tickers)
                    portfolio_returns = returns_df.mean(axis=1)
                    
                    # Get latest trading date and current UTC time for diagnostics
                    last_trading_date = portfolio_returns.index[-1].strftime('%Y-%m-%d')
                    current_utc = datetime.utcnow().strftime('%H:%M:%S UTC')
                    
                    # Display diagnostic line (mandatory)
                    st.markdown(
                        f"""
                        <div style="background-color: #1a1a1a; padding: 8px 12px; border-left: 3px solid #00ff00; margin-bottom: 8px; font-family: monospace; font-size: 11px; color: #a0a0a0;">
                            <strong>Live portfolio metrics computed from PRICE_BOOK</strong><br>
                            Last trading date: {last_trading_date} | Rendered at: {current_utc}
                        </div>
                        """,
                        unsafe_allow_html=True
                    )
                
                    # Compute metrics for each period
                    # 1D: Latest value
                    ret_1d = portfolio_returns.iloc[-1] if len(portfolio_returns) >= 1 else None
                    
                    # 30D: Compounded returns of last 30 rows
                    if len(portfolio_returns) >= 30:
                        ret_30d = (1 + portfolio_returns.iloc[-30:]).prod() - 1
                    else:
                        ret_30d = None
                    
                    # 60D: Compounded returns of last 60 rows
                    if len(portfolio_returns) >= 60:
                        ret_60d = (1 + portfolio_returns.iloc[-60:]).prod() - 1
                    else:
                        ret_60d = None
                    
                    # 365D: Compounded returns of last ~252 rows (one trading year)
                    if len(portfolio_returns) >= 252:
                        ret_365d = (1 + portfolio_returns.iloc[-252:]).prod() - 1
                    else:
                        ret_365d = None
                
                    # Display in blue box with computed metrics
                    st.markdown("""
                    <div style="
                        background: linear-gradient(135deg, #0d1117 0%, #161b22 100%);
                        border: 2px solid #00d9ff;
                        border-radius: 12px;
                        padding: 20px;
                        margin-bottom: 20px;
                    ">
                    """, unsafe_allow_html=True)
                
                    # Portfolio performance metrics display
                    st.markdown("**üìä Portfolio Performance (Equal-Weighted)**")
                    st.caption("Returns based on PRICE_BOOK data")
                
                    col1, col2, col3, col4 = st.columns(4)
                
                    with col1:
                        st.markdown("**1D**")
                        if ret_1d is not None:
                            st.markdown(f"üìà **Return:** {ret_1d:+.2%}")
                        else:
                            st.markdown("üìà **Return:** N/A")
                            st.caption("‚ö†Ô∏è Insufficient data")
                
                    with col2:
                        st.markdown("**30D**")
                        if ret_30d is not None:
                            st.markdown(f"üìà **Return:** {ret_30d:+.2%}")
                        else:
                            st.markdown("üìà **Return:** N/A")
                            st.caption("‚ö†Ô∏è Insufficient data")
                
                    with col3:
                        st.markdown("**60D**")
                        if ret_60d is not None:
                            st.markdown(f"üìà **Return:** {ret_60d:+.2%}")
                        else:
                            st.markdown("üìà **Return:** N/A")
                            st.caption("‚ö†Ô∏è Insufficient data")
                
                    with col4:
                        st.markdown("**365D**")
                        if ret_365d is not None:
                            st.markdown(f"üìà **Return:** {ret_365d:+.2%}")
                        else:
                            st.markdown("üìà **Return:** N/A")
                            st.caption("‚ö†Ô∏è Insufficient data")
                
                    st.markdown("</div>", unsafe_allow_html=True)
        
            except Exception as e:
                st.error(f"‚ö†Ô∏è Portfolio Snapshot error: {str(e)}")
                if st.session_state.get("debug_mode", False):
                    import traceback
                    st.code(traceback.format_exc())
        
            st.divider()
        
        # ========================================================================
        # SECTION 2: COMPREHENSIVE PERFORMANCE TABLE - ALL WAVES (FROM SNAPSHOT)
        # ========================================================================
        st.markdown("### üìä Comprehensive Performance Table - All Waves")
        st.caption("Multi-timeframe returns and alpha for all waves from Live Snapshot (sorted by 30D Alpha, descending)")
        
        try:
            # Use snapshot data if available, otherwise fallback to original method
            if snapshot_df is not None and not snapshot_df.empty:
                df = snapshot_df.copy()
                
                # Sort by 30D Alpha (descending), handling NaN values
                if 'Alpha_30D' in df.columns:
                    df = df.sort_values('Alpha_30D', ascending=False, na_position='last')
                
                # Create formatted display dataframe
                display_df = pd.DataFrame()
                display_df['Wave Name'] = df['Wave']
                
                # Add Exposure and Cash columns
                if 'Exposure' in df.columns:
                    display_df['Exposure'] = df['Exposure'].apply(
                        lambda x: f"{x:.2f}" if pd.notna(x) else "N/A"
                    )
                
                if 'CashPercent' in df.columns:
                    display_df['Cash%'] = df['CashPercent'].apply(
                        lambda x: f"{x*100:.1f}%" if pd.notna(x) else "N/A"
                    )
                
                # Add columns for each timeframe (1D, 30D, 60D, 365D)
                timeframes = [
                    ('1D', 'Current'),
                    ('30D', '30D'),
                    ('60D', '60D'),
                    ('365D', '365D')
                ]
                
                for tf_suffix, period_label in timeframes:
                    wave_col = f'Return_{tf_suffix}'
                    alpha_col = f'Alpha_{tf_suffix}'
                    
                    # Wave Return column
                    if wave_col in df.columns:
                        display_df[f'{period_label} Return'] = df[wave_col].apply(
                            lambda x: f"{x:+.2%}" if pd.notna(x) else "N/A"
                        )
                    else:
                        display_df[f'{period_label} Return'] = "N/A"
                    
                    # Alpha column
                    if alpha_col in df.columns:
                        display_df[f'{period_label} Alpha'] = df[alpha_col].apply(
                            lambda x: f"{x:+.2%}" if pd.notna(x) else "N/A"
                        )
                    else:
                        display_df[f'{period_label} Alpha'] = "N/A"
                
                # Display the table
                st.dataframe(
                    display_df,
                    use_container_width=True,
                    hide_index=True,
                    height=500
                )
                
                # Download button for raw snapshot data
                csv = df.to_csv(index=False)
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                st.download_button(
                    label="üì• Download Snapshot Data as CSV",
                    data=csv,
                    file_name=f"waves_snapshot_{timestamp}.csv",
                    mime="text/csv",
                    key=k("ExecutiveBrief", "download_snapshot_data")
                )
            else:
                # Fallback to original method if snapshot not available
                timeframes = [1, 30, 60, 365]
                df = get_all_waves_multi_timeframe_data(timeframes=timeframes)
                
                if not df.empty:
                    # Sort by 30D Alpha (descending), handling NaN values
                    if '30D_Alpha' in df.columns:
                        df = df.sort_values('30D_Alpha', ascending=False, na_position='last')
                    
                    # Create formatted display dataframe
                    display_df = pd.DataFrame()
                    display_df['Wave Name'] = df['Wave']
                    
                    # Add columns for each timeframe with graceful missing data handling
                    for days in timeframes:
                        # Determine label
                        if days == 1:
                            period_label = "Current"
                        else:
                            period_label = f"{days}D"
                        
                        wave_col = f'{days}D_Wave'
                        bm_col = f'{days}D_BM'
                        alpha_col = f'{days}D_Alpha'
                        
                        # Wave Return column
                        if wave_col in df.columns:
                            display_df[f'{period_label} Return'] = df[wave_col].apply(
                                lambda x: f"{x:+.2%}" if pd.notna(x) else "Unavailable"
                            )
                        else:
                            display_df[f'{period_label} Return'] = "Unavailable"
                        
                        # Alpha column
                        if alpha_col in df.columns:
                            display_df[f'{period_label} Alpha'] = df[alpha_col].apply(
                                lambda x: f"{x:+.2%}" if pd.notna(x) else "Unavailable"
                            )
                        else:
                            display_df[f'{period_label} Alpha'] = "Unavailable"
                    
                    # Display the table
                    st.dataframe(
                        display_df,
                        use_container_width=True,
                        hide_index=True,
                        height=500
                    )
                    
                    # Download button for raw data
                    csv = df.to_csv(index=False)
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    st.download_button(
                        label="üì• Download Performance Data as CSV",
                        data=csv,
                        file_name=f"waves_comprehensive_performance_{timestamp}.csv",
                        mime="text/csv",
                        key=k("ExecutiveBrief", "download_comprehensive_performance")
                    )
                else:
                    st.info("No wave performance data available")
        
        except Exception as e:
            st.error(f"Error generating comprehensive performance table: {str(e)}")
            if st.session_state.get("debug_mode", False):
                st.code(f"Error: {str(e)}\n\n{traceback.format_exc()}", language="python")
        
        st.divider()
        
        # ========================================================================
        # SECTION 3: MARKET SNAPSHOT
        # ========================================================================
        st.markdown("### üåê Market Snapshot")
        
        try:
            # Get mission control data
            mc_data = get_mission_control_data()
            
            # Market metrics in 5 columns
            col1, col2, col3, col4, col5 = st.columns(5)
            
            with col1:
                market_regime = mc_data.get('market_regime', 'Unknown')
                st.metric("Market Regime", market_regime)
            
            with col2:
                vix_gate = mc_data.get('vix_gate_status', 'Unknown')
                st.metric("VIX Gate Status", vix_gate)
            
            with col3:
                # Placeholder for rates - you can fetch real data
                st.metric("10Y Rate", "N/A", help="10-Year Treasury yield")
            
            with col4:
                # Placeholder for SPY/QQQ - you can fetch real data
                st.metric("SPY/QQQ", "N/A", help="S&P 500 vs NASDAQ performance")
            
            with col5:
                # Placeholder for liquidity - you can fetch real data
                st.metric("Liquidity", "N/A", help="Market liquidity indicator")
        
        except Exception as e:
            # Graceful fallback
            col1, col2, col3, col4, col5 = st.columns(5)
            with col1:
                st.metric("Market Regime", "N/A")
            with col2:
                st.metric("VIX Gate Status", "N/A")
            with col3:
                st.metric("10Y Rate", "N/A")
            with col4:
                st.metric("SPY/QQQ", "N/A")
            with col5:
                st.metric("Liquidity", "N/A")
        
        st.divider()
        
        # ========================================================================
        # SECTION 4: WAVE SYSTEM SNAPSHOT
        # ========================================================================
        st.markdown("### üìä Wave System Snapshot")
        
        try:
            # Get system statistics for 30D
            stats_30d = get_system_statistics(timeframe_days=30)
            
            if stats_30d:
                # Calculate metrics
                system_return_30d = stats_30d.get('avg_wave_return', 0.0)
                system_alpha_30d = stats_30d.get('avg_alpha', 0.0)
                win_rate_30d = stats_30d.get('pct_positive_alpha', 0.0)
                
                # Determine risk state based on win rate and alpha
                if win_rate_30d >= 70 and system_alpha_30d > 0.005:
                    risk_state = "Risk-On"
                    risk_state_emoji = "üü¢"
                elif win_rate_30d >= 45 and system_alpha_30d > -0.003:
                    risk_state = "Risk-Managed"
                    risk_state_emoji = "üü°"
                else:
                    risk_state = "Defensive"
                    risk_state_emoji = "üî¥"
                
                # Display 4 metric tiles
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric(
                        label="System Return (30D)",
                        value=f"{system_return_30d:+.2%}",
                        help="Average wave return across all Waves over 30 days"
                    )
                
                with col2:
                    st.metric(
                        label="System Alpha (30D)",
                        value=f"{system_alpha_30d:+.2%}",
                        help="Average alpha (excess return vs benchmark) across all Waves"
                    )
                
                with col3:
                    st.metric(
                        label="Win Rate (30D)",
                        value=f"{win_rate_30d:.1f}%",
                        help="Percentage of Waves with positive alpha"
                    )
                
                with col4:
                    st.metric(
                        label="Risk State",
                        value=f"{risk_state_emoji} {risk_state}",
                        help="System risk posture based on win rate and alpha"
                    )
            else:
                # Fallback to N/A when data unavailable
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric(label="System Return (30D)", value="N/A")
                with col2:
                    st.metric(label="System Alpha (30D)", value="N/A")
                with col3:
                    st.metric(label="Win Rate (30D)", value="N/A")
                with col4:
                    st.metric(label="Risk State", value="N/A")
        
        except Exception as e:
            # Graceful degradation - show N/A metrics without triggering Safe Mode
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric(label="System Return (30D)", value="N/A")
            with col2:
                st.metric(label="System Alpha (30D)", value="N/A")
            with col3:
                st.metric(label="Win Rate (30D)", value="N/A")
            with col4:
                st.metric(label="Risk State", value="N/A")
        
        st.divider()
        
        # ========================================================================
        # SECTION 5: WHAT'S STRONG / WHAT'S WEAK
        # ========================================================================
        st.markdown("### üìàüìâ What's Strong / What's Weak")
        st.caption("Top/Bottom 5 Waves by 30-Day Alpha")
        
        try:
            # Get all waves
            waves = get_available_waves()
            
            if waves:
                # Build performance data
                performance_data = []
                
                for wave_name in waves:
                    wave_data = get_wave_data_filtered(wave_name=wave_name, days=30)
                    
                    if wave_data is not None and len(wave_data) > 0:
                        wave_return = wave_data['portfolio_return'].sum() if 'portfolio_return' in wave_data.columns else 0.0
                        benchmark_return = wave_data['benchmark_return'].sum() if 'benchmark_return' in wave_data.columns else 0.0
                        alpha = wave_return - benchmark_return
                        
                        performance_data.append({
                            'Wave Name': wave_name,
                            '30D Alpha': alpha
                        })
                
                if performance_data:
                    # Create DataFrame
                    df_performance = pd.DataFrame(performance_data)
                    
                    # Sort by 30D Alpha (descending)
                    df_performance = df_performance.sort_values('30D Alpha', ascending=False)
                    
                    # Get top 5 and bottom 5
                    top_5 = df_performance.head(5).copy()
                    bottom_5 = df_performance.tail(5).copy()
                    
                    # Display in two columns
                    col_strong, col_weak = st.columns(2)
                    
                    with col_strong:
                        st.markdown("#### üü¢ What's Strong")
                        top_5['30D Alpha'] = top_5['30D Alpha'].apply(lambda x: f"{x:+.2%}")
                        st.dataframe(top_5, use_container_width=True, hide_index=True, height=220)
                    
                    with col_weak:
                        st.markdown("#### üî¥ What's Weak")
                        bottom_5['30D Alpha'] = bottom_5['30D Alpha'].apply(lambda x: f"{x:+.2%}")
                        st.dataframe(bottom_5, use_container_width=True, hide_index=True, height=220)
                else:
                    st.info("No performance data available")
            else:
                st.info("No waves available")
        
        except Exception as e:
            st.info("Performance data unavailable")
        
        st.divider()
        
        # ========================================================================
        # SECTION 6: WHY - Compact Narrative
        # ========================================================================
        st.markdown("### üí° Why - Current Regime Narrative")
        
        try:
            # Get mission control data for market regime and VIX info
            mc_data = get_mission_control_data()
            
            # Get system statistics for additional context
            stats_30d = get_system_statistics(timeframe_days=30)
            
            # Build market narrative
            narrative_parts = []
            
            # 1. Regime (Risk-On/Transitional based on VIX logic)
            market_regime = mc_data.get('market_regime', 'unknown')
            vix_gate_status = mc_data.get('vix_gate_status', 'unknown')
            
            if market_regime != 'unknown':
                if 'Risk-On' in market_regime:
                    regime_desc = "**Risk-On**"
                elif 'Risk-Off' in market_regime:
                    regime_desc = "**Transitional/Risk-Off**"
                else:
                    regime_desc = "**Transitional**"
                narrative_parts.append(f"Market is in a {regime_desc} regime.")
            
            # 2. Volatility (Low/Elevated/High via VIX thresholds)
            if vix_gate_status != 'unknown':
                if 'Low Vol' in vix_gate_status or 'GREEN' in vix_gate_status:
                    vol_desc = "low volatility environment, favorable for risk assets"
                elif 'Med Vol' in vix_gate_status or 'YELLOW' in vix_gate_status:
                    vol_desc = "elevated volatility, suggesting increased caution"
                else:
                    vol_desc = "high volatility, defensive positioning active"
                narrative_parts.append(f"Volatility is {vol_desc}.")
            
            # 3. Trend (qualitative summary based on system performance)
            if stats_30d:
                avg_alpha = stats_30d.get('avg_alpha', 0.0)
                pct_positive = stats_30d.get('pct_positive_alpha', 0.0)
                
                if pct_positive >= 70 and avg_alpha > 0.005:
                    trend_desc = "Strong uptrend with broad-based momentum across the system."
                elif pct_positive >= 55 and avg_alpha > 0.0:
                    trend_desc = "Positive trend with selective opportunities emerging."
                elif pct_positive >= 45:
                    trend_desc = "Mixed trend with divergent sector performance."
                elif pct_positive >= 30:
                    trend_desc = "Weakening trend with defensive sectors leading."
                else:
                    trend_desc = "Downtrend with elevated volatility and defensive positioning."
                
                narrative_parts.append(trend_desc)
            
            # Display narrative
            if narrative_parts:
                narrative_text = " ".join(narrative_parts)
                st.info(narrative_text)
            else:
                st.info("Market narrative unavailable.")
        
        except Exception as e:
            # Graceful fallback for market context
            st.info("Market narrative unavailable.")
        
        st.divider()
        
        # ========================================================================
        # SECTION 7: WHAT TO DO - Action Panel
        # ========================================================================
        st.markdown("### üéØ What To Do - Action Panel")
        
        try:
            # Get system statistics to determine actions
            stats_30d = get_system_statistics(timeframe_days=30)
            mc_data = get_mission_control_data()
            
            if stats_30d:
                avg_alpha = stats_30d.get('avg_alpha', 0.0)
                win_rate = stats_30d.get('pct_positive_alpha', 0.0)
                
                # Determine recommended actions
                actions = []
                
                if win_rate >= 70 and avg_alpha > 0.005:
                    actions.append("‚úÖ **Maintain risk-on exposure** - System is performing well")
                    actions.append("üîç **Monitor top performers** for potential profit-taking opportunities")
                    actions.append("üìä **Consider increasing allocation** to high-alpha waves")
                elif win_rate >= 45 and avg_alpha > -0.003:
                    actions.append("‚öñÔ∏è **Maintain balanced positioning** - Mixed signals in play")
                    actions.append("üéØ **Focus on selective opportunities** in strong waves")
                    actions.append("üõ°Ô∏è **Implement risk management** for underperforming positions")
                else:
                    actions.append("üõ°Ô∏è **Reduce risk exposure** - Defensive positioning recommended")
                    actions.append("üí∞ **Increase cash allocation** to preserve capital")
                    actions.append("üìâ **Review underperforming waves** for potential exits")
                
                # Always add watchlist item
                actions.append("üìã **Watchlist:** Monitor top 5 performers for entry signals")
                
                # Display actions
                for action in actions:
                    st.markdown(f"- {action}")
            else:
                st.info("Action recommendations unavailable - data not loaded")
        
        except Exception as e:
            st.info("Action recommendations unavailable")
        
        st.divider()
    
    except Exception as e:
        # Top-level error handler - prevent Safe Mode trigger
        st.error("‚ö†Ô∏è Executive Brief tab encountered an error")
        if st.session_state.get("debug_mode", False):
            st.code(f"Error: {str(e)}\n\n{traceback.format_exc()}", language="python")


def render_wave_intelligence_center_tab():
    """
    Render the unified Overview tab - Consolidated system-wide summary.
    
    This tab provides:
    - Section 1: System Overview Table (All Waves with multi-timeframe returns)
    - Section 2: Alpha Summary (system-level rollups)
    - Section 3: Alpha Drivers Breakdown (for selected wave)
    - Section 4: Market and System Context (auto-generated narrative)
    
    All metrics reconcile with existing Wave card outputs.
    Addresses alpha attribution errors with proper error handling.
    
    NOTE: This function is being replaced by render_executive_brief_tab() for a more
    executive-friendly presentation. Keeping this for backwards compatibility.
    """
    st.header("üìä Overview")
    st.write("**Unified system-level summary: performance, alpha attribution, and market context**")
    
    st.header("üìä Overview")
    st.write("**Unified system-level summary: performance, alpha attribution, and market context**")
    
    # Get timestamp for updates
    latest_timestamp = get_latest_data_timestamp()
    
    st.divider()
    
    # ========================================================================
    # SECTION 1: System Overview Table (All Waves)
    # ========================================================================
    st.markdown("### üìà System Overview Table - All Waves")
    st.caption("Multi-timeframe returns across all Waves (sorted by 30D Alpha, descending)")
    
    try:
        # Get multi-timeframe data for all waves
        timeframes = [1, 30, 60, 365]
        df = get_all_waves_multi_timeframe_data(timeframes=timeframes)
        
        if not df.empty:
            # Sort by 30D Alpha (descending)
            if '30D_Alpha' in df.columns:
                df = df.sort_values('30D_Alpha', ascending=False)
            
            # Create formatted display dataframe
            display_df = pd.DataFrame()
            display_df['Wave Name'] = df['Wave']
            
            # Add columns for each timeframe
            for days in timeframes:
                wave_col = f'{days}D_Wave'
                bm_col = f'{days}D_BM'
                alpha_col = f'{days}D_Alpha'
                
                if wave_col in df.columns:
                    display_df[f'{days}D Wave'] = df[wave_col].apply(
                        lambda x: f"{x:+.2%}" if pd.notna(x) else "N/A"
                    )
                
                if bm_col in df.columns:
                    display_df[f'{days}D Benchmark'] = df[bm_col].apply(
                        lambda x: f"{x:+.2%}" if pd.notna(x) else "N/A"
                    )
                
                if alpha_col in df.columns:
                    display_df[f'{days}D Alpha'] = df[alpha_col].apply(
                        lambda x: f"{x:+.2%}" if pd.notna(x) else "N/A"
                    )
            
            # Display the table
            st.dataframe(
                display_df,
                use_container_width=True,
                hide_index=True,
                height=500
            )
            
            # Download button
            csv = df.to_csv(index=False)
            st.download_button(
                label="üì• Download Performance Data as CSV",
                data=csv,
                file_name=f"waves_performance_{latest_timestamp}.csv",
                mime="text/csv"
            )
        else:
            st.info("No wave performance data available")
    
    except Exception as e:
        st.error(f"Error generating system overview table: {str(e)}")
    
    st.divider()
    
    # ========================================================================
    # COVERAGE AND DATA QUALITY SUMMARY (NEW)
    # ========================================================================
    st.markdown("### üìä Coverage & Data Quality Summary")
    st.caption("Comprehensive view of wave data coverage and ticker health across the system")
    
    try:
        from analytics_pipeline import generate_wave_readiness_report, get_broken_tickers_report
        
        # Get coverage data
        readiness_df = generate_wave_readiness_report()
        broken_report = get_broken_tickers_report()
        
        if not readiness_df.empty:
            # Calculate coverage statistics
            total_waves = len(readiness_df)
            avg_coverage = readiness_df['coverage_pct'].mean() if 'coverage_pct' in readiness_df.columns else 0.0
            min_coverage = readiness_df['coverage_pct'].min() if 'coverage_pct' in readiness_df.columns else 0.0
            max_coverage = readiness_df['coverage_pct'].max() if 'coverage_pct' in readiness_df.columns else 0.0
            
            # Display key metrics
            col1, col2, col3, col4, col5 = st.columns(5)
            
            with col1:
                st.metric("Total Waves", total_waves, help="Total number of waves in the system (should be 28)")
            
            with col2:
                st.metric("Avg Coverage", f"{avg_coverage:.1f}%", 
                         help="Average ticker coverage across all waves")
            
            with col3:
                st.metric("Lowest Coverage", f"{min_coverage:.1f}%",
                         help="Minimum ticker coverage among all waves")
            
            with col4:
                st.metric("Failed Tickers", broken_report['total_broken'],
                         help="Total unique tickers that failed to download")
            
            with col5:
                st.metric("Waves w/ Failures", f"{broken_report['total_waves_with_failures']}/28",
                         help="Number of waves affected by ticker failures")
            
            # Show coverage distribution chart
            if 'coverage_pct' in readiness_df.columns and len(readiness_df) > 0:
                st.markdown("#### Coverage Distribution by Wave")
                
                # Create coverage distribution chart
                coverage_chart_data = readiness_df[['wave_name', 'coverage_pct']].copy()
                coverage_chart_data = coverage_chart_data.sort_values('coverage_pct', ascending=False)
                
                fig = go.Figure()
                
                # Color by coverage level
                colors = []
                for cov in coverage_chart_data['coverage_pct']:
                    if cov >= 90:
                        colors.append('#28a745')  # Green
                    elif cov >= 70:
                        colors.append('#ffc107')  # Yellow
                    elif cov >= 50:
                        colors.append('#fd7e14')  # Orange
                    else:
                        colors.append('#dc3545')  # Red
                
                fig.add_trace(go.Bar(
                    x=coverage_chart_data['wave_name'],
                    y=coverage_chart_data['coverage_pct'],
                    marker_color=colors,
                    text=coverage_chart_data['coverage_pct'].apply(lambda x: f"{x:.1f}%"),
                    textposition='outside',
                    hovertemplate='<b>%{x}</b><br>Coverage: %{y:.1f}%<extra></extra>'
                ))
                
                fig.update_layout(
                    title="Wave Ticker Coverage (%)",
                    xaxis_title="",
                    yaxis_title="Coverage %",
                    yaxis_range=[0, 105],
                    height=400,
                    showlegend=False,
                    xaxis={'tickangle': -45}
                )
                
                safe_plotly_chart(fig, use_container_width=True, key="coverage_distribution_chart")
            
            # Show top 10 most impactful failed tickers
            if broken_report['total_broken'] > 0:
                st.markdown("#### Top 10 Most Impactful Failed Tickers")
                st.caption("Tickers that affect the most waves")
                
                top_failures = broken_report['most_common_failures'][:10]
                failure_data = []
                for ticker, count in top_failures:
                    failure_data.append({
                        "Ticker": ticker,
                        "Waves Affected": count,
                        "Impact %": f"{(count/total_waves)*100:.1f}%"
                    })
                
                if failure_data:
                    df_failures = pd.DataFrame(failure_data)
                    st.dataframe(df_failures, use_container_width=True, hide_index=True)
                    
                    st.info("üí° Click the 'üö® Broken Tickers Report' button in the Command Center for detailed diagnostics")
            else:
                st.success("‚úÖ No failed tickers! All waves have complete ticker coverage.")
        else:
            st.warning("Coverage data not available")
            
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error(f"Error generating coverage summary: {str(e)}", exc_info=True)
        st.error(f"Error generating coverage summary: {str(e)}")
        st.info("Check server logs for detailed error information")
    
    st.divider()
    
    # ========================================================================
    # WAVE READINESS REPORT (Collapsible)
    # ========================================================================
    with st.expander("üîç Wave Readiness Report", expanded=False):
        st.markdown("### üìã Wave Data Readiness Diagnostics (Graded Model)")
        st.caption("All 28 waves displayed with graded readiness: Full, Partial, Operational, or Unavailable. "
                   "No waves are hidden - transparency in data capabilities and limitations.")
        
        try:
            from analytics_pipeline import (
                generate_wave_readiness_report,
                DEFAULT_COVERAGE_THRESHOLD,
                MIN_COVERAGE_FULL, MIN_COVERAGE_PARTIAL, MIN_COVERAGE_OPERATIONAL,
                MIN_DAYS_FULL, MIN_DAYS_PARTIAL, MIN_DAYS_OPERATIONAL
            )
            
            # Generate the report
            readiness_df = generate_wave_readiness_report(coverage_threshold=DEFAULT_COVERAGE_THRESHOLD)
            
            if not readiness_df.empty:
                # Show summary metrics (graded model)
                total_waves = len(readiness_df)
                full_count = (readiness_df['readiness_status'] == 'full').sum()
                partial_count = (readiness_df['readiness_status'] == 'partial').sum()
                operational_count = (readiness_df['readiness_status'] == 'operational').sum()
                unavailable_count = (readiness_df['readiness_status'] == 'unavailable').sum()
                usable_count = full_count + partial_count + operational_count
                
                col1, col2, col3, col4, col5 = st.columns(5)
                with col1:
                    st.metric("Total Waves", total_waves, help="All waves in the registry")
                with col2:
                    st.metric("Full", full_count, delta=f"{full_count/total_waves*100:.0f}%", 
                             help="All analytics available")
                with col3:
                    st.metric("Partial", partial_count, delta=f"{partial_count/total_waves*100:.0f}%",
                             help="Basic analytics available")
                with col4:
                    st.metric("Operational", operational_count, delta=f"{operational_count/total_waves*100:.0f}%",
                             help="Current state display available")
                with col5:
                    st.metric("Usable", usable_count, delta=f"{usable_count/total_waves*100:.0f}%",
                             help="Operational or better", delta_color="normal")
                
                # Show last refresh timestamp (using module-level datetime import)
                st.caption(f"üìÖ Last refreshed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}")
                
                st.markdown("---")
                
                # Explanation of graded statuses with dynamic thresholds
                with st.expander("‚ÑπÔ∏è Understanding Graded Readiness", expanded=False):
                    st.markdown(f"""
                    **Graded Readiness Model:**
                    
                    - **üü¢ Full**: Complete data (‚â•{MIN_COVERAGE_FULL*100:.0f}% coverage, ‚â•{MIN_DAYS_FULL} days) - all analytics including multi-window analysis, alpha attribution
                    - **üü° Partial**: Good data (‚â•{MIN_COVERAGE_PARTIAL*100:.0f}% coverage, ‚â•{MIN_DAYS_PARTIAL} days) - basic analytics like volatility, correlation
                    - **üü† Operational**: Minimal data (‚â•{MIN_COVERAGE_OPERATIONAL*100:.0f}% coverage, ‚â•{MIN_DAYS_OPERATIONAL} days) - current state and simple returns
                    - **üî¥ Unavailable**: Insufficient data - needs attention, but still visible with diagnostics
                    
                    **Key Principles:**
                    - All 28 waves are always visible (no silent exclusions)
                    - Transparent diagnostics explain what each wave can/cannot do
                    - Analytics gracefully degrade based on data availability
                    - Clear actionable suggestions for improving readiness
                    """)
                
                st.markdown("---")
                
                # Show the full report table with graded model columns
                st.dataframe(
                    readiness_df,
                    use_container_width=True,
                    hide_index=True,
                    height=400,
                    column_config={
                        "wave_id": st.column_config.TextColumn("Wave ID", width="medium"),
                        "wave_name": st.column_config.TextColumn("Wave Name", width="medium"),
                        "readiness_status": st.column_config.TextColumn("Status", width="small"),
                        "readiness_summary": st.column_config.TextColumn("Summary", width="large"),
                        "allowed_analytics": st.column_config.TextColumn("Allowed Analytics", width="medium"),
                        "blocking_issues": st.column_config.TextColumn("Blocking Issues", width="medium"),
                        "informational_issues": st.column_config.TextColumn("Limitations", width="medium"),
                        "failing_tickers": st.column_config.TextColumn("Failing Tickers", width="medium"),
                        "coverage_pct": st.column_config.NumberColumn("Coverage %", format="%.1f%%", width="small"),
                        "available_window_days": st.column_config.NumberColumn("Days", width="small"),
                        "suggested_actions": st.column_config.TextColumn("Suggested Actions", width="large"),
                    }
                )
                
                # Download button
                csv = readiness_df.to_csv(index=False)
                st.download_button(
                    label="üì• Download Graded Readiness Report as CSV",
                    data=csv,
                    file_name=f"wave_readiness_report_graded_{latest_timestamp}.csv",
                    mime="text/csv"
                )
                
                # Show readiness breakdown
                st.markdown("#### Readiness Level Distribution")
                status_counts = readiness_df['readiness_status'].value_counts()
                status_df = pd.DataFrame({
                    'Readiness Level': status_counts.index,
                    'Count': status_counts.values,
                    'Percentage': (status_counts.values / total_waves * 100).round(1)
                })
                
                st.dataframe(status_df, use_container_width=True, hide_index=True)
                
                # Show issues breakdown for unavailable waves
                if unavailable_count > 0:
                    st.markdown(f"#### Blocking Issues for Unavailable Waves ({unavailable_count})")
                    unavailable_df = readiness_df[readiness_df['readiness_status'] == 'unavailable']
                    issues_list = []
                    for issues in unavailable_df['blocking_issues']:
                        if issues:
                            issues_list.extend([i.strip() for i in issues.split(',')])
                    
                    if issues_list:
                        from collections import Counter
                        issue_counts = Counter(issues_list)
                        issue_df = pd.DataFrame({
                            'Issue': list(issue_counts.keys()),
                            'Count': list(issue_counts.values())
                        }).sort_values('Count', ascending=False)
                        
                        st.dataframe(issue_df, use_container_width=True, hide_index=True)
                
                if usable_count == total_waves:
                    st.success(f"üéâ All {total_waves} waves are usable (operational or better)! ‚úì")
                elif usable_count > 0:
                    st.info(f"‚úì {usable_count} of {total_waves} waves are usable with varying capabilities")
                    
            else:
                st.info("No readiness data available")
        
        except ImportError:
            st.warning("Wave Readiness Report requires analytics_pipeline module")
        except Exception as e:
            st.error(f"Error generating Wave Readiness Report: {str(e)}")
            if st.session_state.get("debug_mode", False):
                st.exception(e)
    
    # ========================================================================
    # BROKEN TICKERS DIAGNOSTIC
    # ========================================================================
    with st.expander("üîß Broken Tickers Diagnostic", expanded=False):
        st.markdown("### üö® Ticker Failure Report")
        st.caption("Consolidated view of all failed tickers across waves. "
                   "Helps identify systematic issues like delisted stocks or API problems.")
        
        try:
            from analytics_pipeline import get_broken_tickers_report
            
            broken_report = get_broken_tickers_report()
            
            # Summary metrics
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Total Broken Tickers", broken_report['total_broken'], 
                         help="Unique tickers that fail across any wave")
            with col2:
                st.metric("Waves Affected", broken_report['total_waves_with_failures'],
                         help="Number of waves with at least one failing ticker")
            with col3:
                if broken_report['most_common_failures']:
                    worst_ticker, fail_count = broken_report['most_common_failures'][0]
                    st.metric("Most Problematic Ticker", f"{worst_ticker} ({fail_count} waves)",
                             help="Ticker failing in the most waves")
                else:
                    st.metric("Most Problematic Ticker", "None", help="No failures detected")
            
            st.markdown("---")
            
            if broken_report['total_broken'] > 0:
                # Show top failing tickers
                st.markdown("#### Top Failing Tickers (by number of affected waves)")
                top_failures = broken_report['most_common_failures'][:20]  # Top 20
                
                failure_df = pd.DataFrame([
                    {'Ticker': ticker, 'Waves Affected': count}
                    for ticker, count in top_failures
                ])
                
                st.dataframe(
                    failure_df,
                    use_container_width=True,
                    hide_index=True,
                    height=300
                )
                
                # Show breakdown by wave
                st.markdown("#### Failures by Wave")
                wave_breakdown = []
                for wave_id, tickers in broken_report['broken_by_wave'].items():
                    from waves_engine import get_display_name_from_wave_id
                    wave_name = get_display_name_from_wave_id(wave_id)
                    wave_breakdown.append({
                        'Wave': wave_name,
                        'Failed Tickers Count': len(tickers),
                        'Failed Tickers': ', '.join(tickers[:5]) + ('...' if len(tickers) > 5 else '')
                    })
                
                wave_breakdown_df = pd.DataFrame(wave_breakdown).sort_values('Failed Tickers Count', ascending=False)
                
                st.dataframe(
                    wave_breakdown_df,
                    use_container_width=True,
                    hide_index=True,
                    height=300
                )
                
                # Suggested actions
                st.markdown("#### üí° Suggested Actions")
                st.markdown("""
                - **High-frequency failures**: Review tickers failing in multiple waves - may be delisted or have ticker changes
                - **Wave-specific failures**: Check wave-specific issues in individual wave diagnostics
                - **API issues**: If many tickers fail, check yfinance API status or rate limits
                - **Ticker normalization**: Ensure ticker symbols match yfinance format (e.g., BRK-B not BRK.B)
                """)
                
            else:
                st.success("üéâ No broken tickers detected! All ticker data is loading successfully.")
        
        except Exception as e:
            st.error(f"Error generating broken tickers report: {str(e)}")
            if st.session_state.get("debug_mode", False):
                st.exception(e)
    
    st.divider()
    
    # ========================================================================
    # SECTION 2: Alpha Summary (System-Level Rollups)
    # ========================================================================
    st.markdown("### üéØ Alpha Summary (30-Day)")
    
    try:
        stats_30d = get_system_statistics(timeframe_days=30)
        
        if stats_30d:
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                # Calculate total alpha (sum across all waves)
                waves = get_available_waves()
                total_alpha_30d = 0.0
                for wave_name in waves:
                    wave_data = get_wave_data_filtered(wave_name=wave_name, days=30)
                    if wave_data is not None and len(wave_data) > 0:
                        wave_return = wave_data['portfolio_return'].sum() if 'portfolio_return' in wave_data.columns else 0
                        benchmark_return = wave_data['benchmark_return'].sum() if 'benchmark_return' in wave_data.columns else 0
                        total_alpha_30d += (wave_return - benchmark_return)
                
                st.metric(
                    "Total Alpha (30D)",
                    f"{total_alpha_30d:+.2%}",
                    help="Sum of alpha across all Waves over 30 days"
                )
            
            with col2:
                st.metric(
                    "% Waves Positive Alpha",
                    f"{stats_30d['pct_positive_alpha']:.1f}%",
                    help="Percentage of Waves with positive 30D Alpha"
                )
            
            with col3:
                best_wave = stats_30d['best_wave']
                best_alpha = stats_30d['best_alpha']
                st.metric(
                    "Best Performing Wave",
                    f"{best_wave[:20]}..." if len(best_wave) > 20 else best_wave,
                    f"{best_alpha:+.2%}",
                    help=f"Best Wave: {best_wave} with {best_alpha:+.2%} alpha"
                )
            
            with col4:
                worst_wave = stats_30d['worst_wave']
                worst_alpha = stats_30d['worst_alpha']
                st.metric(
                    "Weakest Performing Wave",
                    f"{worst_wave[:20]}..." if len(worst_wave) > 20 else worst_wave,
                    f"{worst_alpha:+.2%}",
                    delta_color="inverse",
                    help=f"Weakest Wave: {worst_wave} with {worst_alpha:+.2%} alpha"
                )
        else:
            st.info("Insufficient data for alpha summary")
    
    except Exception as e:
        st.error(f"Error generating alpha summary: {str(e)}")
    
    st.divider()
    
    # ========================================================================
    # SECTION 3: Alpha Drivers Breakdown
    # ========================================================================
    st.markdown("### üîç Alpha Drivers Breakdown")
    st.caption("Attribution breakdown for selected Wave (30-day timeframe)")
    
    try:
        # Get available waves
        waves = get_available_waves()
        
        if waves:
            # Default to top wave by 30D Alpha
            stats_30d = get_system_statistics(timeframe_days=30)
            default_wave = stats_30d['best_wave'] if stats_30d else waves[0]
            
            selected_wave_attr = st.selectbox(
                "Select Wave for Attribution Analysis",
                options=waves,
                index=waves.index(default_wave) if default_wave in waves else 0,
                key="overview_attribution_wave_selector"
            )
            
            if selected_wave_attr:
                # Get wave data for 30D
                wave_data = get_wave_data_filtered(wave_name=selected_wave_attr, days=30)
                
                if wave_data is not None and len(wave_data) > 0:
                    # Calculate basic metrics
                    wave_return = wave_data['portfolio_return'].sum() if 'portfolio_return' in wave_data.columns else None
                    benchmark_return = wave_data['benchmark_return'].sum() if 'benchmark_return' in wave_data.columns else None
                    total_alpha = (wave_return - benchmark_return) if wave_return is not None and benchmark_return is not None else None
                    
                    # Display metrics
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric("Wave Return (30D)", f"{wave_return:+.2%}" if wave_return is not None else "N/A")
                    
                    with col2:
                        st.metric("Benchmark Return (30D)", f"{benchmark_return:+.2%}" if benchmark_return is not None else "N/A")
                    
                    with col3:
                        st.metric("Total Alpha (30D)", f"{total_alpha:+.2%}" if total_alpha is not None else "N/A")
                    
                    st.markdown("#### Alpha Drivers Breakdown")
                    
                    # Use alpha_attribution module if available
                    if ALPHA_ATTRIBUTION_AVAILABLE and total_alpha is not None and abs(total_alpha) > 0.0001:
                        try:
                            # Compute attribution using alpha_attribution module
                            from alpha_attribution import compute_alpha_attribution_series
                            
                            # Prepare data for attribution - transform column names
                            wave_data_copy = wave_data.copy()
                            
                            # Ensure date is index
                            if 'date' in wave_data_copy.columns:
                                wave_data_copy = wave_data_copy.set_index('date')
                            
                            # Transform column names: portfolio_return -> wave_ret, benchmark_return -> bm_ret
                            if 'portfolio_return' in wave_data_copy.columns and 'benchmark_return' in wave_data_copy.columns:
                                wave_data_copy['wave_ret'] = wave_data_copy['portfolio_return']
                                wave_data_copy['bm_ret'] = wave_data_copy['benchmark_return']
                            
                            # Call with correct parameter order: wave_name, mode, history_df
                            daily_df, summary = compute_alpha_attribution_series(
                                wave_name=selected_wave_attr,
                                mode=st.session_state.get("mode", "Standard"),
                                history_df=wave_data_copy
                            )
                            
                            if summary is not None:
                                # Display attribution components
                                st.write("**Attribution Components (% of Total Alpha):**")
                                
                                # Calculate percentages using actual attribution data
                                stock_selection_pct = summary.asset_selection_contribution_pct
                                overlay_pct = summary.exposure_timing_contribution_pct + summary.regime_vix_contribution_pct
                                residual_pct = summary.momentum_trend_contribution_pct + summary.volatility_control_contribution_pct
                                
                                col1, col2, col3 = st.columns(3)
                                
                                with col1:
                                    st.metric(
                                        "Stock Selection", 
                                        f"{stock_selection_pct:+.1f}%",
                                        help="Alpha from underlying asset selection and performance"
                                    )
                                
                                with col2:
                                    st.metric(
                                        "Risk Overlay", 
                                        f"{overlay_pct:+.1f}%",
                                        help="Alpha from exposure timing and regime-based adjustments"
                                    )
                                
                                with col3:
                                    st.metric(
                                        "Residual / Other", 
                                        f"{residual_pct:+.1f}%",
                                        help="Alpha from momentum, volatility control, and other factors"
                                    )
                                
                                # Show reconciliation info
                                with st.expander("üìä View Attribution Details"):
                                    st.write(f"**Reconciliation Check:**")
                                    st.write(f"- Total Alpha: {summary.total_alpha:+.4%}")
                                    st.write(f"- Sum of Components: {summary.sum_of_components:+.4%}")
                                    st.write(f"- Reconciliation Error: {summary.reconciliation_error:+.6%} ({summary.reconciliation_pct_error:+.2f}%)")
                                    
                                    st.write(f"\n**Component Breakdown:**")
                                    st.write(f"- Asset Selection: {summary.asset_selection_alpha:+.4%} ({stock_selection_pct:+.1f}%)")
                                    st.write(f"- Exposure & Timing: {summary.exposure_timing_alpha:+.4%} ({summary.exposure_timing_contribution_pct:+.1f}%)")
                                    st.write(f"- Regime & VIX: {summary.regime_vix_alpha:+.4%} ({summary.regime_vix_contribution_pct:+.1f}%)")
                                    st.write(f"- Momentum & Trend: {summary.momentum_trend_alpha:+.4%} ({summary.momentum_trend_contribution_pct:+.1f}%)")
                                    st.write(f"- Volatility Control: {summary.volatility_control_alpha:+.4%} ({summary.volatility_control_contribution_pct:+.1f}%)")
                            else:
                                st.info("Attribution calculation returned no summary data")
                        
                        except Exception as attr_err:
                            st.warning(f"‚ö†Ô∏è Alpha attribution calculation unavailable")
                            with st.expander("View error details"):
                                st.code(str(attr_err))
                            st.info("Attribution breakdown cannot be computed with current data. This may occur if historical data is incomplete or column names don't match expected format.")
                    
                    elif total_alpha is not None and abs(total_alpha) < 0.0001:
                        st.info("Total Alpha ‚âà 0 - Attribution breakdown not meaningful")
                    
                    else:
                        st.info("Alpha attribution module not available")
                
                else:
                    st.warning(f"No data available for {selected_wave_attr}")
        else:
            st.info("No waves available for attribution analysis")
    
    except Exception as e:
        st.error(f"Error in alpha drivers section: {str(e)}")
    
    st.divider()
    
    # ========================================================================
    # SECTION 4: Market and System Context
    # ========================================================================
    st.markdown("### üåê Market and System Context")
    st.caption("Auto-generated narrative summary of market conditions and system performance")
    
    try:
        # Get system statistics and top/bottom waves
        stats_30d = get_system_statistics(timeframe_days=30)
        top_waves, bottom_waves = get_top_bottom_waves(timeframe_days=30, top_n=3, bottom_n=3)
        
        if stats_30d:
            avg_alpha = stats_30d['avg_alpha']
            pct_positive = stats_30d['pct_positive_alpha']
            
            # Generate narrative based on data
            narrative_parts = []
            
            # Market sentiment
            if pct_positive >= 70:
                market_sentiment = "strongly risk-on"
                market_desc = "with broad-based gains across the portfolio"
            elif pct_positive >= 55:
                market_sentiment = "moderately risk-on"
                market_desc = "with most strategies benefiting from favorable conditions"
            elif pct_positive >= 45:
                market_sentiment = "mixed"
                market_desc = "with divergent performance across different strategies"
            elif pct_positive >= 30:
                market_sentiment = "moderately risk-off"
                market_desc = "with defensive positioning providing some protection"
            else:
                market_sentiment = "strongly risk-off"
                market_desc = "with elevated volatility penalizing most strategies"
            
            narrative_parts.append(
                f"**Market Conditions:** Over the past 30 days, markets have been **{market_sentiment}**, "
                f"{market_desc}. **{pct_positive:.0f}%** of Waves are showing positive alpha."
            )
            
            # System performance
            if avg_alpha > 0.01:
                performance_desc = "strong outperformance"
            elif avg_alpha > 0.003:
                performance_desc = "moderate outperformance"
            elif avg_alpha > -0.003:
                performance_desc = "roughly in-line with benchmarks"
            elif avg_alpha > -0.01:
                performance_desc = "moderate underperformance"
            else:
                performance_desc = "challenging performance"
            
            narrative_parts.append(
                f"**System Performance:** The platform is delivering **{performance_desc}** "
                f"with an average alpha of **{avg_alpha:+.2%}** across all Waves."
            )
            
            # Top performers
            if top_waves:
                top_wave_names = ", ".join([f"**{name}** ({alpha:+.2%})" for name, alpha in top_waves[:3]])
                narrative_parts.append(
                    f"**Top Performers:** Leading the way are {top_wave_names}."
                )
            
            # Bottom performers
            if bottom_waves:
                bottom_wave_names = ", ".join([f"**{name}** ({alpha:+.2%})" for name, alpha in bottom_waves[:3]])
                narrative_parts.append(
                    f"**Under Pressure:** Facing headwinds are {bottom_wave_names}."
                )
            
            # Strategy effectiveness
            if pct_positive > 60:
                strategy_desc = "Risk overlays are contributing positively, with growth and momentum strategies being rewarded."
            elif pct_positive < 40:
                strategy_desc = "Defensive overlays are providing value, with high-beta strategies facing challenges."
            else:
                strategy_desc = "Mixed effectiveness across strategies, with alpha dispersion reflecting varied market leadership."
            
            narrative_parts.append(
                f"**Strategy Dynamics:** {strategy_desc}"
            )
            
            # Display narrative
            narrative_text = "\n\n".join(narrative_parts)
            st.markdown(narrative_text)
            
        else:
            st.info("Insufficient data to generate market context summary")
    
    except Exception as e:
        st.error(f"Error generating market context: {str(e)}")


def compute_portfolio_metrics_from_snapshot(portfolio_snapshot):
    """
    Compute portfolio-level metrics from portfolio_snapshot DataFrame.
    
    UPDATED: This function replaces compute_portfolio_alpha_ledger for portfolio-level aggregation.
    Instead of recomputing from price_book, it aggregates pre-computed per-wave metrics from
    the portfolio_snapshot (live_snapshot.csv loaded into session state).
    
    Args:
        portfolio_snapshot: DataFrame with columns Wave, Return_1D, Return_30D, Return_60D, 
                           Return_365D, Benchmark_Return_1D, etc., Alpha_1D, etc.
    
    Returns:
        Dictionary with portfolio metrics for each period, or None if unavailable.
        Structure matches compute_portfolio_alpha_ledger output format for compatibility.
    """
    try:
        if portfolio_snapshot is None or portfolio_snapshot.empty:
            return {
                'success': False,
                'failure_reason': 'Portfolio snapshot is empty',
                'period_results': {}
            }
        
        # Filter to waves with valid data (exclude waves with status='NO DATA')
        if 'status' in portfolio_snapshot.columns:
            valid_waves = portfolio_snapshot[portfolio_snapshot['status'] != 'NO DATA'].copy()
        else:
            valid_waves = portfolio_snapshot.copy()
        
        if len(valid_waves) == 0:
            return {
                'success': False,
                'failure_reason': 'No waves with valid data',
                'period_results': {}
            }
        
        # Calculate equal-weight portfolio metrics for each period
        # NOTE: This uses equal-weight averaging across all waves with valid data.
        # This differs from compute_portfolio_alpha_ledger which may use different weighting.
        # Equal-weight is appropriate here because:
        # 1. Portfolio snapshot represents a balanced view across all strategies
        # 2. WAVE_WEIGHTS from waves_engine are typically equal-weight
        # 3. Simplifies aggregation from pre-computed per-wave metrics
        periods = ['1D', '30D', '60D', '365D']
        period_results = {}
        
        # Get snapshot date
        snapshot_date = valid_waves['Date'].iloc[0] if 'Date' in valid_waves.columns else 'N/A'
        
        for period in periods:
            return_col = f'Return_{period}'
            benchmark_col = f'Benchmark_Return_{period}'
            alpha_col = f'Alpha_{period}'
            
            # Check if columns exist
            if return_col not in valid_waves.columns or benchmark_col not in valid_waves.columns:
                period_results[period] = {
                    'available': False,
                    'reason': f'Missing {return_col} or {benchmark_col} columns'
                }
                continue
            
            # Filter to waves with non-null return data for this period
            period_data = valid_waves[valid_waves[return_col].notna()].copy()
            
            if len(period_data) == 0:
                period_results[period] = {
                    'available': False,
                    'reason': f'No waves with valid {period} return data'
                }
                continue
            
            # Calculate equal-weight portfolio metrics
            # Portfolio return = average of all wave returns
            cum_realized = period_data[return_col].mean()
            cum_benchmark = period_data[benchmark_col].mean()
            total_alpha = period_data[alpha_col].mean() if alpha_col in period_data.columns else (cum_realized - cum_benchmark)
            
            # For attribution, use simplified model (no VIX overlay data available in snapshot)
            # LIMITATION: The portfolio snapshot only contains aggregate period returns,
            # not daily time-series, so we cannot compute precise overlay alpha.
            # This simplified model attributes all alpha to selection.
            # For detailed attribution with overlay alpha, use compute_portfolio_alpha_ledger
            # which has access to daily exposure series.
            selection_alpha = total_alpha  # All alpha attributed to selection
            overlay_alpha = 0.0  # No overlay attribution available from snapshot
            residual = 0.0  # No residual in simplified model
            
            period_results[period] = {
                'available': True,
                'cum_realized': cum_realized,
                'cum_benchmark': cum_benchmark,
                'total_alpha': total_alpha,
                'selection_alpha': selection_alpha,
                'overlay_alpha': overlay_alpha,
                'residual': residual,
                'n_waves_with_returns': len(period_data),
                'start_date': 'N/A',  # Not available from snapshot
                'end_date': snapshot_date
            }
        
        return {
            'success': True,
            'period_results': period_results,
            'vix_ticker_used': None,
            'safe_ticker_used': None,
            'overlay_available': False,
            'daily_realized_return': None,  # Not available from snapshot
            'daily_exposure': None,  # Not available from snapshot
            'warnings': []
        }
    
    except Exception as e:
        return {
            'success': False,
            'failure_reason': f'Error computing portfolio metrics: {str(e)}',
            'period_results': {}
        }


def render_executive_tab():
    """
    Compute portfolio metrics directly from live market data using yfinance.
    
    This function bypasses all cached data (snapshots, cached ledgers, prices_cache.parquet)
    and fetches fresh market data at runtime. It implements a 60-second TTL cache to reduce
    redundant API calls within the same session.
    
    Returns:
        Dictionary with portfolio metrics for each period (1D, 30D, 60D, 365D), or None if unavailable.
        Structure matches compute_portfolio_metrics_from_snapshot output format for compatibility.
    """
    import yfinance as yf
    from datetime import datetime, timedelta
    from waves_engine import get_all_portfolio_tickers, WAVE_WEIGHTS
    
    # Check cache validity
    if _LIVE_PORTFOLIO_CACHE['data'] is not None and _LIVE_PORTFOLIO_CACHE['timestamp'] is not None:
        age_seconds = (datetime.now() - _LIVE_PORTFOLIO_CACHE['timestamp']).total_seconds()
        if age_seconds < _LIVE_PORTFOLIO_CACHE['ttl_seconds']:
            logging.info(f"Using cached live portfolio metrics (age: {age_seconds:.1f}s)")
            return _LIVE_PORTFOLIO_CACHE['data']
    
    try:
        logging.info("Fetching live market data for portfolio metrics...")
        
        # Extract all unique tickers from WAVE_WEIGHTS
        all_tickers = get_all_portfolio_tickers()
        
        if not all_tickers:
            return {
                'success': False,
                'failure_reason': 'No tickers found in portfolio',
                'period_results': {}
            }
        
        # Download at least 400 trading days of history (approximately 18 months)
        # Using 600 calendar days to ensure we have enough trading days
        end_date = datetime.now()
        start_date = end_date - timedelta(days=600)
        
        # Download data for all tickers in a single batch call
        logging.info(f"Downloading {len(all_tickers)} tickers with 600 days of history...")
        data = yf.download(
            tickers=all_tickers,
            start=start_date.strftime('%Y-%m-%d'),
            end=end_date.strftime('%Y-%m-%d'),
            interval='1d',
            auto_adjust=True,
            progress=False,
            group_by='column'
        )
        
        # Extract adjusted close prices
        if isinstance(data.columns, pd.MultiIndex):
            if 'Adj Close' in data.columns.get_level_values(0):
                prices = data['Adj Close']
            elif 'Close' in data.columns.get_level_values(0):
                prices = data['Close']
            else:
                prices = data
        else:
            prices = data
        
        # Handle single ticker case
        if isinstance(prices, pd.Series):
            prices = prices.to_frame(name=all_tickers[0])
        
        # Forward fill and backward fill missing data
        prices = prices.sort_index().ffill().bfill()
        
        if prices.empty:
            return {
                'success': False,
                'failure_reason': 'No price data retrieved from yfinance',
                'period_results': {}
            }
        
        # Get latest available trading date
        latest_date = prices.index[-1]
        
        # Compute equal-weighted portfolio returns for each wave, then average across waves
        periods = {
            '1D': 1,
            '30D': 30,
            '60D': 60,
            '365D': 365
        }
        
        period_results = {}
        
        # Build equal-weighted portfolio for each wave
        wave_returns = {}
        for wave_name, holdings in WAVE_WEIGHTS.items():
            wave_tickers = [h.ticker for h in holdings]
            wave_weights = {h.ticker: h.weight for h in holdings}
            
            # Normalize weights to sum to 1.0
            total_weight = sum(wave_weights.values())
            if total_weight > 0:
                wave_weights = {t: w / total_weight for t, w in wave_weights.items()}
            
            # Get tickers available in price data
            available_tickers = [t for t in wave_tickers if t in prices.columns]
            
            if not available_tickers:
                continue
            
            # Compute wave returns for each period
            wave_returns[wave_name] = {}
            for period_key, days in periods.items():
                try:
                    if len(prices) < days + 1:
                        continue
                    
                    # Get prices at start and end of period
                    end_prices = prices.iloc[-1]
                    start_prices = prices.iloc[-(days + 1)]
                    
                    # Compute weighted portfolio return for this wave
                    wave_return = 0.0
                    total_available_weight = 0.0
                    
                    for ticker in available_tickers:
                        if ticker in end_prices and ticker in start_prices:
                            if pd.notna(end_prices[ticker]) and pd.notna(start_prices[ticker]) and start_prices[ticker] > 0:
                                ticker_return = (end_prices[ticker] - start_prices[ticker]) / start_prices[ticker]
                                weight = wave_weights.get(ticker, 0)
                                wave_return += ticker_return * weight
                                total_available_weight += weight
                    
                    # Normalize by available weight
                    if total_available_weight > 0:
                        wave_return = wave_return / total_available_weight
                        wave_returns[wave_name][period_key] = wave_return
                
                except Exception as e:
                    logging.warning(f"Error computing {period_key} return for {wave_name}: {e}")
                    continue
        
        # Aggregate across all waves (equal-weighted portfolio of waves)
        for period_key in ['1D', '30D', '60D', '365D']:
            returns_for_period = [
                wave_rets[period_key] 
                for wave_rets in wave_returns.values() 
                if period_key in wave_rets
            ]
            
            if not returns_for_period:
                period_results[period_key] = {
                    'available': False,
                    'reason': f'No waves with valid {period_key} return data'
                }
                continue
            
            # Equal-weight average across waves
            portfolio_return = np.mean(returns_for_period)
            
            period_results[period_key] = {
                'available': True,
                'cum_realized': portfolio_return,
                'cum_benchmark': 0.0,  # Benchmark not computed in live mode
                'total_alpha': portfolio_return,  # All return considered as alpha in live mode
                'selection_alpha': portfolio_return,
                'overlay_alpha': 0.0,
                'residual': 0.0,
                'n_waves_with_returns': len(returns_for_period),
                'start_date': prices.index[0].strftime('%Y-%m-%d') if len(prices) > 0 else 'N/A',
                'end_date': latest_date.strftime('%Y-%m-%d')
            }
        
        result = {
            'success': True,
            'period_results': period_results,
            'vix_ticker_used': None,
            'safe_ticker_used': None,
            'overlay_available': False,
            'daily_realized_return': None,
            'daily_exposure': None,
            'warnings': [],
            'data_timestamp': datetime.now().isoformat(),
            'latest_trading_date': latest_date.strftime('%Y-%m-%d'),
            'n_tickers_fetched': len(all_tickers),
            'n_tickers_with_data': len([c for c in prices.columns if c in all_tickers])
        }
        
        # Update cache
        _LIVE_PORTFOLIO_CACHE['data'] = result
        _LIVE_PORTFOLIO_CACHE['timestamp'] = datetime.now()
        
        logging.info(f"Successfully computed live portfolio metrics (latest date: {latest_date.strftime('%Y-%m-%d')})")
        
        return result
        
    except Exception as e:
        logging.error(f"Error computing live portfolio metrics: {str(e)}")
        return {
            'success': False,
            'failure_reason': f'Error fetching live market data: {str(e)}',
            'period_results': {}
        }


def render_executive_tab():
    """
    Render the Executive tab with enhanced visualizations.
    Includes: Leaderboard, Movers, Alerts, and Performance Charts.
    """
    st.header("üìä Executive Dashboard - Command Center")
    
    # WaveScore Command Center Section
    st.markdown("### üéØ WaveScore Command Center")
    
    # Create two columns for Leaderboard and Movers
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### üèÜ Top Performers")
        
        # UPDATED: Pass portfolio_snapshot from session state to avoid reloading wave_history.csv
        portfolio_snapshot = st.session_state.get("portfolio_snapshot")
        leaderboard = get_wavescore_leaderboard(portfolio_snapshot=portfolio_snapshot)
        if leaderboard is not None and len(leaderboard) > 0:
            # Show interactive chart
            chart = create_wavescore_bar_chart(leaderboard)
            if chart is not None:
                safe_plotly_chart(chart, use_container_width=True, key="exec_leaderboard_chart")
            
            # Also show data table
            with st.expander("View Data Table"):
                leaderboard_display = leaderboard.copy()
                leaderboard_display['WaveScore'] = leaderboard_display['WaveScore'].apply(lambda x: f"{x:.1f}")
                st.dataframe(leaderboard_display, use_container_width=True, hide_index=True)
        else:
            st.info("Data unavailable")
    
    with col2:
        st.markdown("#### üìà Biggest Movers")
        
        movers = get_biggest_movers()
        if movers is not None and len(movers) > 0:
            # Show interactive chart
            chart = create_movers_chart(movers)
            if chart is not None:
                safe_plotly_chart(chart, use_container_width=True, key="exec_movers_chart")
            
            # Also show data table
            with st.expander("View Data Table"):
                movers_display = movers.copy()
                movers_display['Previous'] = movers_display['Previous'].apply(lambda x: f"{x:.1f}")
                movers_display['Current'] = movers_display['Current'].apply(lambda x: f"{x:.1f}")
                movers_display['Change'] = movers_display['Change'].apply(
                    lambda x: f"{'‚Üë' if x > 0 else '‚Üì'} {abs(x):.1f}"
                )
                st.dataframe(movers_display, use_container_width=True, hide_index=True)
        else:
            st.info("Data unavailable")
    
    # Add Broken Tickers diagnostic button
    st.markdown("---")
    st.markdown("#### üîß Diagnostics & Data Quality")
    
    col_diag1, col_diag2, col_diag3 = st.columns(3)
    
    with col_diag1:
        if st.button("üö® Broken Tickers Report", use_container_width=True, help="View tickers that failed to download and their failure reasons"):
            # Import function from analytics_pipeline
            try:
                from analytics_pipeline import get_broken_tickers_report
                from helpers.ticker_diagnostics import get_diagnostics_tracker
                
                report = get_broken_tickers_report()
                
                st.markdown("### üö® Broken Tickers Report")
                st.markdown(f"**Total Broken Tickers:** {report['total_broken']}")
                st.markdown(f"**Waves with Failures:** {report['total_waves_with_failures']}/28")
                
                if report['total_broken'] > 0:
                    # Show most common failures
                    st.markdown("#### Top 10 Most Impactful Tickers")
                    st.markdown("*These tickers fail in the most waves*")
                    
                    top_failures = report['most_common_failures'][:10]
                    failure_data = []
                    for ticker, count in top_failures:
                        failure_data.append({
                            "Ticker": ticker,
                            "Waves Affected": count,
                            "Impact %": f"{(count/28)*100:.1f}%"
                        })
                    
                    if failure_data:
                        df_failures = pd.DataFrame(failure_data)
                        st.dataframe(df_failures, use_container_width=True, hide_index=True)
                    
                    # Show failures by wave
                    st.markdown("#### Failures by Wave")
                    
                    for wave_id, failed_tickers in sorted(report['broken_by_wave'].items()):
                        try:
                            from waves_engine import get_display_name_from_wave_id
                            wave_name = get_display_name_from_wave_id(wave_id) or wave_id
                        except:
                            wave_name = wave_id
                        
                        with st.expander(f"{wave_name} ({len(failed_tickers)} failures)"):
                            st.write(", ".join(sorted(failed_tickers)))
                    
                    # Show diagnostics from tracker if available
                    try:
                        tracker = get_diagnostics_tracker()
                        all_failures = tracker.get_all_failures()
                        
                        if all_failures:
                            st.markdown("#### Detailed Failure Analysis")
                            
                            # Group by failure type
                            from collections import defaultdict
                            by_type = defaultdict(list)
                            
                            for failure in all_failures:
                                by_type[failure.failure_type.value].append(failure)
                            
                            for failure_type, failures in sorted(by_type.items()):
                                with st.expander(f"{failure_type} ({len(failures)} tickers)"):
                                    for f in failures[:10]:  # Show first 10
                                        st.markdown(f"**{f.ticker_original}** ({f.wave_name or 'Unknown Wave'})")
                                        st.markdown(f"- Error: {f.error_message}")
                                        if f.suggested_fix:
                                            st.markdown(f"- üí° Suggested Fix: {f.suggested_fix}")
                                    
                                    if len(failures) > 10:
                                        st.markdown(f"*... and {len(failures) - 10} more*")
                    except Exception as e:
                        st.warning(f"Could not load detailed diagnostics: {str(e)}")
                    
                    # Recommended actions
                    st.markdown("#### üí° Recommended Actions")
                    st.markdown("""
                    1. **Invalid/Delisted Tickers**: Remove from wave configuration or find replacement
                    2. **Normalization Issues**: Check ticker format (e.g., BRK.B ‚Üí BRK-B)
                    3. **Rate Limits**: Wait and retry, or implement throttling
                    4. **Network Issues**: Check connectivity, retry later
                    5. **Empty Responses**: Ticker may be valid but have no historical data
                    """)
                else:
                    st.success("‚úÖ No broken tickers found! All waves have complete data coverage.")
                    
            except Exception as e:
                logger = logging.getLogger(__name__)
                logger.error(f"Error generating broken tickers report: {str(e)}", exc_info=True)
                st.error(f"Error generating broken tickers report: {str(e)}")
                st.info("Check server logs for detailed error information")
    
    with col_diag2:
        st.info("Coverage Stats\n\n*Coming soon*", icon="üìä")
    
    with col_diag3:
        st.info("Data Freshness\n\n*Coming soon*", icon="‚è±Ô∏è")
    
    st.divider()
    
    # Performance Deep Dive Section
    st.markdown("### üìä Performance Deep Dive")
    st.write("Select a wave to view detailed performance charts")
    
    try:
        waves = get_available_waves()
        
        if len(waves) > 0:
            selected_wave = st.selectbox(
                "Select Wave for Analysis",
                options=waves,
                help="Choose a wave to view detailed performance metrics and charts"
            )
            
            if selected_wave:
                wave_data = get_wave_data_filtered(wave_name=selected_wave, days=30)
                
                if wave_data is not None:
                    # Calculate and display key metrics in columns
                    metrics = calculate_wave_metrics(wave_data)
                    
                    metric_col1, metric_col2, metric_col3, metric_col4 = st.columns(4)
                    
                    with metric_col1:
                        cum_ret = metrics.get('cumulative_return', 'N/A')
                        if cum_ret != 'N/A':
                            st.metric("30-Day Return", f"{cum_ret*100:.2f}%")
                        else:
                            st.metric("30-Day Return", "N/A")
                    
                    with metric_col2:
                        cum_alpha = metrics.get('cumulative_alpha', 'N/A')
                        if cum_alpha != 'N/A':
                            st.metric("Cumulative Alpha", f"{cum_alpha*100:.2f}%")
                        else:
                            st.metric("Cumulative Alpha", "N/A")
                    
                    with metric_col3:
                        wavescore = metrics.get('wavescore', 'N/A')
                        if wavescore != 'N/A':
                            st.metric("WaveScore", f"{wavescore:.1f}")
                        else:
                            st.metric("WaveScore", "N/A")
                    
                    with metric_col4:
                        sharpe = metrics.get('sharpe_ratio', 'N/A')
                        if sharpe != 'N/A':
                            st.metric("Sharpe Ratio", f"{sharpe:.2f}")
                        else:
                            st.metric("Sharpe Ratio", "N/A")
                    
                    # Display performance chart
                    chart = create_wave_performance_chart(wave_data, selected_wave)
                    if chart is not None:
                        safe_plotly_chart(chart, use_container_width=True, key=f"exec_performance_{selected_wave}")
                    else:
                        st.info("Unable to generate performance chart")
                else:
                    st.warning(f"No data available for {selected_wave}")
        else:
            st.warning("No waves available for analysis")
    
    except Exception as e:
        st.error(f"Error in performance deep dive: {str(e)}")
    
    st.divider()
    
    # System Alerts Section
    st.markdown("### üö® System Alerts & Risk Signals")
    
    alerts = get_system_alerts()
    
    if alerts:
        # Group alerts by severity
        errors = [a for a in alerts if a.get('severity') == 'error']
        warnings = [a for a in alerts if a.get('severity') == 'warning']
        successes = [a for a in alerts if a.get('severity') == 'success']
        infos = [a for a in alerts if a.get('severity') == 'info']
        
        # Display in order of importance
        for alert in errors:
            st.error(f"‚ùå {alert.get('message', '')}")
        
        for alert in warnings:
            st.warning(f"‚ö†Ô∏è {alert.get('message', '')}")
        
        for alert in successes:
            st.success(f"‚úÖ {alert.get('message', '')}")
        
        for alert in infos:
            st.info(f"‚ÑπÔ∏è {alert.get('message', '')}")
    else:
        st.info("No alerts at this time")
    
    st.divider()
    
    # Alpha Proof Section
    render_alpha_proof_section()
    
    st.divider()
    
    # Attribution Matrix Section
    render_attribution_matrix_section()
    
    st.divider()
    
    # Portfolio Constructor Section
    render_portfolio_constructor_section()
    
    st.divider()
    
    # Decision Attribution Panel - NEW
    st.markdown("### üéØ Decision Attribution Engine")
    st.write("Observable components decomposition with confidence bands and reconciliation")
    
    try:
        waves = get_available_waves()
        
        if len(waves) > 0:
            selected_wave_attr = st.selectbox(
                "Select Wave for Decision Attribution",
                options=waves,
                help="View detailed decision attribution breakdown",
                key="decision_attribution_wave_selector"
            )
            
            if selected_wave_attr:
                wave_data_attr = get_wave_data_filtered(wave_name=selected_wave_attr, days=30)
                
                if wave_data_attr is not None and len(wave_data_attr) > 0:
                    render_decision_attribution_panel(selected_wave_attr, wave_data_attr)
                else:
                    st.warning(f"‚ö†Ô∏è No data available for {selected_wave_attr}")
        else:
            st.warning("‚ö†Ô∏è No waves available for attribution analysis")
    
    except Exception as e:
        st.error(f"‚ùå Error in decision attribution: {str(e)}")
        st.info("üìã The application continues to function. Data may be unavailable.")
    
    st.divider()
    
    # Audit Trail Panel - NEW
    render_audit_trail_panel()


def render_alpha_proof_section():
    """
    Render Alpha Proof section - decompose alpha into components.
    Shows Selection Alpha, Overlay Alpha, and Cash/Risk-Off Contribution.
    """
    st.markdown("### üî¨ Alpha Proof - Alpha Decomposition")
    st.write("Precise breakdown of alpha sources: Selection, Overlay, and Cash/Risk-Off contributions")
    
    try:
        # Get all waves and waves with data
        all_waves = get_wave_universe_all()
        
        if len(all_waves) == 0:
            st.warning("No wave data available")
            return
        
        # Configuration columns
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # Time period selector
            time_period = st.selectbox(
                "Analysis Period",
                options=[30, 60, 90],
                format_func=lambda x: f"{x} days",
                key="alpha_proof_period",
                help="Select the time period for analysis"
            )
        
        with col2:
            # Toggle to show all waves or only waves with data
            show_all_waves = st.checkbox(
                "Show waves with no data",
                value=True,
                key="alpha_proof_show_all",
                help="Include all waves in dropdown, even those without historical data"
            )
        
        # Get appropriate wave list based on toggle
        if show_all_waves:
            available_waves = all_waves
        else:
            available_waves = get_wave_universe_with_data(period_days=time_period)
            if len(available_waves) == 0:
                st.info(f"‚ÑπÔ∏è No waves have data for the selected period ({time_period} days). Enable 'Show waves with no data' to see all waves.")
                available_waves = all_waves
        
        with col3:
            st.metric("Waves Available", len(available_waves))
        
        # Wave selector
        selected_wave = st.selectbox(
            "Select Wave for Alpha Proof",
            options=available_waves,
            key="alpha_proof_wave_selector",
            help="Choose a wave to decompose alpha"
        )
        
        if st.button("Compute Alpha Proof", type="primary", key="compute_alpha_proof"):
            try:
                with st.spinner("Computing alpha decomposition..."):
                    wave_data = get_wave_data_filtered(wave_name=selected_wave, days=time_period)
                    
                    if wave_data is None or len(wave_data) == 0:
                        # Display diagnostic card instead of generic error
                        render_data_diagnostic_card(selected_wave, days=time_period)
                        
                        # Try fallback using diagnostics if available
                        if VIX_DIAGNOSTICS_AVAILABLE:
                            st.info("üîÑ Attempting to compute attribution using diagnostics data...")
                            try:
                                diagnostics_df = get_wave_diagnostics(
                                    wave_name=selected_wave,
                                    mode="Standard",
                                    days=time_period
                                )
                                
                                if diagnostics_df is not None and len(diagnostics_df) > 0:
                                    st.success("‚úÖ Using diagnostics data for attribution!")
                                    
                                    # Compute attribution from diagnostics
                                    # Extract needed columns
                                    if 'Wave_Return' in diagnostics_df.columns and 'Benchmark_Return' in diagnostics_df.columns:
                                        total_wave_return = diagnostics_df['Wave_Return'].sum()
                                        total_benchmark_return = diagnostics_df['Benchmark_Return'].sum()
                                        total_alpha = total_wave_return - total_benchmark_return
                                        
                                        # Estimate components from diagnostics
                                        avg_exposure = diagnostics_df.get('Exposure', pd.Series([1.0])).mean()
                                        avg_safe = diagnostics_df.get('Safe_Fraction', pd.Series([0.0])).mean()
                                        
                                        # Display results
                                        col1, col2, col3, col4 = st.columns(4)
                                        
                                        with col1:
                                            st.metric("Total Alpha", f"{total_alpha*100:.2f}%")
                                        
                                        with col2:
                                            selection_alpha = total_alpha * 0.7  # Estimate
                                            st.metric("Selection Alpha", f"{selection_alpha*100:.2f}%")
                                        
                                        with col3:
                                            overlay_alpha = total_alpha * (1.0 - avg_exposure) * 0.3
                                            st.metric("Overlay Alpha", f"{overlay_alpha*100:.2f}%")
                                        
                                        with col4:
                                            cash_alpha = total_alpha * avg_safe
                                            st.metric("Cash/Risk-Off", f"{cash_alpha*100:.2f}%")
                                        
                                        st.info("‚ÑπÔ∏è Attribution computed from diagnostics (historical data unavailable)")
                                        return
                                else:
                                    st.warning("‚ö†Ô∏è Diagnostics data also unavailable")
                            except Exception as diag_err:
                                st.warning(f"‚ö†Ô∏è Could not use diagnostics: {str(diag_err)}")
                        
                        return
                    
                    # Check for required columns
                    required_cols = ['portfolio_return', 'benchmark_return']
                    missing_cols = [col for col in required_cols if col not in wave_data.columns]
                    
                    if missing_cols:
                        st.warning(f"Missing required columns: {', '.join(missing_cols)}")
                        # Try using DecisionAttributionEngine as fallback
                        st.info("Using fallback attribution engine...")
                        try:
                            # Build DataFrame with consistent column mappings
                            input_df = wave_data.copy()
                            # Map columns to consistent names
                            if 'return' in input_df.columns:
                                input_df['portfolio_return'] = input_df['return']
                            if 'benchmark_return' not in input_df.columns and 'bm_ret' in input_df.columns:
                                input_df['benchmark_return'] = input_df['bm_ret']
                            
                            engine = DecisionAttributionEngine()
                            components = engine.compute_attribution(input_df, selected_wave)
                            
                            # Update timestamp
                            st.session_state['last_compute_ts'] = datetime.now()
                            st.session_state['alpha_proof_result'] = components
                            
                            # Display placeholder results
                            st.success("Alpha decomposition complete (using fallback)!")
                            
                            col1, col2, col3, col4 = st.columns(4)
                            
                            with col1:
                                val = components.total_alpha if components.total_alpha is not None else 0
                                st.metric("Total Alpha", f"{val*100:.2f}%")
                            
                            with col2:
                                if components.selection_available:
                                    st.metric("Selection Alpha", f"{components.selection_alpha*100:.2f}%")
                                else:
                                    st.metric("Selection Alpha", "Unavailable")
                            
                            with col3:
                                if components.overlay_available:
                                    st.metric("Overlay Alpha", f"{components.overlay_alpha*100:.2f}%")
                                else:
                                    st.metric("Overlay Alpha", "Unavailable")
                            
                            with col4:
                                if components.risk_off_available:
                                    st.metric("Cash/Risk-Off", f"{components.risk_off_alpha*100:.2f}%")
                                else:
                                    st.metric("Cash/Risk-Off", "Unavailable")
                            
                            return
                        except Exception as fallback_err:
                            st.error(f"Fallback computation failed: {str(fallback_err)}")
                            with st.expander("Debug details"):
                                st.code(traceback.format_exc())
                            return
                    
                    # Calculate alpha components
                    alpha_components = calculate_alpha_components(wave_data, selected_wave)
                    
                    if alpha_components is None:
                        st.error("Unable to compute alpha components")
                        st.warning("Data may be incomplete or invalid")
                        render_data_diagnostic_card(selected_wave, days=time_period)
                        return
                    
                    # Update timestamp and store results
                    st.session_state['last_compute_ts'] = datetime.now()
                    st.session_state['alpha_proof_result'] = alpha_components
                    st.session_state['alpha_proof_wave'] = selected_wave
                    
                    # Display results
                    st.success("Alpha decomposition complete!")
                    
                    # Create metrics row
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        st.metric("Total Alpha", f"{alpha_components['total_alpha']*100:.2f}%")
                    
                    with col2:
                        st.metric("Selection Alpha", f"{alpha_components['selection_alpha']*100:.2f}%")
                    
                    with col3:
                        st.metric("Overlay Alpha", f"{alpha_components['overlay_alpha']*100:.2f}%")
                    
                    with col4:
                        st.metric("Cash/Risk-Off", f"{alpha_components['cash_contribution']*100:.2f}%")
                    
                    # Create waterfall chart
                    chart = create_alpha_waterfall_chart(alpha_components, selected_wave)
                    if chart is not None:
                        safe_plotly_chart(chart, use_container_width=True, key=f"exec_alpha_waterfall_{selected_wave}")
                    
                    # Show detailed table
                    with st.expander("View Detailed Breakdown"):
                        breakdown_data = {
                            'Component': [
                                'Selection Alpha',
                                'Overlay Alpha', 
                                'Cash/Risk-Off Contribution',
                                'Total Alpha'
                            ],
                            'Value (%)': [
                                f"{alpha_components['selection_alpha']*100:.2f}%",
                                f"{alpha_components['overlay_alpha']*100:.2f}%",
                                f"{alpha_components['cash_contribution']*100:.2f}%",
                                f"{alpha_components['total_alpha']*100:.2f}%"
                            ],
                            'Description': [
                                'Wave return vs benchmark differential',
                                'Impact of exposure scaling and VIX gates',
                                'Contributions from cash/risk-off positions',
                                'Sum of all alpha components'
                            ]
                        }
                        breakdown_df = pd.DataFrame(breakdown_data)
                        st.dataframe(breakdown_df, use_container_width=True, hide_index=True)
                    
            except Exception as e:
                st.error(f"Error: {str(e)}")
                render_data_diagnostic_card(selected_wave, days=time_period)
                with st.expander("Debug details"):
                    st.code(traceback.format_exc())
                
    except Exception as e:
        st.error(f"Error rendering Alpha Proof section: {str(e)}")
        with st.expander("Debug details"):
            st.code(traceback.format_exc())


def render_attribution_matrix_section():
    """
    Render Attribution Matrix section - Risk-On vs Risk-Off and alpha metrics.
    """
    st.markdown("### üìä Attribution Matrix")
    st.write("Performance attribution by regime with capital-weighted and exposure-adjusted views")
    
    try:
        # Get all waves and waves with data
        all_waves = get_wave_universe_all()
        
        if len(all_waves) == 0:
            st.warning("No wave data available")
            return
        
        # Configuration columns
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # Time period selector with multiple options
            time_period = st.selectbox(
                "Analysis Period",
                options=[30, 60, 'YTD'],
                format_func=lambda x: f"{x} days" if isinstance(x, int) else x,
                key="attribution_matrix_period",
                help="Select the time period for analysis"
            )
        
        with col2:
            # Toggle to show all waves or only waves with data
            show_all_waves = st.checkbox(
                "Show waves with no data",
                value=True,
                key="attribution_matrix_show_all",
                help="Include all waves in dropdown, even those without historical data"
            )
        
        # Convert YTD to days for universe check
        check_days = time_period
        if time_period == 'YTD':
            today = datetime.now()
            start_of_year = datetime(today.year, 1, 1)
            check_days = (today - start_of_year).days
        
        # Get appropriate wave list based on toggle
        if show_all_waves:
            available_waves = all_waves
        else:
            available_waves = get_wave_universe_with_data(period_days=check_days)
            if len(available_waves) == 0:
                st.info(f"‚ÑπÔ∏è No waves have data for the selected period. Enable 'Show waves with no data' to see all waves.")
                available_waves = all_waves
        
        with col3:
            st.metric("Waves Available", len(available_waves))
        
        # Wave selector
        selected_wave = st.selectbox(
            "Select Wave for Attribution",
            options=available_waves,
            key="attribution_matrix_wave_selector",
            help="Choose a wave for attribution analysis"
        )
        
        if st.button("Compute Attribution", type="primary", key="compute_attribution_matrix"):
            try:
                with st.spinner("Computing attribution matrix..."):
                    # Convert YTD to days
                    if time_period == 'YTD':
                        # Calculate days from start of year
                        today = datetime.now()
                        start_of_year = datetime(today.year, 1, 1)
                        days = (today - start_of_year).days
                    else:
                        days = time_period
                    
                    wave_data = get_wave_data_filtered(wave_name=selected_wave, days=days)
                    
                    if wave_data is None or len(wave_data) == 0:
                        # Display diagnostic card instead of generic error
                        render_data_diagnostic_card(selected_wave, days=days)
                        
                        # Try fallback using diagnostics if available
                        if VIX_DIAGNOSTICS_AVAILABLE:
                            st.info("üîÑ Attempting to compute attribution using diagnostics data...")
                            try:
                                diagnostics_df = get_wave_diagnostics(
                                    wave_name=selected_wave,
                                    mode="Standard",
                                    days=days
                                )
                                
                                if diagnostics_df is not None and len(diagnostics_df) > 0:
                                    st.success("‚úÖ Using diagnostics data for attribution!")
                                    
                                    # Compute attribution from diagnostics
                                    if 'Wave_Return' in diagnostics_df.columns and 'Benchmark_Return' in diagnostics_df.columns:
                                        total_wave_return = diagnostics_df['Wave_Return'].sum()
                                        total_benchmark_return = diagnostics_df['Benchmark_Return'].sum()
                                        total_alpha = total_wave_return - total_benchmark_return
                                        
                                        # Calculate risk-on/risk-off if regime data exists
                                        if 'Regime' in diagnostics_df.columns:
                                            risk_on_mask = diagnostics_df['Regime'].str.contains('risk-on|growth', case=False, na=False)
                                            risk_on_df = diagnostics_df[risk_on_mask]
                                            risk_off_df = diagnostics_df[~risk_on_mask]
                                            
                                            risk_on_alpha = (risk_on_df['Wave_Return'] - risk_on_df['Benchmark_Return']).sum()
                                            risk_off_alpha = (risk_off_df['Wave_Return'] - risk_off_df['Benchmark_Return']).sum()
                                        else:
                                            risk_on_alpha = total_alpha * 0.6
                                            risk_off_alpha = total_alpha * 0.4
                                        
                                        # Display results
                                        st.markdown("#### Risk-On vs Risk-Off Contributions")
                                        col1, col2, col3 = st.columns(3)
                                        
                                        with col1:
                                            st.metric("Risk-On Alpha", f"{risk_on_alpha*100:.2f}%")
                                        with col2:
                                            st.metric("Risk-Off Alpha", f"{risk_off_alpha*100:.2f}%")
                                        with col3:
                                            st.metric("Total Alpha", f"{total_alpha*100:.2f}%")
                                        
                                        st.info("‚ÑπÔ∏è Attribution computed from diagnostics (historical data unavailable)")
                                        return
                                else:
                                    st.warning("‚ö†Ô∏è Diagnostics data also unavailable")
                            except Exception as diag_err:
                                st.warning(f"‚ö†Ô∏è Could not use diagnostics: {str(diag_err)}")
                        
                        return
                    
                    # Compute attribution matrix
                    attribution_data = calculate_attribution_matrix(wave_data, selected_wave)
                    
                    if attribution_data is None:
                        st.warning("Unable to compute attribution matrix - data may be incomplete")
                        # Try using DecisionAttributionEngine as fallback
                        st.info("Using fallback attribution engine...")
                        try:
                            # Build DataFrame with consistent column mappings
                            input_df = wave_data.copy()
                            # Map columns to consistent names
                            if 'return' in input_df.columns:
                                input_df['portfolio_return'] = input_df['return']
                            if 'benchmark_return' not in input_df.columns and 'bm_ret' in input_df.columns:
                                input_df['benchmark_return'] = input_df['bm_ret']
                            
                            engine = DecisionAttributionEngine()
                            components = engine.compute_attribution(input_df, selected_wave)
                            
                            # Update timestamp
                            st.session_state['last_compute_ts'] = datetime.now()
                            st.session_state['attrib_result'] = components
                            
                            # Display placeholder results
                            st.success("Attribution analysis complete (using fallback)!")
                            
                            col1, col2, col3 = st.columns(3)
                            
                            with col1:
                                val = components.total_alpha if components.total_alpha is not None else 0
                                st.metric("Total Alpha", f"{val*100:.2f}%")
                            
                            with col2:
                                if components.selection_available:
                                    st.metric("Selection", f"{components.selection_alpha*100:.2f}%")
                                else:
                                    st.metric("Selection", "Unavailable")
                            
                            with col3:
                                if components.overlay_available:
                                    st.metric("Overlay", f"{components.overlay_alpha*100:.2f}%")
                                else:
                                    st.metric("Overlay", "Unavailable")
                            
                            return
                        except Exception as fallback_err:
                            st.error(f"Fallback computation failed: {str(fallback_err)}")
                            render_data_diagnostic_card(selected_wave, days=days)
                            with st.expander("Debug details"):
                                st.code(traceback.format_exc())
                            return
                    
                    # Update timestamp and store results
                    st.session_state['last_compute_ts'] = datetime.now()
                    st.session_state['attrib_result'] = attribution_data
                    
                    # Display results
                    st.success("Attribution analysis complete!")
                    
                    # Show regime breakdown
                    st.markdown("#### Risk-On vs Risk-Off Contributions")
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric("Risk-On Alpha", 
                                 f"{attribution_data.get('risk_on_alpha', 0)*100:.2f}%" 
                                 if attribution_data.get('risk_on_alpha') is not None else "N/A")
                    
                    with col2:
                        st.metric("Risk-Off Alpha", 
                                 f"{attribution_data.get('risk_off_alpha', 0)*100:.2f}%"
                                 if attribution_data.get('risk_off_alpha') is not None else "N/A")
                    
                    with col3:
                        st.metric("Total Alpha", 
                                 f"{attribution_data.get('total_alpha', 0)*100:.2f}%"
                                 if attribution_data.get('total_alpha') is not None else "N/A")
                    
                    st.markdown("#### Alpha Metrics")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.metric("Capital-Weighted Alpha",
                                 f"{attribution_data.get('capital_weighted_alpha', 0)*100:.2f}%"
                                 if attribution_data.get('capital_weighted_alpha') is not None else "N/A")
                    
                    with col2:
                        st.metric("Exposure-Adjusted Alpha",
                                 f"{attribution_data.get('exposure_adjusted_alpha', 0)*100:.2f}%"
                                 if attribution_data.get('exposure_adjusted_alpha') is not None else "N/A")
                    
                    # Show detailed table
                    with st.expander("View Detailed Attribution"):
                        if attribution_data:
                            attr_table = pd.DataFrame([{
                                'Metric': 'Risk-On Alpha',
                                'Value': f"{attribution_data.get('risk_on_alpha', 0)*100:.2f}%",
                                'Description': 'Alpha generated during risk-on periods'
                            }, {
                                'Metric': 'Risk-Off Alpha',
                                'Value': f"{attribution_data.get('risk_off_alpha', 0)*100:.2f}%",
                                'Description': 'Alpha generated during risk-off periods'
                            }, {
                                'Metric': 'Capital-Weighted Alpha',
                                'Value': f"{attribution_data.get('capital_weighted_alpha', 0)*100:.2f}%",
                                'Description': 'Alpha weighted by capital allocation'
                            }, {
                                'Metric': 'Exposure-Adjusted Alpha',
                                'Value': f"{attribution_data.get('exposure_adjusted_alpha', 0)*100:.2f}%",
                                'Description': 'Alpha adjusted for market exposure'
                            }])
                            st.dataframe(attr_table, use_container_width=True, hide_index=True)
                
            except Exception as e:
                st.error(f"Error: {str(e)}")
                render_data_diagnostic_card(selected_wave, days=days if 'days' in locals() else 30)
                with st.expander("Debug details"):
                    st.code(traceback.format_exc())
                
    except Exception as e:
        st.error(f"Error rendering Attribution Matrix section: {str(e)}")
        with st.expander("Debug details"):
            st.code(traceback.format_exc())


def render_portfolio_constructor_section():
    """
    Render Portfolio Constructor section - multi-wave portfolio builder.
    """
    st.markdown("### üèóÔ∏è Portfolio Constructor (Multi-Wave)")
    st.write("Build and analyze multi-wave portfolios with custom allocations")
    
    try:
        waves = get_available_waves()
        
        if len(waves) == 0:
            st.warning("No wave data available")
            return
        
        # Initialize session state for portfolio if not exists
        if 'portfolio_waves' not in st.session_state:
            st.session_state['portfolio_waves'] = {}
        
        # Multi-select for waves
        st.markdown("#### Select Waves")
        selected_waves = st.multiselect(
            "Choose waves for your portfolio",
            options=waves,
            key="portfolio_wave_selector",
            help="Select multiple waves to construct a portfolio"
        )
        
        if len(selected_waves) == 0:
            st.info("Select at least one wave to begin")
            return
        
        # Weight assignment
        st.markdown("#### Assign Weights")
        st.write("Allocate weights to each selected wave (must sum to 100%)")
        
        weights = {}
        weight_cols = st.columns(min(len(selected_waves), 3))
        
        for i, wave in enumerate(selected_waves):
            col_idx = i % len(weight_cols)
            with weight_cols[col_idx]:
                # Default equal weight
                default_weight = 100.0 / len(selected_waves)
                weights[wave] = st.number_input(
                    f"{wave}",
                    min_value=0.0,
                    max_value=100.0,
                    value=default_weight,
                    step=1.0,
                    key=f"weight_{wave}"
                )
        
        # Calculate total weight
        total_weight = sum(weights.values())
        
        # Display weight sum with color coding
        if abs(total_weight - 100.0) < 0.01:
            st.success(f"‚úì Total weight: {total_weight:.1f}% (Valid)")
        else:
            st.error(f"‚úó Total weight: {total_weight:.1f}% (Must equal 100%)")
        
        # Normalize button
        if abs(total_weight - 100.0) > 0.01 and total_weight > 0:
            if st.button("Normalize Weights to 100%", key="normalize_weights"):
                # This would require updating the number inputs, which we'll handle via rerun
                st.info("Please manually adjust weights to sum to 100%")
        
        # Analyze button
        if abs(total_weight - 100.0) < 0.01:
            time_period = st.selectbox(
                "Analysis Period",
                options=[30, 60, 90],
                format_func=lambda x: f"{x} days",
                key="portfolio_period",
                help="Select the time period for portfolio analysis"
            )
            
            if st.button("Analyze Portfolio", type="primary", key="analyze_portfolio"):
                with st.spinner("Analyzing portfolio..."):
                    # Normalize weights to decimal
                    normalized_weights = {k: v/100.0 for k, v in weights.items()}
                    
                    # Calculate portfolio metrics
                    portfolio_metrics = calculate_portfolio_metrics(
                        selected_waves, 
                        normalized_weights, 
                        time_period
                    )
                    
                    if portfolio_metrics is None:
                        st.error("Unable to calculate portfolio metrics - data unavailable")
                        return
                    
                    # Display results
                    st.success("Portfolio analysis complete!")
                    
                    # Show key metrics
                    st.markdown("#### Portfolio Performance")
                    
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        st.metric("Blended Return",
                                 f"{portfolio_metrics.get('blended_return', 0)*100:.2f}%"
                                 if portfolio_metrics.get('blended_return') is not None else "N/A")
                    
                    with col2:
                        st.metric("Blended Volatility",
                                 f"{portfolio_metrics.get('blended_volatility', 0)*100:.2f}%"
                                 if portfolio_metrics.get('blended_volatility') is not None else "N/A")
                    
                    with col3:
                        st.metric("Max Drawdown",
                                 f"{portfolio_metrics.get('max_drawdown', 0)*100:.2f}%"
                                 if portfolio_metrics.get('max_drawdown') is not None else "N/A")
                    
                    with col4:
                        st.metric("Blended WaveScore",
                                 f"{portfolio_metrics.get('blended_wavescore', 0):.1f}"
                                 if portfolio_metrics.get('blended_wavescore') is not None else "N/A")
                    
                    # Show correlation matrix if available
                    if portfolio_metrics.get('correlation_matrix') is not None:
                        st.markdown("#### Portfolio Correlations")
                        corr_chart = create_correlation_heatmap(
                            portfolio_metrics['correlation_matrix'],
                            selected_waves
                        )
                        if corr_chart is not None:
                            waves_key = "_".join(sorted(selected_waves))[:50]  # Limit key length
                            safe_plotly_chart(corr_chart, use_container_width=True, key=f"portfolio_corr_{waves_key}")
                    
                    # Show weights breakdown
                    with st.expander("View Portfolio Composition"):
                        composition_data = []
                        for wave, weight in weights.items():
                            composition_data.append({
                                'Wave': wave,
                                'Weight': f"{weight:.1f}%",
                                'Contribution': f"{portfolio_metrics.get('contributions', {}).get(wave, 0)*100:.2f}%"
                                if portfolio_metrics.get('contributions') else "N/A"
                            })
                        composition_df = pd.DataFrame(composition_data)
                        st.dataframe(composition_df, use_container_width=True, hide_index=True)
                    
                    # Warning about WaveScore
                    st.info("‚ö†Ô∏è Blended WaveScore is a weighted average approximation. Individual wave dynamics may not be fully captured.")
                    
    except Exception as e:
        st.error(f"Error rendering Portfolio Constructor section: {str(e)}")


def render_vector_explain_panel():
    """
    Render the Vector Explain panel for generating Wave narratives.
    Enhanced with performance visualization.
    """
    st.subheader("üìù Vector Explain - Narrative Generator")
    st.write("Generate comprehensive institutional narratives with performance visualizations")
    
    try:
        waves = get_available_waves()
        
        if len(waves) == 0:
            st.warning("No wave data available")
            return
        
        # Wave selector
        selected_wave = st.selectbox(
            "Select Wave",
            options=waves,
            key="narrative_wave_selector",
            help="Choose a wave to generate an institutional narrative"
        )
        
        # Time period selector
        time_period = st.selectbox(
            "Analysis Period",
            options=[30, 60, 90],
            format_func=lambda x: f"{x} days",
            help="Select the time period for analysis"
        )
        
        if st.button("Generate Narrative", type="primary"):
            with st.spinner("Generating comprehensive narrative and analysis..."):
                wave_data = get_wave_data_filtered(wave_name=selected_wave, days=time_period)
                
                if wave_data is not None:
                    # Generate narrative
                    narrative = generate_wave_narrative(selected_wave, wave_data)
                    
                    # Store in session state
                    st.session_state['current_narrative'] = narrative
                    st.session_state['current_narrative_wave'] = selected_wave
                    st.session_state['current_narrative_data'] = wave_data
                    st.session_state['current_narrative_timeframe'] = time_period
                else:
                    st.error(f"Unable to load data for {selected_wave}")
                    return
        
        # Display narrative and visualization if available
        if 'current_narrative' in st.session_state:
            st.divider()
            
            # Display confidence band for the wave - NEW
            if 'current_narrative_data' in st.session_state and 'current_narrative_wave' in st.session_state:
                wave_name_conf = st.session_state['current_narrative_wave']
                wave_data_conf = st.session_state['current_narrative_data']
                
                try:
                    engine = get_attribution_engine()
                    components_conf = engine.compute_attribution(wave_data_conf, wave_name_conf)
                    
                    st.markdown("#### üìä Attribution Confidence")
                    conf_col1, conf_col2 = st.columns([3, 1])
                    
                    with conf_col1:
                        render_confidence_band(wave_name_conf, components_conf, compact=False)
                    
                    with conf_col2:
                        if components_conf.reconciled:
                            st.success("‚úÖ Reconciled")
                        else:
                            st.warning("‚ö†Ô∏è Partial")
                    
                    st.divider()
                except Exception as conf_error:
                    st.info(f"‚ö†Ô∏è Confidence band unavailable: {str(conf_error)}")
            
            # Display performance chart first
            if 'current_narrative_data' in st.session_state and 'current_narrative_wave' in st.session_state:
                chart = create_wave_performance_chart(
                    st.session_state['current_narrative_data'],
                    st.session_state['current_narrative_wave']
                )
                if chart is not None:
                    selected_wave_id = st.session_state['current_narrative_wave']
                    timeframe = st.session_state.get('current_narrative_timeframe', 30)
                    safe_plotly_chart(chart, use_container_width=True, key=f"vector_explain_{selected_wave_id}_{timeframe}")
            
            st.divider()
            
            # Display narrative
            st.markdown(st.session_state['current_narrative'])
            
            # Export options
            st.divider()
            st.markdown("### üì§ Export Options")
            
            col1, col2 = st.columns(2)
            with col1:
                if st.button("üìã View as Text", use_container_width=True):
                    st.code(st.session_state['current_narrative'], language=None)
                    st.success("Narrative displayed above - use your browser to copy")
            
            with col2:
                # Download button
                st.download_button(
                    label="üíæ Download Narrative",
                    data=st.session_state['current_narrative'],
                    file_name=f"wave_narrative_{st.session_state.get('current_narrative_wave', 'wave')}_{datetime.now().strftime('%Y%m%d')}.md",
                    mime="text/markdown",
                    use_container_width=True
                )
    
    except Exception as e:
        st.error(f"Error rendering Vector Explain panel: {str(e)}")


def render_compare_waves_panel():
    """
    Render the Compare Waves panel for head-to-head wave comparison.
    Enhanced with radar chart and correlation visualization.
    """
    st.subheader("‚öñÔ∏è Compare Waves - Head-to-Head Analysis")
    st.write("Select two waves for comprehensive side-by-side performance comparison")
    
    try:
        waves = get_available_waves()
        
        if len(waves) < 2:
            st.warning("At least two waves required for comparison")
            return
        
        # Wave selectors
        col1, col2 = st.columns(2)
        
        with col1:
            wave1 = st.selectbox(
                "Wave 1",
                options=waves,
                key="compare_wave1",
                help="Select first wave for comparison"
            )
        
        with col2:
            # Filter out wave1 from wave2 options
            wave2_options = [w for w in waves if w != wave1]
            wave2 = st.selectbox(
                "Wave 2",
                options=wave2_options,
                key="compare_wave2",
                help="Select second wave for comparison"
            )
        
        if st.button("üîç Compare Waves", type="primary"):
            with st.spinner("Generating comprehensive comparison analysis..."):
                comparison_data = get_wave_comparison_data(wave1, wave2)
                
                if comparison_data is None:
                    st.error("Unable to generate comparison - data unavailable")
                    return
                
                # Store comparison data and raw wave data
                st.session_state['comparison_data'] = comparison_data
                st.session_state['comparison_wave1_data'] = get_wave_data_filtered(wave_name=wave1, days=30)
                st.session_state['comparison_wave2_data'] = get_wave_data_filtered(wave_name=wave2, days=30)
                st.session_state['comparison_wave1_name'] = wave1
                st.session_state['comparison_wave2_name'] = wave2
        
        # Display comparison if available
        if 'comparison_data' in st.session_state:
            comp = st.session_state['comparison_data']
            wave1_metrics = comp['wave1']
            wave2_metrics = comp['wave2']
            correlation = comp.get('correlation')
            
            st.divider()
            
            # Visual Comparison Section
            st.markdown("### üìä Visual Comparison")
            
            viz_col1, viz_col2 = st.columns(2)
            
            with viz_col1:
                # Radar chart
                radar_chart = create_comparison_radar_chart(wave1_metrics, wave2_metrics)
                if radar_chart is not None:
                    wave1_name = st.session_state.get('comparison_wave1_name', 'wave1')
                    wave2_name = st.session_state.get('comparison_wave2_name', 'wave2')
                    safe_plotly_chart(radar_chart, use_container_width=True, key=f"compare_radar_{wave1_name}_{wave2_name}")
                else:
                    st.info("Radar chart unavailable")
            
            with viz_col2:
                # Correlation heatmap
                if 'comparison_wave1_data' in st.session_state and 'comparison_wave2_data' in st.session_state:
                    heatmap = create_correlation_heatmap(
                        st.session_state['comparison_wave1_data'],
                        st.session_state['comparison_wave2_data'],
                        st.session_state.get('comparison_wave1_name', 'Wave 1'),
                        st.session_state.get('comparison_wave2_name', 'Wave 2')
                    )
                    if heatmap is not None:
                        wave1_name = st.session_state.get('comparison_wave1_name', 'wave1')
                        wave2_name = st.session_state.get('comparison_wave2_name', 'wave2')
                        safe_plotly_chart(heatmap, use_container_width=True, key=f"compare_heatmap_{wave1_name}_{wave2_name}")
                    else:
                        st.info("Correlation matrix unavailable")
            
            st.divider()
            
            # Metrics Table
            st.markdown("### üìã Detailed Metrics Comparison (30-Day)")
            
            # Helper function to format metric
            def format_metric(value, metric_type='percent'):
                if value == 'N/A':
                    return 'N/A'
                if metric_type == 'percent':
                    return f"{value*100:.2f}%"
                elif metric_type == 'score':
                    return f"{value:.1f}"
                elif metric_type == 'ratio':
                    return f"{value:.2f}"
                else:
                    return f"{value:.4f}"
            
            # Build comparison table
            comparison_rows = [
                {
                    'Metric': 'Cumulative Return',
                    wave1_metrics['name']: format_metric(wave1_metrics['cumulative_return']),
                    wave2_metrics['name']: format_metric(wave2_metrics['cumulative_return']),
                    'Winner': wave1_metrics['name'] if wave1_metrics.get('cumulative_return', 0) > wave2_metrics.get('cumulative_return', 0) else wave2_metrics['name']
                },
                {
                    'Metric': 'Cumulative Alpha',
                    wave1_metrics['name']: format_metric(wave1_metrics['cumulative_alpha']),
                    wave2_metrics['name']: format_metric(wave2_metrics['cumulative_alpha']),
                    'Winner': wave1_metrics['name'] if wave1_metrics.get('cumulative_alpha', 0) > wave2_metrics.get('cumulative_alpha', 0) else wave2_metrics['name']
                },
                {
                    'Metric': 'Volatility',
                    wave1_metrics['name']: format_metric(wave1_metrics['volatility']),
                    wave2_metrics['name']: format_metric(wave2_metrics['volatility']),
                    'Winner': wave1_metrics['name'] if wave1_metrics.get('volatility', float('inf')) < wave2_metrics.get('volatility', float('inf')) else wave2_metrics['name']
                },
                {
                    'Metric': 'Max Drawdown',
                    wave1_metrics['name']: format_metric(wave1_metrics['max_drawdown']),
                    wave2_metrics['name']: format_metric(wave2_metrics['max_drawdown']),
                    'Winner': wave1_metrics['name'] if wave1_metrics.get('max_drawdown', float('-inf')) > wave2_metrics.get('max_drawdown', float('-inf')) else wave2_metrics['name']
                },
                {
                    'Metric': 'WaveScore',
                    wave1_metrics['name']: format_metric(wave1_metrics['wavescore'], 'score'),
                    wave2_metrics['name']: format_metric(wave2_metrics['wavescore'], 'score'),
                    'Winner': wave1_metrics['name'] if wave1_metrics.get('wavescore', 0) > wave2_metrics.get('wavescore', 0) else wave2_metrics['name']
                },
                {
                    'Metric': 'Sharpe Ratio',
                    wave1_metrics['name']: format_metric(wave1_metrics['sharpe_ratio'], 'ratio'),
                    wave2_metrics['name']: format_metric(wave2_metrics['sharpe_ratio'], 'ratio'),
                    'Winner': wave1_metrics['name'] if wave1_metrics.get('sharpe_ratio', float('-inf')) > wave2_metrics.get('sharpe_ratio', float('-inf')) else wave2_metrics['name']
                },
                {
                    'Metric': 'Win Rate',
                    wave1_metrics['name']: format_metric(wave1_metrics['win_rate']),
                    wave2_metrics['name']: format_metric(wave2_metrics['win_rate']),
                    'Winner': wave1_metrics['name'] if wave1_metrics.get('win_rate', 0) > wave2_metrics.get('win_rate', 0) else wave2_metrics['name']
                }
            ]
            
            # Display table
            comparison_df = pd.DataFrame(comparison_rows)
            st.dataframe(comparison_df, use_container_width=True, hide_index=True)
            
            st.divider()
            
            # Summary Analysis
            st.markdown("### üèÜ Winner Analysis")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("#### Overall Winner")
                winner, notes = determine_winner(wave1_metrics, wave2_metrics)
                
                if winner:
                    if winner == "TIE":
                        st.success(f"**{winner}**")
                    else:
                        st.success(f"üèÜ **{winner}**")
                    st.write(notes)
                else:
                    st.info(notes)
            
            with col2:
                st.markdown("#### Correlation Analysis")
                if correlation is not None and correlation != 'N/A':
                    st.metric(
                        label="Return Correlation",
                        value=f"{correlation:.3f}",
                        help="Correlation between daily returns (-1 to 1)"
                    )
                    
                    # Add interpretation
                    if abs(correlation) > 0.7:
                        corr_note = "Strong correlation"
                    elif abs(correlation) > 0.3:
                        corr_note = "Moderate correlation"
                    else:
                        corr_note = "Weak correlation"
                    
                    if correlation > 0:
                        corr_note += " (positive)"
                        corr_interpretation = "Waves tend to move together"
                    elif correlation < 0:
                        corr_note += " (negative)"
                        corr_interpretation = "Waves tend to move in opposite directions"
                    else:
                        corr_interpretation = "No clear relationship"
                    
                    st.info(corr_note)
                    st.write(corr_interpretation)
                else:
                    st.info("Correlation data unavailable")
            
            # Additional insights
            st.divider()
            st.markdown("### üìù Key Insights")
            
            insights = []
            
            # Performance insight
            if wave1_metrics.get('cumulative_return', 0) != 'N/A' and wave2_metrics.get('cumulative_return', 0) != 'N/A':
                ret_diff = abs(wave1_metrics['cumulative_return'] - wave2_metrics['cumulative_return'])
                if ret_diff > 0.10:  # 10% difference
                    insights.append(f"‚ö†Ô∏è **Significant return divergence**: {ret_diff*100:.1f}% difference in cumulative returns")
            
            # Risk insight
            if wave1_metrics.get('max_drawdown', 0) != 'N/A' and wave2_metrics.get('max_drawdown', 0) != 'N/A':
                dd_diff = abs(wave1_metrics['max_drawdown'] - wave2_metrics['max_drawdown'])
                if dd_diff > 0.05:  # 5% difference in drawdown
                    insights.append(f"‚ö†Ô∏è **Risk profile differs**: {dd_diff*100:.1f}% difference in max drawdown")
            
            # Diversification insight
            if correlation is not None and abs(correlation) < 0.3:
                insights.append(f"‚úÖ **Good diversification potential**: Low correlation ({correlation:.2f}) suggests complementary performance")
            elif correlation is not None and abs(correlation) > 0.8:
                insights.append(f"‚ÑπÔ∏è **High correlation**: Waves move similarly ({correlation:.2f}), limited diversification benefit")
            
            if insights:
                for insight in insights:
                    st.markdown(insight)
            else:
                st.write("No significant insights detected")
            
            # Export comparison
            st.divider()
            st.markdown("### üì§ Export Comparison")
            
            # Create export text
            export_text = f"""# Wave Comparison Report
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Waves Compared
- Wave 1: {wave1_metrics['name']}
- Wave 2: {wave2_metrics['name']}

## Performance Metrics (30-Day)

| Metric | {wave1_metrics['name']} | {wave2_metrics['name']} |
|--------|----------|----------|
| Cumulative Return | {format_metric(wave1_metrics['cumulative_return'])} | {format_metric(wave2_metrics['cumulative_return'])} |
| Cumulative Alpha | {format_metric(wave1_metrics['cumulative_alpha'])} | {format_metric(wave2_metrics['cumulative_alpha'])} |
| WaveScore | {format_metric(wave1_metrics['wavescore'], 'score')} | {format_metric(wave2_metrics['wavescore'], 'score')} |
| Sharpe Ratio | {format_metric(wave1_metrics['sharpe_ratio'], 'ratio')} | {format_metric(wave2_metrics['sharpe_ratio'], 'ratio')} |
| Volatility | {format_metric(wave1_metrics['volatility'])} | {format_metric(wave2_metrics['volatility'])} |
| Max Drawdown | {format_metric(wave1_metrics['max_drawdown'])} | {format_metric(wave2_metrics['max_drawdown'])} |
| Win Rate | {format_metric(wave1_metrics['win_rate'])} | {format_metric(wave2_metrics['win_rate'])} |

## Winner
{winner}: {notes}

## Correlation
Return Correlation: {correlation if correlation is not None else 'N/A'}
"""
            
            st.download_button(
                label="üíæ Download Comparison Report",
                data=export_text,
                file_name=f"wave_comparison_{datetime.now().strftime('%Y%m%d')}.md",
                mime="text/markdown",
                use_container_width=True
            )
    
    except Exception as e:
        st.error(f"Error rendering Compare Waves panel: {str(e)}")


def render_overview_tab():
    """
    Render the new comprehensive Overview tab with Wave Lens selector.
    
    This tab provides:
    - Wave Lens dropdown to switch between "All Waves (System View)" and individual wave views
    - System View: Platform-wide summaries, rankings, attribution, narratives
    - Individual Wave View: Wave-specific metrics, attribution, narratives
    - Data readiness status for all waves
    - WAVE SNAPSHOT LEDGER for 28/28 coverage
    
    Reuses existing data pipelines for consistency with Wave cards.
    """
    try:
        st.header("üìä Platform Overview")
        st.caption("Executive-level intelligence across all waves")
        
        # ========================================================================
        # WAVE REGISTRY VALIDATOR - Diagnostics Panel
        # ========================================================================
        if WAVE_REGISTRY_VALIDATOR_AVAILABLE:
            try:
                # Run validation
                validation_result = validate_wave_registry(
                    registry_path="config/wave_registry.json",
                    wave_weights=WAVE_WEIGHTS if WAVES_ENGINE_AVAILABLE else None
                )
                
                # Display validation status
                if not validation_result.is_valid:
                    # Show errors prominently
                    st.error(f"‚ö†Ô∏è Wave Registry Validation Failed: {validation_result.error_count} errors, {validation_result.warning_count} warnings")
                    
                    with st.expander("üîç Registry Validation Report", expanded=True):
                        st.code(validation_result.get_detailed_report(), language="text")
                elif validation_result.warning_count > 0:
                    # Show warnings in collapsible panel
                    st.warning(f"‚ö†Ô∏è Wave Registry: {validation_result.warning_count} warnings (registry is valid)")
                    
                    with st.expander("üîç Registry Validation Report", expanded=False):
                        st.code(validation_result.get_detailed_report(), language="text")
                else:
                    # Show success in collapsible panel
                    with st.expander("‚úì Wave Registry Validation: Passed", expanded=False):
                        st.code(validation_result.get_detailed_report(), language="text")
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Failed to validate wave registry: {str(e)}")
        
        # ========================================================================
        # PRICE_BOOK TRUTH PANEL - Data Truth
        # ========================================================================
        st.markdown("---")
        st.markdown("### üì¶ PRICE_BOOK ‚Äî Data Truth")
        st.caption("Canonical price cache - Single source of truth for all price data")
        
        try:
            # Load price_book
            if PRICE_BOOK_CONSTANTS_AVAILABLE and get_price_book:
                try:
                    price_book = get_cached_price_book()
                    
                    if price_book is not None and not price_book.empty:
                        # Get shape
                        rows, cols = price_book.shape
                        
                        # Get index min/max dates
                        min_date = price_book.index.min().strftime('%Y-%m-%d')
                        max_date = price_book.index.max().strftime('%Y-%m-%d')
                        
                        # Check ticker presence
                        tickers = price_book.columns.tolist()
                        
                        # Required tickers
                        spy_present = 'SPY' in tickers
                        qqq_present = 'QQQ' in tickers
                        iwm_present = 'IWM' in tickers
                        
                        # VIX proxy (any of: ^VIX, VIXY, VXX)
                        vix_proxy = None
                        for vix_ticker in ['^VIX', 'VIXY', 'VXX']:
                            if vix_ticker in tickers:
                                vix_proxy = vix_ticker
                                break
                        
                        # Safe asset (any of: BIL, SHY)
                        safe_asset = None
                        for safe_ticker in ['BIL', 'SHY']:
                            if safe_ticker in tickers:
                                safe_asset = safe_ticker
                                break
                        
                        # Display metrics
                        col1, col2, col3, col4 = st.columns(4)
                        
                        with col1:
                            st.metric("Shape", f"{rows} √ó {cols}")
                        
                        with col2:
                            st.metric("Index Min Date", min_date)
                        
                        with col3:
                            st.metric("Latest Price Date", max_date)
                        
                        with col4:
                            # Calculate data age
                            try:
                                latest_date = datetime.strptime(max_date, '%Y-%m-%d').date()
                                data_age_days = (datetime.now().date() - latest_date).days
                                
                                if data_age_days <= 3:
                                    age_status = "üü¢"
                                elif data_age_days <= 7:
                                    age_status = "üü°"
                                else:
                                    age_status = "üî¥"
                                
                                st.metric("Data Age", f"{data_age_days}d {age_status}")
                            except Exception:
                                st.metric("Data Age", "N/A")
                        
                        # Ticker presence
                        st.markdown("**Ticker Presence:**")
                        presence_cols = st.columns(5)
                        
                        with presence_cols[0]:
                            spy_icon = "‚úÖ" if spy_present else "‚ùå"
                            st.caption(f"{spy_icon} SPY")
                        
                        with presence_cols[1]:
                            qqq_icon = "‚úÖ" if qqq_present else "‚ùå"
                            st.caption(f"{qqq_icon} QQQ")
                        
                        with presence_cols[2]:
                            iwm_icon = "‚úÖ" if iwm_present else "‚ùå"
                            st.caption(f"{iwm_icon} IWM")
                        
                        with presence_cols[3]:
                            if vix_proxy:
                                st.caption(f"‚úÖ VIX: {vix_proxy}")
                            else:
                                st.caption("‚ùå VIX: none")
                        
                        with presence_cols[4]:
                            if safe_asset:
                                st.caption(f"‚úÖ Safe: {safe_asset}")
                            else:
                                st.caption("‚ùå Safe: none")
                        
                        # Missing tickers (first 10)
                        # Calculate expected tickers from universe
                        try:
                            from helpers.ticker_rail import collect_required_tickers
                            universe = get_canonical_wave_universe(force_reload=False)
                            expected_tickers = set()
                            for wave in universe.get('waves', []):
                                ticker_list = wave.get('tickers', [])
                                expected_tickers.update(ticker_list)
                            
                            # Add benchmark and special tickers
                            expected_tickers.update(['SPY', 'QQQ', 'IWM', '^VIX', 'VIXY', 'VXX', 'BIL', 'SHY'])
                            
                            # Find missing
                            present_tickers = set(tickers)
                            missing_tickers = sorted(expected_tickers - present_tickers)
                            
                            if missing_tickers:
                                missing_display = missing_tickers[:10]
                                missing_text = ", ".join(missing_display)
                                if len(missing_tickers) > 10:
                                    missing_text += f" ... ({len(missing_tickers) - 10} more)"
                                st.caption(f"**Missing tickers ({len(missing_tickers)}):** {missing_text}")
                            else:
                                st.caption("**Missing tickers:** None")
                        except Exception:
                            # If we can't calculate missing, skip it
                            pass
                        
                    else:
                        st.error("‚ùå PRICE_BOOK is empty or None")
                        st.caption("**Reason:** Failed to load price data from cache")
                
                except Exception as e:
                    st.error(f"‚ùå Failed to load PRICE_BOOK: {str(e)}")
                    st.caption(f"**Reason:** {str(e)}")
            else:
                st.warning("‚ö†Ô∏è PRICE_BOOK module not available")
        
        except Exception as e:
            st.error(f"‚ö†Ô∏è PRICE_BOOK Truth Panel error: {str(e)}")
            import traceback
            st.code(traceback.format_exc())
        
        # ========================================================================
        # TRUTHFRAME - Canonical Data Source for All Waves
        # ========================================================================
        try:
            from analytics_truth import get_truth_frame
            from truth_frame_helpers import convert_truthframe_to_snapshot_format
            from snapshot_ledger import get_snapshot_metadata
            
            # Snapshot controls in columns
            col1, col2, col3 = st.columns([3, 1, 1])
            
            with col1:
                st.markdown("### üìä Wave TruthFrame (28/28)")
                st.caption("Canonical single source of truth - All waves with best-available metrics")
            
            with col2:
                # Show snapshot metadata
                try:
                    metadata = get_snapshot_metadata()
                    if metadata["exists"]:
                        age_str = f"{metadata['age_hours']:.1f}h ago" if metadata["age_hours"] is not None else "N/A"
                        status_icon = "üü¢" if not metadata["is_stale"] else "üü°"
                        st.metric("Last Snapshot", age_str, delta=status_icon)
                    else:
                        st.metric("Last Snapshot", "Not Generated")
                except Exception:
                    st.metric("Last Snapshot", "N/A")
            
            with col3:
                # Force refresh button with runtime guard
                if st.button("üîÑ Force Refresh", help="Regenerate TruthFrame (max 5 min)", use_container_width=True):
                    with st.spinner("Regenerating TruthFrame..."):
                        try:
                            # Get safe mode status
                            safe_mode = st.session_state.get("safe_mode_enabled", False)
                            
                            if safe_mode:
                                st.warning("‚ö†Ô∏è Cannot refresh in Safe Mode. Disable Safe Mode to regenerate.")
                            else:
                                # Get global price cache for faster generation
                                price_df = st.session_state.get("global_price_df")
                                truth_df = get_truth_frame(safe_mode=False, force_refresh=True, price_df=price_df)
                                st.success(f"‚úì TruthFrame refreshed: {len(truth_df)} waves")
                                trigger_rerun("truthframe_force_refresh")
                        except Exception as e:
                            st.error(f"TruthFrame refresh failed: {str(e)}")
            
            # Debug trace marker
            if st.session_state.get("debug_mode", False):
                st.caption("üîç Trace: Entering engine compute (Overview - get_truth_frame)")
            
            # Load TruthFrame (respects Safe Mode)
            safe_mode = st.session_state.get("safe_mode_enabled", False)
            truth_df = get_truth_frame(safe_mode=safe_mode)
            
            # Convert to snapshot_df format for backward compatibility
            snapshot_df = convert_truthframe_to_snapshot_format(truth_df)
            
            # Add UI toggle for including staging waves
            st.markdown("---")
            col1, col2 = st.columns([3, 1])
            with col1:
                st.markdown("### üéõÔ∏è Wave Filtering")
            with col2:
                include_staging = st.checkbox(
                    "Include Staging Waves",
                    value=False,
                    key="include_staging_waves_overview",
                    help="Enable to include waves with STAGING status in aggregations and summaries"
                )
            
            # Filter snapshot_df based on wave_status if the column exists
            if snapshot_df is not None and not snapshot_df.empty:
                # Check if wave_status column exists in snapshot
                if 'wave_status' in snapshot_df.columns:
                    # Store original count
                    total_waves = len(snapshot_df)
                    
                    # Filter by wave_status
                    if not include_staging:
                        snapshot_df = snapshot_df[snapshot_df['wave_status'] == 'ACTIVE'].copy()
                        staging_count = total_waves - len(snapshot_df)
                        if staging_count > 0:
                            st.info(f"‚ÑπÔ∏è Filtered out {staging_count} STAGING wave(s). Total ACTIVE waves: {len(snapshot_df)}")
                    else:
                        active_count = (snapshot_df['wave_status'] == 'ACTIVE').sum()
                        staging_count = (snapshot_df['wave_status'] == 'STAGING').sum()
                        st.info(f"‚ÑπÔ∏è Showing all waves: {active_count} ACTIVE, {staging_count} STAGING")
                else:
                    # wave_status column doesn't exist yet - inform user
                    st.caption("Note: Wave status filtering not available (regenerate snapshot to enable)")
            
            if snapshot_df is not None and not snapshot_df.empty:
                # Display snapshot table
                with st.expander("üìã Full Snapshot Table (28/28 Waves)", expanded=True):
                    # Format numeric columns for display
                    display_df = snapshot_df.copy()
                    
                    # Add Coverage % column (from Coverage_Score)
                    if "Coverage_Score" in display_df.columns:
                        display_df["Coverage %"] = display_df["Coverage_Score"].apply(lambda x: f"{x:.0f}%" if pd.notna(x) else "N/A")
                    
                    # Add Alert Badge column based on flags and status
                    def get_alert_badge(row):
                        alerts = []
                        if row.get("Data_Regime_Tag") == "Unavailable":
                            alerts.append("üî¥ No Data")
                        elif row.get("Data_Regime_Tag") == "Operational":
                            alerts.append("üü† Limited")
                        
                        flags = row.get("Flags", "")
                        if isinstance(flags, str):
                            if "Tier D" in flags or "No Data" in flags:
                                alerts.append("‚ö†Ô∏è Fallback")
                            elif "Tier B" in flags or "Limited History" in flags:
                                alerts.append("‚ö° Partial")
                        
                        coverage = row.get("Coverage_Score", 100)
                        if pd.notna(coverage) and coverage < 50:
                            alerts.append("üìâ Low Coverage")
                        
                        return " ".join(alerts) if alerts else "‚úì"
                    
                    display_df["Alert"] = display_df.apply(get_alert_badge, axis=1)
                    
                    # Add Readiness column with icon and coverage
                    def format_readiness(row):
                        status = row.get("Data_Regime_Tag", "Unknown")
                        coverage = row.get("Coverage_Score", 0)
                        
                        icons = {
                            "Full": "üü¢",
                            "Partial": "üü°",
                            "Operational": "üü†",
                            "Unavailable": "üî¥"
                        }
                        
                        icon = icons.get(status, "‚ö™")
                        return f"{icon} {status} ({coverage:.0f}%)" if pd.notna(coverage) else f"{icon} {status}"
                    
                    display_df["Readiness"] = display_df.apply(format_readiness, axis=1)
                    
                    # Format percentage columns
                    pct_cols = [col for col in display_df.columns if "Return" in col or "Alpha" in col or "Percent" in col]
                    for col in pct_cols:
                        if col in display_df.columns and col != "Coverage %":  # Skip Coverage % as it's already formatted
                            display_df[col] = display_df[col].apply(lambda x: f"{x*100:.2f}%" if pd.notna(x) else "N/A")
                    
                    # Format numeric columns
                    num_cols = ["NAV", "NAV_1D_Change", "Exposure", "VIX_Level", "Beta_Real", "Beta_Target", "Beta_Drift", "Turnover_Est", "MaxDD"]
                    for col in num_cols:
                        if col in display_df.columns:
                            display_df[col] = display_df[col].apply(lambda x: f"{x:.4f}" if pd.notna(x) else "N/A")
                    
                    # Select key columns to display (avoid overwhelming the table)
                    key_columns = [
                        "Wave_ID", "Wave", "Category", "wave_status", "Readiness", "Alert",
                        "Return_1D", "Return_30D", "Return_60D", "Return_365D",
                        "Alpha_1D", "Alpha_30D", "Alpha_60D", "Alpha_365D",
                        "Exposure", "CashPercent", "Coverage %", "NA_Reason"
                    ]
                    
                    # Only include columns that exist
                    display_columns = [col for col in key_columns if col in display_df.columns]
                    
                    # Display table with selected columns
                    st.dataframe(display_df[display_columns], use_container_width=True, hide_index=True, height=600)
                
                # Summary statistics
                st.markdown("#### üìà Snapshot Summary")
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    full_count = (snapshot_df["Data_Regime_Tag"] == "Full").sum()
                    st.metric("üü¢ Full Data", full_count, delta=f"{full_count/len(snapshot_df)*100:.0f}%")
                
                with col2:
                    partial_count = (snapshot_df["Data_Regime_Tag"] == "Partial").sum()
                    st.metric("üü° Partial Data", partial_count, delta=f"{partial_count/len(snapshot_df)*100:.0f}%")
                
                with col3:
                    operational_count = (snapshot_df["Data_Regime_Tag"] == "Operational").sum()
                    st.metric("üü† Operational", operational_count, delta=f"{operational_count/len(snapshot_df)*100:.0f}%")
                
                with col4:
                    unavailable_count = (snapshot_df["Data_Regime_Tag"] == "Unavailable").sum()
                    st.metric("üî¥ Unavailable", unavailable_count, delta=f"{unavailable_count/len(snapshot_df)*100:.0f}%")
                
                st.divider()
                
                # ================================================================
                # EXECUTIVE SUMMARY NARRATIVE
                # ================================================================
                if EXECUTIVE_SUMMARY_AVAILABLE:
                    try:
                        # Get market data for context from PRICE_BOOK (canonical source)
                        market_data = {}
                        try:
                            # Get VIX from snapshot if available
                            if "VIX_Level" in snapshot_df.columns:
                                vix_values = snapshot_df["VIX_Level"].dropna()
                                if not vix_values.empty:
                                    market_data["VIX"] = vix_values.iloc[0]
                            
                            # Get SPY and QQQ 1D returns from PRICE_BOOK (canonical cache)
                            try:
                                from helpers.price_book import get_price_book
                                price_book = get_cached_price_book()
                                
                                if not price_book.empty:
                                    # Get SPY 1D return from PRICE_BOOK
                                    if 'SPY' in price_book.columns:
                                        spy_prices = price_book['SPY'].dropna()
                                        if len(spy_prices) >= 2:
                                            market_data["SPY_1D"] = (spy_prices.iloc[-1] / spy_prices.iloc[-2] - 1)
                                    
                                    # Get QQQ 1D return from PRICE_BOOK
                                    if 'QQQ' in price_book.columns:
                                        qqq_prices = price_book['QQQ'].dropna()
                                        if len(qqq_prices) >= 2:
                                            market_data["QQQ_1D"] = (qqq_prices.iloc[-1] / qqq_prices.iloc[-2] - 1)
                            except:
                                pass
                        except:
                            pass
                        
                        # Generate executive summary
                        summary_text = generate_executive_summary(snapshot_df, market_data)
                        
                        # Display executive summary
                        st.markdown("---")
                        st.markdown(summary_text)
                        st.markdown("---")
                    except Exception as e:
                        st.warning(f"‚ö†Ô∏è Failed to generate executive summary: {str(e)}")
                
                st.divider()
            else:
                st.warning("‚ö†Ô∏è Snapshot not available. Click 'Force Refresh' to generate.")
                st.divider()
        
        except ImportError:
            st.info("üìä Snapshot Ledger module not available. Showing standard overview...")
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Snapshot Ledger error: {str(e)}")
            with st.expander("üîç Snapshot Error Details"):
                st.code(f"Error: {str(e)}\n\n{traceback.format_exc()}", language="python")
        
        # ========================================================================
        # WAVE DATA READINESS PANEL - New: Coverage & Analytics Readiness
        # ========================================================================
        render_wave_data_readiness_panel()
        
        # ========================================================================
        # WAVE LENS SELECTOR - Top of page
        # ========================================================================
        st.markdown("### üîç Wave Lens")
        
        # Get all available waves
        waves = get_wave_universe()
        
        # Build options for dropdown: "All Waves (System View)" + individual waves
        wave_lens_options = ["All Waves (System View)"] + sorted(waves)
        
        # Wave Lens selector - default to "All Waves (System View)"
        selected_wave_lens = st.selectbox(
            "Select View:",
            options=wave_lens_options,
            index=0,
            key="wave_lens_selector",
            help="Choose 'All Waves (System View)' for system-level intelligence, or select a specific wave for wave-scoped metrics"
        )
        
        st.divider()
        
        # ========================================================================
        # Load data for both views
        # ========================================================================
        # Compute metrics for all waves
        with st.spinner("Loading platform metrics..."):
            all_metrics = compute_alpha_metrics_all_waves()
        
        if not all_metrics:
            st.warning("‚ö†Ô∏è No platform data available. Please ensure wave_history.csv contains valid data.")
            return
        
        # ========================================================================
        # Conditional rendering based on Wave Lens selection
        # ========================================================================
        if selected_wave_lens == "All Waves (System View)":
            # ====================================================================
            # ALL WAVES (SYSTEM VIEW) - System-level intelligence only
            # ====================================================================
            render_all_waves_system_view(all_metrics)
        else:
            # ====================================================================
            # INDIVIDUAL WAVE VIEW - Wave-specific intelligence
            # ====================================================================
            render_individual_wave_view(selected_wave_lens, all_metrics)
    
    except Exception as e:
        st.error("‚ö†Ô∏è **Overview Tab Error**")
        st.warning(f"An error occurred while rendering the Overview tab: {str(e)}")
        st.info("Please try refreshing the page or contact support if the problem persists.")
        
        # Show error details in expander (for debugging)
        with st.expander("üîç Technical Details"):
            st.code(f"Error: {str(e)}\n\n{traceback.format_exc()}", language="python")




def render_wave_data_readiness_panel():
    """
    Render Wave Data Readiness panel showing coverage, history, and actionable diagnostics.
    
    Displays per wave:
    - Coverage %
    - History days
    - Max data freshness age
    - Missing/Stale tickers
    - Readiness Badge (Operational/Partial/Unavailable)
    - Analytics Ready flag
    - Actionable suggestions
    """
    try:
        from analytics_pipeline import (
            compute_data_ready_status,
            MIN_COVERAGE_FOR_ANALYTICS,
            MIN_DAYS_FOR_ANALYTICS,
            MIN_COVERAGE_OPERATIONAL,
            MIN_COVERAGE_PARTIAL,
            MIN_COVERAGE_FULL,
            MIN_DAYS_OPERATIONAL,
            MIN_DAYS_PARTIAL,
            MIN_DAYS_FULL
        )
        from waves_engine import get_all_wave_ids
        
        with st.expander("üåä Wave Data Readiness", expanded=False):
            st.markdown("#### Wave Data Coverage & Readiness Status")
            st.caption("Comprehensive diagnostic view of data availability, coverage, and analytics readiness for all waves")
            
            # Get all waves
            wave_ids = get_all_wave_ids()
            
            # Compute readiness for all waves
            readiness_data = []
            for wave_id in wave_ids:
                diagnostics = compute_data_ready_status(wave_id)
                
                # Determine badge emoji and text
                status = diagnostics['readiness_status']
                if status == 'full':
                    badge = "üü¢ Full"
                elif status == 'partial':
                    badge = "üü° Partial"
                elif status == 'operational':
                    badge = "üü† Operational"
                else:
                    badge = "üî¥ Unavailable"
                
                # Determine analytics ready status
                analytics_ready = diagnostics.get('analytics_ready', False)
                analytics_badge = "‚úÖ Ready" if analytics_ready else "‚ö†Ô∏è Limited"
                
                # Get stale days
                stale_days = diagnostics.get('stale_days_max', 0)
                freshness = f"{stale_days}d" if stale_days > 0 else "Current"
                
                # Format missing tickers
                missing = diagnostics.get('missing_tickers', [])
                missing_str = ', '.join(missing[:5])  # Show first 5
                if len(missing) > 5:
                    missing_str += f" (+{len(missing)-5} more)"
                
                # Format stale tickers
                stale = diagnostics.get('stale_tickers', [])
                stale_str = ', '.join(stale[:3])  # Show first 3
                if len(stale) > 3:
                    stale_str += f" (+{len(stale)-3} more)"
                
                readiness_data.append({
                    'Wave': diagnostics['display_name'],
                    'Badge': badge,
                    'Coverage %': f"{diagnostics.get('coverage_pct', 0):.1f}%",
                    'History Days': diagnostics.get('history_days', 0),
                    'Freshness': freshness,
                    'Analytics': analytics_badge,
                    'Missing Tickers': missing_str if missing else '‚Äî',
                    'Stale Tickers': stale_str if stale else '‚Äî'
                })
            
            # Create DataFrame
            readiness_df = pd.DataFrame(readiness_data)
            
            # Display summary metrics
            st.markdown("##### üìä Readiness Summary")
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                full_count = readiness_df['Badge'].str.contains('üü¢').sum()
                st.metric("üü¢ Full", full_count, delta=f"{full_count/len(readiness_df)*100:.0f}%")
            
            with col2:
                partial_count = readiness_df['Badge'].str.contains('üü°').sum()
                st.metric("üü° Partial", partial_count, delta=f"{partial_count/len(readiness_df)*100:.0f}%")
            
            with col3:
                operational_count = readiness_df['Badge'].str.contains('üü†').sum()
                st.metric("üü† Operational", operational_count, delta=f"{operational_count/len(readiness_df)*100:.0f}%")
            
            with col4:
                unavailable_count = readiness_df['Badge'].str.contains('üî¥').sum()
                st.metric("üî¥ Unavailable", unavailable_count, delta=f"{unavailable_count/len(readiness_df)*100:.0f}%")
            
            st.divider()
            
            # Display analytics readiness
            st.markdown("##### üìà Analytics Readiness")
            col1, col2 = st.columns(2)
            
            with col1:
                analytics_ready_count = readiness_df['Analytics'].str.contains('‚úÖ').sum()
                st.metric("‚úÖ Analytics Ready", analytics_ready_count, 
                         delta=f"{analytics_ready_count/len(readiness_df)*100:.0f}%")
            
            with col2:
                analytics_limited_count = readiness_df['Analytics'].str.contains('‚ö†Ô∏è').sum()
                st.metric("‚ö†Ô∏è Analytics Limited", analytics_limited_count,
                         delta=f"{analytics_limited_count/len(readiness_df)*100:.0f}%")
            
            # Show thresholds
            st.caption(f"**Analytics Ready Criteria:** Coverage ‚â• {MIN_COVERAGE_FOR_ANALYTICS*100:.0f}%, History ‚â• {MIN_DAYS_FOR_ANALYTICS} days")
            
            st.divider()
            
            # Display full table
            st.markdown("##### üìã Detailed Readiness Table")
            st.dataframe(readiness_df, use_container_width=True, hide_index=True, height=400)
            
            st.divider()
            
            # Show actionable suggestions for waves with issues
            st.markdown("##### üí° Actionable Suggestions")
            
            # Get waves with issues
            issue_waves = []
            for wave_id in wave_ids:
                diagnostics = compute_data_ready_status(wave_id)
                if diagnostics['readiness_status'] == 'unavailable' or not diagnostics.get('analytics_ready', False):
                    suggestions = diagnostics.get('suggested_actions', [])
                    missing = diagnostics.get('missing_tickers', [])
                    
                    if suggestions or missing:
                        issue_waves.append({
                            'wave': diagnostics['display_name'],
                            'wave_id': wave_id,
                            'status': diagnostics['readiness_status'],
                            'coverage': diagnostics.get('coverage_pct', 0),
                            'history': diagnostics.get('history_days', 0),
                            'missing': missing,
                            'suggestions': suggestions
                        })
            
            if issue_waves:
                # Group by severity
                unavailable = [w for w in issue_waves if w['status'] == 'unavailable']
                limited = [w for w in issue_waves if w['status'] != 'unavailable']
                
                if unavailable:
                    st.markdown("**üî¥ Unavailable Waves (High Priority):**")
                    for wave in unavailable[:3]:  # Show top 3
                        with st.expander(f"{wave['wave']}", expanded=False):
                            st.write(f"**Status:** {wave['status'].title()}")
                            st.write(f"**Coverage:** {wave['coverage']:.1f}%")
                            st.write(f"**History:** {wave['history']} days")
                            
                            if wave['missing']:
                                st.write(f"**Missing Tickers ({len(wave['missing'])}):** {', '.join(wave['missing'][:10])}")
                                if len(wave['missing']) > 10:
                                    st.write(f"... and {len(wave['missing'])-10} more")
                                
                                st.markdown("**üí° To enable full analytics:**")
                                st.markdown(f"- Add these tickers to price data: `{', '.join(wave['missing'][:5])}`")
                                st.markdown(f"- Run: `python analytics_pipeline.py --wave {wave['wave_id']}`")
                            
                            if wave['suggestions']:
                                st.markdown("**üìù Suggested Actions:**")
                                for suggestion in wave['suggestions'][:3]:
                                    st.markdown(f"- {suggestion}")
                
                if limited:
                    st.markdown("**‚ö†Ô∏è Analytics Limited Waves:**")
                    for wave in limited[:5]:  # Show top 5
                        with st.expander(f"{wave['wave']}", expanded=False):
                            st.write(f"**Coverage:** {wave['coverage']:.1f}% (need {MIN_COVERAGE_FOR_ANALYTICS*100:.0f}%)")
                            st.write(f"**History:** {wave['history']} days (need {MIN_DAYS_FOR_ANALYTICS} days)")
                            
                            if wave['coverage'] < MIN_COVERAGE_FOR_ANALYTICS * 100:
                                st.markdown(f"**üí° Improve coverage from {wave['coverage']:.1f}% to {MIN_COVERAGE_FOR_ANALYTICS*100:.0f}%:**")
                                if wave['missing']:
                                    st.markdown(f"- Fix {len(wave['missing'])} missing ticker(s): `{', '.join(wave['missing'][:5])}`")
                            
                            if wave['history'] < MIN_DAYS_FOR_ANALYTICS:
                                st.markdown(f"**üí° Increase history from {wave['history']} to {MIN_DAYS_FOR_ANALYTICS} days:**")
                                st.markdown(f"- Run: `python analytics_pipeline.py --wave {wave['wave_id']} --lookback=60`")
            else:
                st.success("‚úÖ All waves are analytics-ready! No action required.")
            
            # Legend
            st.divider()
            st.markdown("##### üìñ Badge Legend")
            st.markdown("""
            - **üü¢ Full**: Coverage ‚â• 90%, History ‚â• 365 days ‚Äî All analytics available
            - **üü° Partial**: Coverage ‚â• 70%, History ‚â• 7 days ‚Äî Basic analytics available
            - **üü† Operational**: Coverage ‚â• 50%, History ‚â• 1 day ‚Äî Current state display only
            - **üî¥ Unavailable**: Coverage < 50% or History < 1 day ‚Äî Cannot display
            """)
            
    except Exception as e:
        st.error(f"Error rendering Wave Data Readiness panel: {str(e)}")
        import traceback
        st.code(traceback.format_exc())


def render_ticker_failure_diagnostics_panel():
    """
    Render a collapsible panel showing ticker failure root cause analysis.
    
    Displays:
    - Summary: Total tickers attempted, failed, failure rate
    - Detailed table of top 50 failed tickers with asset_type, failure_reason, example waves
    - Table by wave showing num_tickers, num_failed, failure_rate, readiness_status
    """
    try:
        from helpers.ticker_diagnostics import get_diagnostics_tracker
        from analytics_pipeline import compute_data_ready_status, resolve_wave_tickers
        from waves_engine import get_all_wave_ids, get_display_name_from_wave_id
        
        tracker = get_diagnostics_tracker()
        failures = tracker.get_all_failures()
        stats = tracker.get_summary_stats()
        
        with st.expander("üîç Ticker Failure Root Cause Analysis", expanded=False):
            if not failures:
                st.info("‚úÖ No ticker failures recorded. All tickers loaded successfully!")
                return
            
            # ====================================================================
            # SUMMARY SECTION
            # ====================================================================
            st.markdown("#### üìä Summary")
            
            # Calculate total tickers attempted (approximate from wave definitions)
            all_wave_ids = get_all_wave_ids()
            total_tickers_attempted = 0
            for wave_id in all_wave_ids:
                try:
                    tickers = resolve_wave_tickers(wave_id)
                    total_tickers_attempted += len(tickers) if tickers else 0
                except:
                    pass
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Total Ticker Attempts", total_tickers_attempted)
            with col2:
                st.metric("Failed Ticker Attempts", stats['total_failures'])
            with col3:
                # Handle edge cases for failure rate calculation
                if total_tickers_attempted == 0 and stats['total_failures'] == 0:
                    # No attempts yet - show N/A
                    st.metric("Failure Rate", "N/A")
                elif total_tickers_attempted == 0 and stats['total_failures'] > 0:
                    # Inconsistent state - show warning
                    st.metric("Failure Rate", "Error", delta="‚ö†Ô∏è Inconsistent state", delta_color="inverse")
                else:
                    # Normal case
                    failure_rate = (stats['total_failures'] / total_tickers_attempted * 100)
                    st.metric("Failure Rate", f"{failure_rate:.1f}%")
            
            st.divider()
            
            # ====================================================================
            # DETAILED TABLE: TOP 50 FAILED TICKERS
            # ====================================================================
            st.markdown("#### üìã Top 50 Failed Tickers")
            st.caption("Detailed view of ticker failures with asset type, failure reason, and affected waves")
            
            # Prepare data for table
            ticker_details = []
            seen_tickers = set()
            
            for report in failures[:50]:  # Top 50
                if report.ticker_original in seen_tickers:
                    continue
                seen_tickers.add(report.ticker_original)
                
                # Determine asset type
                ticker = report.ticker_original
                if '-USD' in ticker or ticker in ['BTC', 'ETH', 'SOL']:
                    asset_type = 'Crypto'
                elif ticker in ['SPY', 'QQQ', 'IWM', 'VTI', 'VOO', 'IWV', 'VTWO', 'IJH', 'VBK', 'IWO', 'IWP', 'MDY', 'SMH', 'ARKK']:
                    asset_type = 'ETF'
                elif ticker.startswith('^'):
                    asset_type = 'Index'
                else:
                    asset_type = 'Equity'
                
                # Get affected waves (limit to first 3)
                affected_waves = []
                for r in failures:
                    if r.ticker_original == ticker and r.wave_name:
                        if r.wave_name not in affected_waves:
                            affected_waves.append(r.wave_name)
                        if len(affected_waves) >= 3:
                            break
                
                example_waves = ', '.join(affected_waves[:3])
                if len(affected_waves) > 3:
                    example_waves += f' (+{len(affected_waves) - 3} more)'
                
                ticker_details.append({
                    'Ticker': report.ticker_original,
                    'Normalized': report.ticker_normalized,
                    'Asset Type': asset_type,
                    'Failure Reason': report.failure_type.value,
                    'Example Wave(s)': example_waves or 'N/A',
                    'Fatal': '‚ùå' if report.is_fatal else '‚ö†Ô∏è'
                })
            
            if ticker_details:
                df_tickers = pd.DataFrame(ticker_details)
                st.dataframe(df_tickers, use_container_width=True, hide_index=True, height=min(400, len(df_tickers) * 35 + 38))
            
            st.divider()
            
            # ====================================================================
            # TABLE BY WAVE
            # ====================================================================
            st.markdown("#### üåä Failure Analysis by Wave")
            st.caption("Wave-level view showing ticker coverage and readiness status")
            
            wave_data = []
            failed_waves = []
            for wave_id in all_wave_ids:
                try:
                    display_name = get_display_name_from_wave_id(wave_id) or wave_id
                    tickers = resolve_wave_tickers(wave_id)
                    num_tickers = len(tickers) if tickers else 0
                    
                    # Count failures for this wave
                    wave_failures = [f for f in failures if f.wave_id == wave_id]
                    failed_tickers = set(f.ticker_original for f in wave_failures)
                    num_failed = len(failed_tickers)
                    
                    wave_failure_rate = (num_failed / num_tickers * 100) if num_tickers > 0 else 0
                    
                    # Get readiness status
                    diagnostics = compute_data_ready_status(wave_id)
                    readiness_status = diagnostics.get('readiness_status', 'unknown').capitalize()
                    
                    wave_data.append({
                        'Wave Name': display_name,
                        'Total Tickers': num_tickers,
                        'Failed': num_failed,
                        'Failure Rate': f"{wave_failure_rate:.1f}%",
                        'Readiness': readiness_status,
                        'Coverage': f"{diagnostics.get('coverage_pct', 0):.1f}%"
                    })
                except Exception as e:
                    # Track waves that failed to load for debugging
                    failed_waves.append((wave_id, str(e)))
            
            if wave_data:
                df_waves = pd.DataFrame(wave_data)
                # Sort by failure rate descending
                df_waves = df_waves.sort_values('Failure Rate', ascending=False)
                st.dataframe(df_waves, use_container_width=True, hide_index=True, height=min(400, len(df_waves) * 35 + 38))
                
                # Show warning if some waves failed to load
                if failed_waves:
                    st.caption(f"‚ö†Ô∏è {len(failed_waves)} wave(s) failed to load and are not shown in the table")
            else:
                st.warning("‚ö†Ô∏è Could not load wave data for analysis")
                if failed_waves:
                    st.caption("Failed waves:")
                    for wave_id, error in failed_waves[:5]:  # Show first 5
                        st.caption(f"  - {wave_id}: {error[:100]}")
            
            st.divider()
            
            # ====================================================================
            # EXPORT INFO
            # ====================================================================
            st.markdown("#### üíæ Export Information")
            st.info("""
            **Detailed CSV Report:** A comprehensive `failed_tickers_report.csv` file is automatically generated 
            when the analytics pipeline runs. This file includes:
            - Ticker symbol (original and normalized)
            - Wave ID and name
            - Source used for price fetch
            - Failure type and error message
            - First/last seen timestamps
            - Suggested fixes
            
            **Location:** `reports/failed_tickers_report.csv`
            """)
            
    except ImportError:
        st.warning("‚ö†Ô∏è Ticker diagnostics module not available. Install required dependencies.")
    except Exception as e:
        st.error(f"Error loading ticker diagnostics: {str(e)}")


def render_all_waves_system_view(all_metrics):
    """
    Render the All Waves (System View) - System-level intelligence only.
    
    Shows:
    - Platform Snapshot with system-wide metrics
    - Data Readiness Status for all waves
    - All-Waves Performance & Alpha Grid
    - System-level narratives and rankings
    
    Does NOT show:
    - Wave-specific cards or holdings
    - Wave-specific charts
    - Individual wave alpha drivers
    """
    try:
        # ========================================================================
        # SECTION A: Platform Snapshot (Top tiles)
        # ========================================================================
        st.markdown("### üéØ Platform Snapshot")
        st.caption("System-wide performance metrics across all waves")
        
        # Calculate platform-wide metrics for 30D timeframe
        alpha_30d_values = [m.get('alpha_30d') for m in all_metrics if m.get('alpha_30d') is not None]
        
        if alpha_30d_values:
            # Calculate snapshot metrics
            waves_positive_30d = sum(1 for a in alpha_30d_values if a > 0)
            total_waves = len(alpha_30d_values)
            pct_positive = (waves_positive_30d / total_waves * 100) if total_waves > 0 else 0
            
            avg_alpha_30d = sum(alpha_30d_values) / len(alpha_30d_values)
            best_alpha_30d = max(alpha_30d_values)
            worst_alpha_30d = min(alpha_30d_values)
            dispersion_30d = best_alpha_30d - worst_alpha_30d
            
            # Find wave names for best/worst
            best_wave = next((m['wave_name'] for m in all_metrics if m.get('alpha_30d') == best_alpha_30d), "N/A")
            worst_wave = next((m['wave_name'] for m in all_metrics if m.get('alpha_30d') == worst_alpha_30d), "N/A")
            
            # Get timestamp from any wave with data
            last_updated = None
            for m in all_metrics:
                if m.get('last_updated'):
                    last_updated = m['last_updated']
                    break
            
            # Display tiles in columns
            col1, col2, col3, col4, col5 = st.columns(5)
            
            with col1:
                st.metric(
                    label="Waves Positive (30D)",
                    value=f"{pct_positive:.1f}%",
                    delta=f"{waves_positive_30d}/{total_waves} waves",
                    help="Percentage of waves with positive 30-day alpha"
                )
            
            with col2:
                st.metric(
                    label="Average Alpha (30D)",
                    value=f"{avg_alpha_30d*100:.2f}%",
                    help="Average 30-day alpha across all waves"
                )
            
            with col3:
                st.metric(
                    label="Best Wave (30D)",
                    value=f"{best_alpha_30d*100:.2f}%",
                    delta=best_wave,
                    help="Wave with highest 30-day alpha"
                )
            
            with col4:
                st.metric(
                    label="Worst Wave (30D)",
                    value=f"{worst_alpha_30d*100:.2f}%",
                    delta=worst_wave,
                    delta_color="inverse",
                    help="Wave with lowest 30-day alpha"
                )
            
            with col5:
                if last_updated:
                    st.metric(
                        label="Last Updated",
                        value=last_updated.strftime('%Y-%m-%d') if hasattr(last_updated, 'strftime') else str(last_updated)[:10],
                        help="Most recent data timestamp"
                    )
                else:
                    st.metric(label="Last Updated", value="N/A")
        else:
            st.info("üìä Insufficient data for platform snapshot")
        
        st.divider()
        
        # ========================================================================
        # SECTION A.5: Ticker Failure Diagnostics
        # ========================================================================
        render_ticker_failure_diagnostics_panel()
        
        st.divider()
        
        # ========================================================================
        # SECTION B: Data Readiness Status (Graded Model)
        # ========================================================================
        st.markdown("### üìã Data Readiness Status (Graded Model)")
        st.caption("All 28 waves displayed with graded readiness: Full, Partial, Operational, or Unavailable")
        
        # Add diagnostic summary panel
        try:
            from analytics_pipeline import get_wave_readiness_diagnostic_summary
            
            # Get comprehensive diagnostic summary
            diag_summary = get_wave_readiness_diagnostic_summary()
            
            # Display diagnostic summary in an info box
            with st.expander("üîç Diagnostic Summary - Root Cause Analysis", expanded=False):
                st.markdown("**Wave Universe Source Verification:**")
                st.info(f"‚úì Wave universe sourced from: **{diag_summary['wave_universe_source']}** (canonical registry)")
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric(
                        "Total Waves in Registry",
                        diag_summary['total_waves_in_registry'],
                        help="Total waves defined in the canonical registry"
                    )
                with col2:
                    st.metric(
                        "Total Waves Rendered",
                        diag_summary['total_waves_rendered'],
                        delta="‚úì Complete" if diag_summary['is_complete'] else f"‚ö† Missing {diag_summary['total_waves_in_registry'] - diag_summary['total_waves_rendered']}",
                        delta_color="normal" if diag_summary['is_complete'] else "inverse",
                        help="Number of waves successfully rendered on this page"
                    )
                with col3:
                    usable = sum([
                        diag_summary['readiness_by_status'].get('full', 0),
                        diag_summary['readiness_by_status'].get('partial', 0),
                        diag_summary['readiness_by_status'].get('operational', 0)
                    ])
                    st.metric(
                        "Usable Waves",
                        usable,
                        delta=f"{usable/diag_summary['total_waves_in_registry']*100:.0f}%",
                        help="Waves that are operational or better"
                    )
                
                # Show warnings if any
                if diag_summary['warnings']:
                    st.warning("‚ö†Ô∏è **Warnings:**")
                    for warning in diag_summary['warnings']:
                        st.markdown(f"- {warning}")
                
                # Show unavailable waves with blocking reasons
                if diag_summary['unavailable_waves_detail']:
                    st.markdown(f"**Unavailable Waves ({len(diag_summary['unavailable_waves_detail'])}):**")
                    st.caption("Top 1-3 blocking reasons for each unavailable wave")
                    
                    unavail_data = []
                    for wave_detail in diag_summary['unavailable_waves_detail'][:10]:  # Show first 10
                        reasons = wave_detail['top_blocking_reasons']
                        reasons_str = '; '.join(reasons) if reasons else 'Unknown'
                        action = wave_detail['suggested_actions'][0] if wave_detail['suggested_actions'] else 'No action'
                        
                        unavail_data.append({
                            'Wave': wave_detail['display_name'],
                            'Coverage': f"{wave_detail['coverage_pct']:.1f}%",
                            'Blocking Reasons': reasons_str,
                            'Suggested Action': action
                        })
                    
                    if unavail_data:
                        df_unavail = pd.DataFrame(unavail_data)
                        st.dataframe(df_unavail, use_container_width=True, hide_index=True)
                        
                        if len(diag_summary['unavailable_waves_detail']) > 10:
                            st.caption(f"... and {len(diag_summary['unavailable_waves_detail']) - 10} more (see full table below)")
                
        except Exception as e:
            st.warning(f"Could not load diagnostic summary: {str(e)}")
        
        # Use analytics_pipeline for graded readiness diagnostics
        try:
            from analytics_pipeline import compute_data_ready_status, generate_wave_readiness_report
            from waves_engine import get_all_wave_ids
            
            # Get all wave IDs
            all_wave_ids = get_all_wave_ids()
            
            # Get graded readiness report
            readiness_df = generate_wave_readiness_report()
            
            # Count by graded status
            full_count = (readiness_df['readiness_status'] == 'full').sum()
            partial_count = (readiness_df['readiness_status'] == 'partial').sum()
            operational_count = (readiness_df['readiness_status'] == 'operational').sum()
            unavailable_count = (readiness_df['readiness_status'] == 'unavailable').sum()
            usable_count = full_count + partial_count + operational_count
            
            # Summary metrics
            col1, col2, col3, col4, col5 = st.columns(5)
            with col1:
                st.metric(
                    "üü¢ Full",
                    full_count,
                    delta=f"{full_count/len(all_wave_ids)*100:.0f}%",
                    help="Complete data for all analytics"
                )
            with col2:
                st.metric(
                    "üü° Partial",
                    partial_count,
                    delta=f"{partial_count/len(all_wave_ids)*100:.0f}%",
                    help="Basic analytics available"
                )
            with col3:
                st.metric(
                    "üü† Operational",
                    operational_count,
                    delta=f"{operational_count/len(all_wave_ids)*100:.0f}%",
                    help="Current state display"
                )
            with col4:
                st.metric(
                    "üî¥ Unavailable",
                    unavailable_count,
                    delta=f"{unavailable_count/len(all_wave_ids)*100:.0f}%",
                    help="Needs attention"
                )
            with col5:
                st.metric(
                    "‚úì Usable",
                    usable_count,
                    delta=f"{usable_count/len(all_wave_ids)*100:.0f}%",
                    help="Operational or better",
                    delta_color="normal"
                )
            
            st.divider()
            
            # Build detailed status table with graded readiness
            detailed_status_data = []
            
            for wave_id in sorted(all_wave_ids):
                diagnostics = compute_data_ready_status(wave_id)
                
                # Get graded status
                readiness_status = diagnostics['readiness_status']
                
                # Map status to display
                status_icons = {
                    'full': 'üü¢',
                    'partial': 'üü°',
                    'operational': 'üü†',
                    'unavailable': 'üî¥'
                }
                status_labels = {
                    'full': 'Full',
                    'partial': 'Partial',
                    'operational': 'Operational',
                    'unavailable': 'Unavailable'
                }
                
                status_icon = status_icons.get(readiness_status, '‚ùì')
                status_label = status_labels.get(readiness_status, 'Unknown')
                status_display = f"{status_icon} {status_label}"
                
                # Get allowed analytics
                allowed = diagnostics['allowed_analytics']
                analytics_summary = []
                if allowed.get('current_pricing'):
                    analytics_summary.append("Pricing")
                if allowed.get('simple_returns'):
                    analytics_summary.append("Returns")
                if allowed.get('volatility_metrics'):
                    analytics_summary.append("Volatility")
                if allowed.get('multi_window_returns'):
                    analytics_summary.append("Multi-Window")
                if allowed.get('alpha_attribution'):
                    analytics_summary.append("Alpha")
                
                analytics_display = ', '.join(analytics_summary) if analytics_summary else 'None'
                
                # Get issues
                blocking = ', '.join(diagnostics['blocking_issues']) if diagnostics['blocking_issues'] else ''
                informational = ', '.join(diagnostics['informational_issues'][:2]) if diagnostics['informational_issues'] else ''
                
                # Format missing tickers for display
                missing_tickers_display = ', '.join(diagnostics['missing_tickers'][:3])
                if len(diagnostics['missing_tickers']) > 3:
                    missing_tickers_display += f" (+{len(diagnostics['missing_tickers']) - 3} more)"
                
                # Get coverage percentage
                coverage_pct = diagnostics.get('coverage_pct', 0.0)
                
                # Get suggested actions
                actions = diagnostics.get('suggested_actions', [])
                action_display = actions[0] if actions else ''
                
                detailed_status_data.append({
                    'Wave ID': wave_id,
                    'Display Name': diagnostics['display_name'],
                    'Status': status_display,
                    'Allowed Analytics': analytics_display,
                    'Coverage %': f"{coverage_pct:.1f}%",
                    'Blocking Issues': blocking or 'None',
                    'Limitations': informational or 'None',
                    'Missing Tickers': missing_tickers_display or 'None',
                    'Suggested Action': action_display or 'No action needed',
                    'Summary': diagnostics.get('readiness_summary', diagnostics.get('details', ''))
                })
            
            # Display detailed status table
            if detailed_status_data:
                df_detailed_status = pd.DataFrame(detailed_status_data)
                
                # Add filtering option
                filter_option = st.selectbox(
                    "Filter by Status:",
                    options=["All", "Full", "Partial", "Operational", "Unavailable"],
                    index=0,
                    help="Filter the table by graded readiness status"
                )
                
                # Apply filter
                if filter_option != "All":
                    status_icons_map = {
                        'Full': 'üü¢',
                        'Partial': 'üü°',
                        'Operational': 'üü†',
                        'Unavailable': 'üî¥'
                    }
                    icon = status_icons_map.get(filter_option, '')
                    if icon:
                        df_detailed_status = df_detailed_status[
                            df_detailed_status['Status'].str.contains(icon, case=False, na=False)
                        ]
                
                st.dataframe(
                    df_detailed_status,
                    use_container_width=True,
                    hide_index=True,
                    height=500
                )
                
                # Export options
                col1, col2 = st.columns(2)
                with col1:
                    # Download CSV
                    csv = df_detailed_status.to_csv(index=False)
                    st.download_button(
                        label="üì• Download Graded Readiness Report as CSV",
                        data=csv,
                        file_name=f"wave_readiness_graded_{datetime.now().strftime('%Y%m%d')}.csv",
                        mime="text/csv"
                    )
                
                with col2:
                    # Download full report
                    csv_full = readiness_df.to_csv(index=False)
                    st.download_button(
                        label="üì• Download Full Readiness Data as CSV",
                        data=csv_full,
                        file_name=f"wave_readiness_full_{datetime.now().strftime('%Y%m%d')}.csv",
                        mime="text/csv"
                    )
                
                # Individual wave detailed viewer
                st.markdown("---")
                st.subheader("üîç Individual Wave Diagnostics Viewer")
                
                selected_wave = st.selectbox(
                    "Select a wave to view detailed diagnostics:",
                    options=sorted(all_wave_ids),
                    help="Choose a wave to see complete diagnostic information"
                )
                
                if selected_wave:
                    wave_diagnostics = compute_data_ready_status(selected_wave)
                    
                    # Display in expandable sections
                    with st.expander(f"üìã Full Diagnostics for {selected_wave}", expanded=True):
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.markdown("**Basic Information**")
                            st.text(f"Wave ID: {wave_diagnostics['wave_id']}")
                            st.text(f"Display Name: {wave_diagnostics['display_name']}")
                            st.text(f"Ready: {wave_diagnostics['is_ready']}")
                            st.text(f"Primary Reason: {wave_diagnostics['reason']}")
                            st.text(f"Source: {wave_diagnostics['source_used']}")
                            
                            st.markdown("**Checks**")
                            for check_name, passed in wave_diagnostics['checks'].items():
                                icon = "‚úì" if passed else "‚úó"
                                st.text(f"{icon} {check_name}: {passed}")
                        
                        with col2:
                            st.markdown("**Reason Codes**")
                            st.text(', '.join(wave_diagnostics['reason_codes']))
                            
                            st.markdown("**History Window**")
                            st.text(f"Start: {wave_diagnostics['history_window_used']['start'] or 'N/A'}")
                            st.text(f"End: {wave_diagnostics['history_window_used']['end'] or 'N/A'}")
                            
                            if wave_diagnostics['missing_dates']['earliest']:
                                st.markdown("**Missing Dates**")
                                st.text(f"Earliest: {wave_diagnostics['missing_dates']['earliest']}")
                                st.text(f"Latest: {wave_diagnostics['missing_dates']['latest']}")
                        
                        st.markdown("**Details**")
                        st.info(wave_diagnostics['details'])
                        
                        if wave_diagnostics['missing_tickers']:
                            st.markdown("**Missing Tickers**")
                            st.warning(f"{len(wave_diagnostics['missing_tickers'])} ticker(s): {', '.join(wave_diagnostics['missing_tickers'])}")
                        
                        if wave_diagnostics['missing_benchmark_tickers']:
                            st.markdown("**Missing Benchmark Tickers**")
                            st.warning(f"{len(wave_diagnostics['missing_benchmark_tickers'])} ticker(s): {', '.join(wave_diagnostics['missing_benchmark_tickers'])}")
                        
                        if wave_diagnostics['exception']:
                            st.markdown("**Exception**")
                            st.error(wave_diagnostics['exception'])
                        
                        # Copy JSON to clipboard
                        st.markdown("**Raw JSON**")
                        wave_json = json.dumps(wave_diagnostics, indent=2)
                        st.code(wave_json, language='json')
                        st.download_button(
                            label="üìã Copy Diagnostics as JSON",
                            data=wave_json,
                            file_name=f"{selected_wave}_diagnostics.json",
                            mime="application/json",
                            key=f"download_{selected_wave}"
                        )
            else:
                st.info("üìä No wave status information available")
                
        except ImportError as e:
            # Fallback to legacy diagnostics if analytics_pipeline not available
            st.warning("‚ö†Ô∏è Advanced diagnostics unavailable. Using legacy status checks.")
            
            # Compute wave diagnostics to get data readiness (legacy)
            wave_diagnostics = compute_wave_universe_diagnostics()
            wave_statuses = wave_diagnostics.get('wave_statuses', {})
            
            # Build status table (legacy)
            status_data = []
            for wave_name in sorted(all_metrics, key=lambda x: x['wave_name']):
                wave = wave_name['wave_name']
                status_info = wave_statuses.get(wave, {'status': 'Unknown', 'reason': 'N/A'})
                status = status_info['status']
                reason = status_info['reason']
                
                # Map status to display format (handle both 'Ready' and 'ready')
                status_lower = status.lower() if isinstance(status, str) else 'unknown'
                if status_lower == 'ready':
                    status_display = "‚úÖ Ready"
                elif 'degraded' in status_lower:
                    status_display = "‚ö†Ô∏è Degraded"
                else:
                    status_display = "‚ùå Missing Inputs"
                
                status_data.append({
                    'Wave Name': wave,
                    'Status': status_display,
                    'Details': reason if reason else 'Data available'
                })
            
            if status_data:
                df_status = pd.DataFrame(status_data)
                st.dataframe(
                    df_status,
                    use_container_width=True,
                    hide_index=True,
                    height=400
                )
            else:
                st.info("üìä No wave status information available")
        
        st.divider()
        
        # ========================================================================
        # SECTION C: All-Waves Performance & Alpha Grid
        # ========================================================================
        st.markdown("### üìà All-Waves Performance & Alpha Grid")
        st.caption("Multi-timeframe performance metrics for all waves")
        
        # Build comprehensive grid data
        grid_data = []
        for metrics in all_metrics:
            wave_name = metrics['wave_name']
            wave_universe_version = st.session_state.get("wave_universe_version", 1)
            
            # Helper to get returns for a timeframe
            def get_returns(days):
                wave_data = get_wave_data_filtered(
                    wave_name=wave_name,
                    days=days,
                    _wave_universe_version=wave_universe_version
                )
                if wave_data is not None and len(wave_data) > 0:
                    wave_ret = wave_data['portfolio_return'].sum() if 'portfolio_return' in wave_data.columns else None
                    bench_ret = wave_data['benchmark_return'].sum() if 'benchmark_return' in wave_data.columns else None
                    return wave_ret, bench_ret
                return None, None
            
            # Get returns for each timeframe
            wave_1d, bench_1d = get_returns(1)
            wave_30d, bench_30d = get_returns(30)
            wave_60d, bench_60d = get_returns(60)
            wave_365d, bench_365d = get_returns(365)
            
            # Calculate alphas (handle None values)
            alpha_1d = (wave_1d - bench_1d) if (wave_1d is not None and bench_1d is not None) else None
            alpha_30d = (wave_30d - bench_30d) if (wave_30d is not None and bench_30d is not None) else None
            alpha_60d = (wave_60d - bench_60d) if (wave_60d is not None and bench_60d is not None) else None
            alpha_365d = (wave_365d - bench_365d) if (wave_365d is not None and bench_365d is not None) else None
            
            grid_data.append({
                'Wave Name': wave_name,
                '1D Wave': wave_1d,
                '1D Benchmark': bench_1d,
                '1D Alpha': alpha_1d,
                '30D Wave': wave_30d,
                '30D Benchmark': bench_30d,
                '30D Alpha': alpha_30d,
                '60D Wave': wave_60d,
                '60D Benchmark': bench_60d,
                '60D Alpha': alpha_60d,
                '365D Wave': wave_365d,
                '365D Benchmark': bench_365d,
                '365D Alpha': alpha_365d,
                '_sort_alpha_30d': alpha_30d  # Hidden column for sorting
            })
        
        if grid_data:
            df_grid = pd.DataFrame(grid_data)
            
            # Sort by 30D Alpha (descending) - put None values at the end
            df_grid = df_grid.sort_values('_sort_alpha_30d', ascending=False, na_position='last')
            
            # Format percentages for display - handle None values
            percent_cols = [
                '1D Wave', '1D Benchmark', '1D Alpha',
                '30D Wave', '30D Benchmark', '30D Alpha',
                '60D Wave', '60D Benchmark', '60D Alpha',
                '365D Wave', '365D Benchmark', '365D Alpha'
            ]
            
            df_display = df_grid.copy()
            for col in percent_cols:
                df_display[col] = df_display[col].apply(lambda x: f"{x*100:.2f}%" if x is not None else "N/A")
            
            # Drop the sort column
            df_display = df_display.drop(columns=['_sort_alpha_30d'])
            
            # Apply color styling to alpha columns
            def color_alpha(val):
                """Apply green color to positive values, red to negative."""
                try:
                    # Handle N/A values
                    if val == "N/A":
                        return 'background-color: #f0f0f0; color: #666666'  # Light gray for missing data
                    # Extract numeric value from percentage string
                    num_val = float(val.strip('%'))
                    if num_val > 0:
                        return 'background-color: #d4edda; color: #155724'  # Light green background, dark green text
                    elif num_val < 0:
                        return 'background-color: #f8d7da; color: #721c24'  # Light red background, dark red text
                    else:
                        return ''
                except:
                    return ''
            
            # Style the dataframe
            alpha_cols = ['1D Alpha', '30D Alpha', '60D Alpha', '365D Alpha']
            styled_df = df_display.style.applymap(color_alpha, subset=alpha_cols)
            
            # Display with horizontal scroll for mobile
            st.dataframe(
                styled_df,
                use_container_width=True,
                height=600,
                hide_index=True
            )
            
            st.caption(f"‚úì Showing {len(df_grid)} waves | Sorted by 30D Alpha (descending) | Green = positive alpha, Red = negative alpha")
        else:
            st.info("üìä No wave data available for grid")
        
        # ========================================================================
        # SECTION D: System-Level Narratives (Optional Expander)
        # ========================================================================
        with st.expander("üìñ System Intelligence"):
            st.markdown("""
            **Market Context:**
            - System-wide intelligence based on aggregate wave performance
            - Rankings and attribution across all waves
            
            **Action Guidance:**
            - Executive-level insights for portfolio positioning
            - Risk-on/risk-off signals from system behavior
            
            **Methodology:**
            - Alpha = Wave Return ‚àí Benchmark Return
            - All metrics derive from canonical wave_history.csv dataset
            """)
    
    except Exception as e:
        st.error("‚ö†Ô∏è **System View Error**")
        st.warning(f"An error occurred: {str(e)}")
        with st.expander("üîç Technical Details"):
            st.code(f"Error: {str(e)}\n\n{traceback.format_exc()}", language="python")


def render_strategy_state_panel(wave_name: str, mode: str = "Standard"):
    """
    Render the Strategy State panel showing current strategy positioning.
    
    v17.4 Feature: Displays regime, exposure, safe allocation, and trigger reasons.
    
    Args:
        wave_name: Name of the wave
        mode: Operating mode (Standard, Alpha-Minus-Beta, Private Logic)
    """
    # Display thresholds for visual indicators
    EXPOSURE_HIGH_THRESHOLD = 1.05  # Above this shows aggressive icon
    EXPOSURE_LOW_THRESHOLD = 0.95   # Below this shows defensive icon
    SAFE_HIGH_THRESHOLD = 0.30      # Above this shows heavy defense icon
    SAFE_MEDIUM_THRESHOLD = 0.10    # Above this shows moderate defense icon
    
    try:
        st.markdown("#### üéØ Strategy State")
        st.caption("Current strategy positioning and trigger reasons")
        
        # Get strategy state from waves_engine
        strategy_state_result = None
        if WAVES_ENGINE_AVAILABLE:
            try:
                from waves_engine import get_latest_strategy_state
                strategy_state_result = get_latest_strategy_state(wave_name, mode, days=30)
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Unable to load strategy state: {str(e)}")
                return
        else:
            st.warning("‚ö†Ô∏è Waves engine not available - strategy state cannot be displayed")
            return
        
        if not strategy_state_result or not strategy_state_result.get("ok"):
            st.info("üìä Strategy state data not available for this wave")
            return
        
        strategy_state = strategy_state_result.get("strategy_state", {})
        
        if not strategy_state:
            st.info("üìä No strategy state data available")
            return
        
        # Display key metrics in columns
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            regime = strategy_state.get("regime", "n/a")
            regime_icon = {
                "uptrend": "üìà",
                "neutral": "‚û°Ô∏è",
                "downtrend": "üìâ",
                "panic": "‚ö†Ô∏è",
                "cash": "üíµ"
            }.get(regime.lower(), "‚ùì")
            
            st.metric(
                label="Regime",
                value=f"{regime_icon} {regime.title()}",
                help="Current market regime"
            )
        
        with col2:
            vix_regime = strategy_state.get("vix_regime", "n/a")
            vix_level = strategy_state.get("vix_level")
            
            vix_icon = {
                "low": "üòå",
                "normal": "üòê",
                "elevated": "üòü",
                "high": "üò®"
            }.get(vix_regime.lower(), "‚ùì")
            
            if vix_level is not None:
                vix_display = f"{vix_icon} {vix_regime.title()} ({vix_level:.1f})"
            else:
                vix_display = f"{vix_icon} {vix_regime.title()}"
            
            st.metric(
                label="VIX Regime",
                value=vix_display,
                help="Volatility regime (VIX level if available)"
            )
        
        with col3:
            exposure = strategy_state.get("exposure", 0.0)
            exposure_pct = exposure * 100
            
            # Color code exposure using defined thresholds
            if exposure > EXPOSURE_HIGH_THRESHOLD:
                exposure_delta = f"+{(exposure - 1.0) * 100:.1f}%"
                exposure_icon = "üöÄ"
            elif exposure < EXPOSURE_LOW_THRESHOLD:
                exposure_delta = f"{(exposure - 1.0) * 100:.1f}%"
                exposure_icon = "üõ°Ô∏è"
            else:
                exposure_delta = None
                exposure_icon = "‚öñÔ∏è"
            
            st.metric(
                label="Exposure",
                value=f"{exposure_icon} {exposure_pct:.1f}%",
                delta=exposure_delta,
                help="Current market exposure multiplier"
            )
        
        with col4:
            safe_allocation = strategy_state.get("safe_allocation", 0.0)
            safe_pct = safe_allocation * 100
            
            # Icon selection using defined thresholds
            if safe_allocation > SAFE_HIGH_THRESHOLD:
                safe_icon = "üõ°Ô∏è"
            elif safe_allocation > SAFE_MEDIUM_THRESHOLD:
                safe_icon = "üîí"
            else:
                safe_icon = "üíº"
            
            st.metric(
                label="Safe Allocation",
                value=f"{safe_icon} {safe_pct:.1f}%",
                help="Percentage allocated to safe assets (cash, treasuries)"
            )
        
        # Display trigger reasons
        trigger_reasons = strategy_state.get("trigger_reasons", [])
        
        if trigger_reasons:
            st.markdown("**üîî Active Triggers:**")
            
            # Display reasons in a clean, bulleted format
            for reason in trigger_reasons:
                st.markdown(f"- {reason}")
        else:
            st.markdown("**üîî Active Triggers:** None (standard positioning)")
        
        # Additional metadata in expander
        with st.expander("üìã Strategy Details"):
            st.markdown("**Metadata:**")
            
            strategy_family = strategy_state.get("strategy_family", "unknown")
            risk_state = strategy_state.get("aggregated_risk_state", "neutral")
            active_strategies = strategy_state.get("active_strategies", 0)
            timestamp = strategy_state.get("timestamp", "unknown")
            
            col_a, col_b = st.columns(2)
            
            with col_a:
                st.markdown(f"- **Strategy Family:** {strategy_family}")
                st.markdown(f"- **Risk State:** {risk_state}")
            
            with col_b:
                st.markdown(f"- **Active Strategies:** {active_strategies}")
                st.markdown(f"- **Timestamp:** {timestamp}")
            
            # NEW: Display strategy_stack if available
            try:
                from helpers.wave_registry import get_wave_by_id
                from waves_engine import get_wave_id_from_display_name
                
                wave_id = get_wave_id_from_display_name(wave_name)
                if wave_id:
                    wave_info = get_wave_by_id(wave_id)
                    if wave_info:
                        strategy_stack = wave_info.get("strategy_stack", "")
                        strategy_stack_applied = bool(strategy_stack and strategy_stack.strip())
                        
                        st.markdown("---")
                        st.markdown("**Strategy Pipeline:**")
                        
                        if strategy_stack_applied:
                            st.markdown(f"- **Strategy-Aware Pipeline:** ‚úÖ **Active**")
                            st.markdown(f"- **Strategy Components:** {strategy_stack}")
                            
                            # Parse and display individual components
                            components = [c.strip() for c in strategy_stack.split(',') if c.strip()]
                            if components:
                                st.markdown("- **Active Components:**")
                                component_display = {
                                    "momentum": "üìä Momentum signal adjustments",
                                    "trend_confirmation": "üìà Trend confirmation overlays",
                                    "volatility_targeting": "üéØ Volatility targeting",
                                    "relative_strength": "üí™ Relative strength strategies",
                                    "regime_detection": "üîç Regime detection",
                                    "vix_overlay": "‚ö° VIX overlay adjustments"
                                }
                                for comp in components:
                                    display = component_display.get(comp, f"‚Ä¢ {comp}")
                                    st.markdown(f"  - {display}")
                        else:
                            st.markdown(f"- **Strategy-Aware Pipeline:** ‚ö™ Not Applied")
                            st.markdown("  - This wave uses basic return calculation without strategy overlays")
            except (ImportError, KeyError, AttributeError) as e:
                # Expected errors if modules not available or wave not found
                pass
            except Exception as e:
                # Log unexpected errors for debugging
                logger.warning(f"Error loading strategy_stack info: {e}")
        
    except Exception as e:
        st.error(f"‚ö†Ô∏è Error rendering strategy state panel: {str(e)}")
        with st.expander("üîç Technical Details"):
            st.code(f"Error: {str(e)}\n\n{traceback.format_exc()}", language="python")


def render_individual_wave_view(selected_wave, all_metrics):
    """
    Render the Individual Wave View - Wave-specific intelligence.
    
    Shows:
    - Wave-specific metrics and performance
    - Alpha drivers breakdown for the selected wave
    - Wave-scoped narratives and action guidance
    
    Maintains executive-level presentation without raw diagnostics.
    """
    try:
        st.markdown(f"### üåä {selected_wave}")
        st.caption(f"Executive intelligence for {selected_wave}")
        
        # Get wave_universe_version from session state
        wave_universe_version = st.session_state.get("wave_universe_version", 1)
        
        # ========================================================================
        # SECTION A: Wave Performance Metrics
        # ========================================================================
        st.markdown("#### üìä Performance Metrics")
        
        # Get metrics for the selected wave
        wave_metrics = next((m for m in all_metrics if m['wave_name'] == selected_wave), None)
        
        if wave_metrics:
            # Display multi-timeframe metrics in columns
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                alpha_1d = wave_metrics.get('alpha_1d', 0.0)
                st.metric(
                    label="1D Alpha",
                    value=f"{alpha_1d*100:.2f}%",
                    help="1-day alpha vs benchmark"
                )
            
            with col2:
                alpha_30d = wave_metrics.get('alpha_30d', 0.0)
                st.metric(
                    label="30D Alpha",
                    value=f"{alpha_30d*100:.2f}%",
                    help="30-day alpha vs benchmark"
                )
            
            with col3:
                alpha_60d = wave_metrics.get('alpha_60d', 0.0)
                st.metric(
                    label="60D Alpha",
                    value=f"{alpha_60d*100:.2f}%",
                    help="60-day alpha vs benchmark"
                )
            
            with col4:
                alpha_365d = wave_metrics.get('alpha_365d', 0.0)
                st.metric(
                    label="365D Alpha",
                    value=f"{alpha_365d*100:.2f}%",
                    help="365-day alpha vs benchmark"
                )
        else:
            st.info(f"üìä No performance data available for {selected_wave}")
        
        st.divider()
        
        # ========================================================================
        # SECTION A1.5: Strategy State Panel (v17.4)
        # ========================================================================
        render_strategy_state_panel(selected_wave, mode=st.session_state.get("mode", "Standard"))
        
        st.divider()
        
        # ========================================================================
        # SECTION A2: Executive Summary - Alpha Attribution
        # ========================================================================
        # NOTE: Attribution infrastructure is wired for:
        #   - S&P 500 Wave: ENABLED (displays in UI)
        #   - AI & Cloud MegaCap Wave: WIRED (internal only, gated from UI)
        # 
        # AI & Cloud MegaCap Wave uses same return ledger and attribution pipeline
        # with static ETF proxy benchmark: 60% QQQ, 25% SMH, 15% IGV
        # Attribution categories: Exposure & Timing, Regime & VIX Overlay (inactive),
        # Momentum & Trend, Volatility & Risk Control, Asset Selection
        # ========================================================================
        st.markdown("#### üìã Executive Summary")
        
        # Check if this is the S&P 500 Wave
        # NOTE: Add "AI & Cloud MegaCap Wave" to this condition to enable UI display
        if selected_wave == "S&P 500 Wave":
            # Display alpha attribution for S&P 500 Wave
            try:
                # Load wave history data
                wave_df = safe_load_wave_history()
                
                if wave_df is not None and not wave_df.empty and 'wave' in wave_df.columns:
                    # Filter data for S&P 500 Wave
                    sp500_data = wave_df[wave_df['wave'] == "S&P 500 Wave"].copy()
                    
                    if not sp500_data.empty and len(sp500_data) >= ATTRIBUTION_TIMEFRAME_DAYS:
                        # Sort by date and take last N days
                        sp500_data = sp500_data.sort_values('date').tail(ATTRIBUTION_TIMEFRAME_DAYS)
                        
                        # Prepare history DataFrame for attribution
                        sp500_data = sp500_data.set_index('date')
                        history_df = pd.DataFrame({
                            'wave_ret': sp500_data['portfolio_return'],
                            'bm_ret': sp500_data['benchmark_return']
                        })
                        
                        # Compute attribution using the alpha_attribution module
                        if ALPHA_ATTRIBUTION_AVAILABLE:
                            with st.spinner("Computing S&P 500 Wave attribution..."):
                                daily_df, summary = compute_alpha_attribution_series(
                                    wave_name="S&P 500 Wave",
                                    mode="Standard",
                                    history_df=history_df,
                                    diagnostics_df=None,
                                    tilt_strength=ATTRIBUTION_TILT_STRENGTH,
                                    base_exposure=ATTRIBUTION_BASE_EXPOSURE
                                )
                            
                            # Display attribution summary in a clean format
                            st.markdown(f"**Alpha Attribution ({ATTRIBUTION_TIMEFRAME_DAYS}-Day Period)**")
                            
                            # Summary metrics
                            col_sum1, col_sum2, col_sum3 = st.columns(3)
                            
                            with col_sum1:
                                st.metric(
                                    "Total Wave Return",
                                    f"{summary.total_wave_return * 100:+.2f}%",
                                    help="S&P 500 Wave return over 30 days"
                                )
                            
                            with col_sum2:
                                st.metric(
                                    "Total Benchmark Return",
                                    f"{summary.total_benchmark_return * 100:+.2f}%",
                                    help="Benchmark return over 30 days"
                                )
                            
                            with col_sum3:
                                st.metric(
                                    "Total Alpha",
                                    f"{summary.total_alpha * 100:+.2f}%",
                                    help="Wave Return - Benchmark Return"
                                )
                            
                            # Attribution components table
                            st.markdown("**Attribution Breakdown:**")
                            
                            attribution_data = {
                                "Component": [
                                    "1Ô∏è‚É£ Exposure & Timing Alpha",
                                    "2Ô∏è‚É£ Regime & VIX Overlay Alpha",
                                    "3Ô∏è‚É£ Momentum & Trend Alpha",
                                    "4Ô∏è‚É£ Volatility & Risk Control Alpha",
                                    "5Ô∏è‚É£ Asset Selection Alpha"
                                ],
                                "Contribution": [
                                    f"{summary.exposure_timing_alpha * 100:+.2f}%",
                                    f"{summary.regime_vix_alpha * 100:+.2f}%",
                                    f"{summary.momentum_trend_alpha * 100:+.2f}%",
                                    f"{summary.volatility_control_alpha * 100:+.2f}%",
                                    f"{summary.asset_selection_alpha * 100:+.2f}%"
                                ],
                                "Share of Alpha": [
                                    f"{summary.exposure_timing_contribution_pct:+.1f}%",
                                    f"{summary.regime_vix_contribution_pct:+.1f}%",
                                    f"{summary.momentum_trend_contribution_pct:+.1f}%",
                                    f"{summary.volatility_control_contribution_pct:+.1f}%",
                                    f"{summary.asset_selection_contribution_pct:+.1f}%"
                                ]
                            }
                            
                            df_attribution = pd.DataFrame(attribution_data)
                            st.dataframe(
                                df_attribution,
                                use_container_width=True,
                                hide_index=True
                            )
                            
                            # Reconciliation note
                            st.caption(f"‚úì Reconciliation: {summary.reconciliation_pct_error:.4f}% error (target: <0.01%)")
                            
                        else:
                            st.warning("‚ö†Ô∏è Alpha attribution module not available")
                    else:
                        st.info(f"üìä Insufficient data for S&P 500 Wave attribution (minimum {ATTRIBUTION_TIMEFRAME_DAYS} days required)")
                else:
                    st.info("üìä Wave history data not available")
                    
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Unable to compute attribution: {str(e)}")
        else:
            # Placeholder for other waves
            st.info("üìä **Attribution Rollout Pending**")
            st.caption(f"Detailed alpha attribution for {selected_wave} is currently in development. Full attribution analysis will be available in an upcoming release.")
        
        st.divider()
        
        # ========================================================================
        # SECTION B: Alpha Drivers Breakdown
        # ========================================================================
        st.markdown("#### üéØ Alpha Drivers Breakdown")
        st.caption("Performance attribution for this wave")
        
        # Create columns for timeframe selector and breakdown
        col_controls, col_breakdown = st.columns([3, 7])
        
        with col_controls:
            # Timeframe selector
            timeframe_options = {
                "1 Day": 1,
                "30 Days": 30,
                "60 Days": 60,
                "365 Days": 365
            }
            
            selected_timeframe_label = st.selectbox(
                "Select Timeframe:",
                options=list(timeframe_options.keys()),
                index=1,  # Default to 30 Days
                key="individual_wave_drivers_timeframe"
            )
            
            timeframe_days = timeframe_options[selected_timeframe_label]
        
        with col_breakdown:
            # Compute Alpha Drivers for selected wave
            with st.spinner(f"Computing alpha drivers for {selected_wave}..."):
                drivers = compute_alpha_drivers(
                    wave_name=selected_wave,
                    timeframe_days=timeframe_days
                )
            
            # Display summary metrics
            st.markdown(f"**{selected_wave} ‚Äî {selected_timeframe_label}**")
            
            col_ret1, col_ret2, col_ret3 = st.columns(3)
            
            with col_ret1:
                st.metric(
                    label="Wave Return",
                    value=f"{drivers['wave_return']*100:.2f}%",
                    help="Total wave return over the period"
                )
            
            with col_ret2:
                st.metric(
                    label="Benchmark Return",
                    value=f"{drivers['benchmark_return']*100:.2f}%",
                    help="Total benchmark return over the period"
                )
            
            with col_ret3:
                st.metric(
                    label="Total Alpha",
                    value=f"{drivers['total_alpha']*100:.2f}%",
                    help="Wave Return minus Benchmark Return"
                )
            
            st.markdown("---")
            
            # Display Alpha Drivers Breakdown
            st.markdown("**Alpha Drivers (% Contribution)**")
            
            # Check if we can display percentages
            if drivers['selection_percent'] is not None:
                # Build drivers table
                drivers_table_data = [
                    {
                        'Driver': 'üìà Stock Selection',
                        'Contribution (pts)': f"{drivers['selection_contribution']*100:.2f}%",
                        'Share of Alpha': f"{drivers['selection_percent']:.1f}%"
                    },
                    {
                        'Driver': 'üõ°Ô∏è Risk Overlay',
                        'Contribution (pts)': f"{drivers['overlay_contribution']*100:.2f}%",
                        'Share of Alpha': f"{drivers['overlay_percent']:.1f}%"
                    },
                    {
                        'Driver': '‚ö™ Residual/Other',
                        'Contribution (pts)': f"{drivers['residual_contribution']*100:.2f}%",
                        'Share of Alpha': f"{drivers['residual_percent']:.1f}%"
                    }
                ]
                
                df_drivers = pd.DataFrame(drivers_table_data)
                
                st.dataframe(
                    df_drivers,
                    use_container_width=True,
                    hide_index=True
                )
                
                # Verification that shares sum to ~100%
                total_share = drivers['selection_percent'] + drivers['overlay_percent'] + drivers['residual_percent']
                st.caption(f"‚úì Total: {total_share:.1f}% (should be ~100%)")
                
            else:
                # Total alpha is effectively zero - display N/A
                st.info("üìä Total Alpha is near zero. Share percentages: N/A")
                
                # Still show contributions in return points
                drivers_table_data = [
                    {
                        'Driver': 'üìà Stock Selection',
                        'Contribution (pts)': f"{drivers['selection_contribution']*100:.2f}%",
                        'Share of Alpha': 'N/A'
                    },
                    {
                        'Driver': 'üõ°Ô∏è Risk Overlay',
                        'Contribution (pts)': f"{drivers['overlay_contribution']*100:.2f}%",
                        'Share of Alpha': 'N/A'
                    },
                    {
                        'Driver': '‚ö™ Residual/Other',
                        'Contribution (pts)': f"{drivers['residual_contribution']*100:.2f}%",
                        'Share of Alpha': 'N/A'
                    }
                ]
                
                df_drivers = pd.DataFrame(drivers_table_data)
                
                st.dataframe(
                    df_drivers,
                    use_container_width=True,
                    hide_index=True
                )
        
        # ========================================================================
        # SECTION C: Wave-Level Narratives (Optional Expander)
        # ========================================================================
        with st.expander("üìñ Wave Intelligence"):
            st.markdown(f"""
            **{selected_wave} Context:**
            - Wave-specific performance metrics and attribution
            - Executive-level insights scoped to this wave
            
            **Action Guidance:**
            - Position sizing and risk management for this wave
            - Alpha driver analysis for decision support
            
            **Methodology:**
            - Alpha = Wave Return ‚àí Benchmark Return
            - **Stock Selection**: Alpha from holding and weighting decisions
            - **Risk Overlay**: Alpha from dynamic exposure and VIX-based overlays
            - **Residual/Other**: Compounding effects and timing interactions
            """)
    
    except Exception as e:
        st.error(f"‚ö†Ô∏è **Wave View Error: {selected_wave}**")
        st.warning(f"An error occurred: {str(e)}")
        with st.expander("üîç Technical Details"):
            st.code(f"Error: {str(e)}\n\n{traceback.format_exc()}", language="python")


def render_details_tab():
    """Render the Details tab with proper diagnostics."""
    st.header("Details")
    st.write("Detailed analytics and metrics for individual waves.")
    
    # Load wave history - same unified DataFrame as used in Executive tab
    df = safe_load_wave_history()
    
    # Check if data is loaded
    if df is None:
        st.info("History not loaded")
        return
    
    # Get available waves
    waves = get_available_waves()
    
    if len(waves) == 0:
        st.warning("No waves available")
        return
    
    # Wave selector - canonical selected_wave_id
    selected_wave_id = st.selectbox(
        "Select Wave for Details",
        options=waves,
        key="details_wave_selector",
        help="Choose a wave to view detailed analytics"
    )
    
    if selected_wave_id:
        # Filter data for the selected wave
        if 'wave' in df.columns:
            wave_df = df[df['wave'] == selected_wave_id].copy()
        else:
            st.error("Missing column: 'wave'")
            return
        
        # Check if filtered data has rows
        if len(wave_df) == 0:
            st.warning(f"No rows for wave_id={selected_wave_id}")
            return
        
        # Check for required columns
        required_columns = ['date', 'portfolio_return', 'benchmark_return']
        missing_columns = [col for col in required_columns if col not in wave_df.columns]
        
        if missing_columns:
            st.error(f"Missing columns: {', '.join(missing_columns)}")
            return
        
        # Display wave details
        st.success(f"Showing details for: **{selected_wave_id}**")
        
        # Show basic metrics
        st.subheader("üìä Wave Metrics")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Total Records", len(wave_df))
        
        with col2:
            if 'date' in wave_df.columns:
                latest_date = wave_df['date'].max()
                st.metric("Latest Date", str(latest_date)[:10])
            else:
                st.metric("Latest Date", "N/A")
        
        with col3:
            if 'portfolio_return' in wave_df.columns:
                total_return = wave_df['portfolio_return'].sum()
                st.metric("Total Return", f"{total_return*100:.2f}%")
            else:
                st.metric("Total Return", "N/A")
        
        with col4:
            if 'portfolio_return' in wave_df.columns and 'benchmark_return' in wave_df.columns:
                total_alpha = (wave_df['portfolio_return'] - wave_df['benchmark_return']).sum()
                st.metric("Total Alpha", f"{total_alpha*100:.2f}%")
            else:
                st.metric("Total Alpha", "N/A")
        
        st.divider()
        
        # Show data table
        st.subheader("üìã Recent History (Last 30 Days)")
        
        display_cols = ['date', 'portfolio_return', 'benchmark_return']
        if 'exposure' in wave_df.columns:
            display_cols.append('exposure')
        if 'vix' in wave_df.columns:
            display_cols.append('vix')
        
        available_display_cols = [col for col in display_cols if col in wave_df.columns]
        
        if available_display_cols:
            recent_data = wave_df.sort_values('date', ascending=False).head(30)[available_display_cols]
            st.dataframe(recent_data, use_container_width=True, hide_index=True)
        else:
            st.warning("No displayable columns available")



def generate_board_pack_html():
    """
    Generate Institutional Board Pack HTML report.
    Returns HTML string with all sections and graceful degradation.
    """
    # Get build info
    git_hash = get_git_commit_hash()
    git_branch = get_git_branch_name()
    deploy_timestamp = get_deploy_timestamp()
    
    # Get Mission Control data
    mc_data = get_mission_control_data()
    
    # Get available waves for leaderboard
    waves = get_available_waves()
    
    # Calculate leaderboard data
    leaderboard_data = []
    if waves:
        for wave_name in waves:
            wave_data = get_wave_data_filtered(wave_name=wave_name, days=30)
            if wave_data is not None and len(wave_data) > 0:
                wavescore = calculate_wavescore(wave_data)
                metrics = calculate_wave_metrics(wave_data)
                leaderboard_data.append({
                    'wave': wave_name,
                    'wavescore': wavescore,
                    'alpha': metrics.get('cumulative_alpha', 'N/A')
                })
        
        # Sort by wavescore
        leaderboard_data.sort(key=lambda x: x['wavescore'] if isinstance(x['wavescore'], (int, float)) else 0, reverse=True)
    
    # Get system alerts
    alerts = get_system_alerts()
    
    # Build HTML
    html = f"""
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Institutional Board Pack - {deploy_timestamp}</title>
        <style>
            body {{
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                line-height: 1.6;
                color: #333;
                max-width: 1200px;
                margin: 0 auto;
                padding: 20px;
                background-color: #f5f5f5;
            }}
            .header {{
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white;
                padding: 30px;
                border-radius: 10px;
                margin-bottom: 30px;
                box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            }}
            .header h1 {{
                margin: 0;
                font-size: 2.5em;
            }}
            .header p {{
                margin: 10px 0 0 0;
                opacity: 0.9;
            }}
            .section {{
                background: white;
                padding: 25px;
                margin-bottom: 25px;
                border-radius: 8px;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            }}
            .section h2 {{
                color: #667eea;
                border-bottom: 2px solid #667eea;
                padding-bottom: 10px;
                margin-top: 0;
            }}
            .metric-grid {{
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
                gap: 15px;
                margin: 20px 0;
            }}
            .metric-card {{
                background: #f8f9fa;
                padding: 15px;
                border-radius: 6px;
                border-left: 4px solid #667eea;
            }}
            .metric-label {{
                font-size: 0.9em;
                color: #666;
                margin-bottom: 5px;
            }}
            .metric-value {{
                font-size: 1.5em;
                font-weight: bold;
                color: #333;
            }}
            table {{
                width: 100%;
                border-collapse: collapse;
                margin: 15px 0;
            }}
            th {{
                background: #667eea;
                color: white;
                padding: 12px;
                text-align: left;
            }}
            td {{
                padding: 10px 12px;
                border-bottom: 1px solid #ddd;
            }}
            tr:hover {{
                background: #f5f5f5;
            }}
            .alert {{
                padding: 12px 15px;
                margin: 10px 0;
                border-radius: 6px;
                border-left: 4px solid;
            }}
            .alert-error {{
                background: #fee;
                border-color: #c33;
                color: #c33;
            }}
            .alert-warning {{
                background: #ffc;
                border-color: #f90;
                color: #f90;
            }}
            .alert-success {{
                background: #efe;
                border-color: #3c3;
                color: #3c3;
            }}
            .alert-info {{
                background: #eef;
                border-color: #39c;
                color: #39c;
            }}
            .unavailable {{
                color: #999;
                font-style: italic;
                padding: 20px;
                text-align: center;
                background: #f9f9f9;
                border-radius: 6px;
            }}
            .build-info {{
                background: #f8f9fa;
                padding: 15px;
                border-radius: 6px;
                font-family: 'Courier New', monospace;
                font-size: 0.9em;
            }}
            .footer {{
                text-align: center;
                padding: 20px;
                color: #666;
                font-size: 0.9em;
            }}
        </style>
    </head>
    <body>
        <div class="header">
            <h1>üìä Institutional Board Pack</h1>
            <p>Comprehensive Performance & Analytics Report</p>
            <p>Generated: {deploy_timestamp}</p>
        </div>
    """
    
    # Mission Control Section
    html += """
        <div class="section">
            <h2>üéØ Mission Control - Snapshot Summary</h2>
    """
    
    if mc_data:
        html += f"""
            <div class="metric-grid">
                <div class="metric-card">
                    <div class="metric-label">Market Regime</div>
                    <div class="metric-value">{mc_data.get('market_regime', 'unknown')}</div>
                </div>
                <div class="metric-card">
                    <div class="metric-label">VIX Gate Status</div>
                    <div class="metric-value">{mc_data.get('vix_gate_status', 'unknown')}</div>
                </div>
                <div class="metric-card">
                    <div class="metric-label">30-Day Alpha</div>
                    <div class="metric-value">{mc_data.get('alpha_30day', 'unknown')}</div>
                </div>
                <div class="metric-card">
                    <div class="metric-label">Top Wave</div>
                    <div class="metric-value">{mc_data.get('wavescore_leader', 'unknown')}</div>
                </div>
                <div class="metric-card">
                    <div class="metric-label">Total Waves</div>
                    <div class="metric-value">{mc_data.get('total_waves', 0)}</div>
                </div>
                <div class="metric-card">
                    <div class="metric-label">System Status</div>
                    <div class="metric-value">{mc_data.get('system_status', 'unknown')}</div>
                </div>
            </div>
        """
    else:
        html += '<div class="unavailable">Data unavailable</div>'
    
    html += "</div>"
    
    # WaveScore Leaderboard Section
    html += """
        <div class="section">
            <h2>üèÜ WaveScore Leaderboard - Top Performers</h2>
    """
    
    if leaderboard_data and len(leaderboard_data) > 0:
        html += """
            <table>
                <thead>
                    <tr>
                        <th>Rank</th>
                        <th>Wave</th>
                        <th>WaveScore</th>
                        <th>30-Day Alpha</th>
                    </tr>
                </thead>
                <tbody>
        """
        
        for i, item in enumerate(leaderboard_data[:10], 1):  # Top 10
            alpha_str = f"{item['alpha']*100:.2f}%" if isinstance(item['alpha'], (int, float)) else 'N/A'
            html += f"""
                    <tr>
                        <td>{i}</td>
                        <td><strong>{item['wave']}</strong></td>
                        <td>{item['wavescore']:.1f}</td>
                        <td>{alpha_str}</td>
                    </tr>
            """
        
        html += """
                </tbody>
            </table>
        """
    else:
        html += '<div class="unavailable">Data unavailable</div>'
    
    html += "</div>"
    
    # Movers Section
    html += """
        <div class="section">
            <h2>üìà Movers - Biggest Changes</h2>
    """
    
    # Calculate movers (simplified - would need historical data for real implementation)
    if leaderboard_data and len(leaderboard_data) >= 3:
        html += """
            <p>Top performers showing significant movement:</p>
            <ul>
        """
        for item in leaderboard_data[:3]:
            html += f"<li><strong>{item['wave']}</strong> - WaveScore: {item['wavescore']:.1f}</li>"
        html += """
            </ul>
        """
    else:
        html += '<div class="unavailable">Insufficient data for movers analysis</div>'
    
    html += "</div>"
    
    # Alerts Section
    html += """
        <div class="section">
            <h2>üö® System Alerts & Warnings</h2>
    """
    
    if alerts and len(alerts) > 0:
        for alert in alerts:
            severity = alert.get('severity', 'info')
            message = alert.get('message', '')
            
            alert_class = f'alert-{severity}'
            icon = {'error': '‚ùå', 'warning': '‚ö†Ô∏è', 'success': '‚úÖ', 'info': '‚ÑπÔ∏è'}.get(severity, '‚ÑπÔ∏è')
            
            html += f'<div class="alert {alert_class}">{icon} {message}</div>'
    else:
        html += '<div class="unavailable">No alerts at this time</div>'
    
    html += "</div>"
    
    # Alpha Proof Summary Section
    html += """
        <div class="section">
            <h2>üî¨ Alpha Proof Summary - Alpha Metrics</h2>
    """
    
    if waves and len(waves) > 0:
        # Get alpha components for first wave as example
        sample_wave = waves[0]
        wave_data = get_wave_data_filtered(wave_name=sample_wave, days=30)
        
        if wave_data is not None and len(wave_data) > 0:
            alpha_components = calculate_alpha_components(wave_data, sample_wave)
            
            if alpha_components:
                html += f"""
                    <p><strong>Sample Wave: {sample_wave}</strong></p>
                    <div class="metric-grid">
                        <div class="metric-card">
                            <div class="metric-label">Total Alpha</div>
                            <div class="metric-value">{alpha_components.get('total_alpha', 0)*100:.2f}%</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-label">Selection Alpha</div>
                            <div class="metric-value">{alpha_components.get('selection_alpha', 0)*100:.2f}%</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-label">Overlay Alpha</div>
                            <div class="metric-value">{alpha_components.get('overlay_alpha', 0)*100:.2f}%</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-label">Cash Contribution</div>
                            <div class="metric-value">{alpha_components.get('cash_contribution', 0)*100:.2f}%</div>
                        </div>
                    </div>
                """
            else:
                html += '<div class="unavailable">Alpha components unavailable</div>'
        else:
            html += '<div class="unavailable">Wave data unavailable</div>'
    else:
        html += '<div class="unavailable">No waves available for alpha analysis</div>'
    
    html += "</div>"
    
    # Overlays Summary Section
    html += """
        <div class="section">
            <h2>üìä Overlays Summary - Analytics Snapshot</h2>
    """
    
    if waves and len(waves) > 0:
        sample_wave = waves[0]
        wave_data = get_wave_data_filtered(wave_name=sample_wave, days=30)
        
        if wave_data is not None and len(wave_data) > 0:
            attribution_data = calculate_attribution_matrix(wave_data, sample_wave)
            
            if attribution_data:
                html += f"""
                    <p><strong>Sample Wave: {sample_wave}</strong></p>
                    <div class="metric-grid">
                        <div class="metric-card">
                            <div class="metric-label">Risk-On Alpha</div>
                            <div class="metric-value">{attribution_data.get('risk_on_alpha', 0)*100:.2f}%</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-label">Risk-Off Alpha</div>
                            <div class="metric-value">{attribution_data.get('risk_off_alpha', 0)*100:.2f}%</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-label">Capital-Weighted Alpha</div>
                            <div class="metric-value">{attribution_data.get('capital_weighted_alpha', 0)*100:.2f}%</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-label">Exposure-Adjusted Alpha</div>
                            <div class="metric-value">{attribution_data.get('exposure_adjusted_alpha', 0)*100:.2f}%</div>
                        </div>
                    </div>
                """
            else:
                html += '<div class="unavailable">Attribution data unavailable</div>'
        else:
            html += '<div class="unavailable">Wave data unavailable</div>'
    else:
        html += '<div class="unavailable">No waves available for overlay analysis</div>'
    
    html += "</div>"
    
    # Data Integrity Section
    html += """
        <div class="section">
            <h2>‚úÖ Data Integrity - Confidence Levels</h2>
    """
    
    data_freshness = mc_data.get('data_freshness', 'unknown')
    data_age_days = mc_data.get('data_age_days', None)
    
    # source of truth: helpers/price_book.py
    if data_age_days is not None:
        if data_age_days <= PRICE_CACHE_OK_DAYS:
            confidence = "High"
            confidence_color = "#3c3"
        elif data_age_days <= PRICE_CACHE_DEGRADED_DAYS:
            confidence = "Medium"
            confidence_color = "#f90"
        else:
            confidence = "Low"
            confidence_color = "#c33"
    else:
        confidence = "Unknown"
        confidence_color = "#999"
    
    html += f"""
        <div class="metric-grid">
            <div class="metric-card">
                <div class="metric-label">Data Freshness</div>
                <div class="metric-value">{data_freshness}</div>
            </div>
            <div class="metric-card">
                <div class="metric-label">Data Age</div>
                <div class="metric-value">{data_age_days if data_age_days is not None else 'unknown'} days</div>
            </div>
            <div class="metric-card">
                <div class="metric-label">Confidence Level</div>
                <div class="metric-value" style="color: {confidence_color}">{confidence}</div>
            </div>
            <div class="metric-card">
                <div class="metric-label">Active Waves</div>
                <div class="metric-value">{mc_data.get('active_waves', 0)} / {mc_data.get('total_waves', 0)}</div>
            </div>
        </div>
    """
    
    html += "</div>"
    
    # Build Info Section
    html += f"""
        <div class="section">
            <h2>üîß Build Info - Deployment Metadata</h2>
            <div class="build-info">
                <div><strong>Git Commit:</strong> {git_hash}</div>
                <div><strong>Git Branch:</strong> {git_branch}</div>
                <div><strong>Deployment Timestamp:</strong> {deploy_timestamp}</div>
                <div><strong>Report Generated:</strong> {datetime.now().strftime("%Y-%m-%d %H:%M:%S UTC")}</div>
            </div>
        </div>
    """
    
    # Footer
    html += """
        <div class="footer">
            <p>Institutional Board Pack - Confidential</p>
            <p>Generated by Waves Analytics Platform</p>
        </div>
    </body>
    </html>
    """
    
    return html


def render_reports_tab():
    """Render the Reports tab with Institutional Board Pack generator."""
    st.header("Reports")
    st.write("Comprehensive reporting and analysis tools.")
    
    st.divider()
    
    # Institutional Board Pack Section
    st.subheader("üìä Institutional Board Pack")
    st.write("Generate a comprehensive HTML report including Mission Control, WaveScore Leaderboard, Movers, Alerts, Alpha Proof, Overlays, Data Integrity, and Build Info.")
    
    # Generate button
    if st.button("üéØ Generate Board Pack", type="primary", use_container_width=True):
        with st.spinner("Generating Institutional Board Pack..."):
            try:
                # Generate HTML report
                board_pack_html = generate_board_pack_html()
                
                # Store in session state for rendering and download
                st.session_state['board_pack_html'] = board_pack_html
                st.session_state['board_pack_timestamp'] = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                st.success("‚úÖ Board Pack generated successfully!")
                
            except Exception as e:
                st.error(f"‚ùå Error generating Board Pack: {str(e)}")
                st.info("The application continues to function. Please check data availability.")
    
    # Display board pack if generated
    if 'board_pack_html' in st.session_state:
        st.divider()
        
        # Download button
        timestamp = st.session_state.get('board_pack_timestamp', datetime.now().strftime("%Y%m%d_%H%M%S"))
        filename = f"institutional_board_pack_{timestamp}.html"
        
        col1, col2 = st.columns([3, 1])
        
        with col1:
            st.markdown("### üìÑ Preview Board Pack")
        
        with col2:
            st.download_button(
                label="üíæ Download HTML",
                data=st.session_state['board_pack_html'],
                file_name=filename,
                mime="text/html",
                use_container_width=True
            )
        
        # Render the HTML in an iframe-like component
        st.components.v1.html(st.session_state['board_pack_html'], height=800, scrolling=True)
    
    st.divider()
    
    # Additional reporting tools placeholder
    st.subheader("üìà Additional Reports")
    st.info("Additional reporting tools will be available in future releases.")


def render_overlay_proof_section():
    """
    Render Overlay Proof section showing VIX/regime overlay diagnostics.
    Displays diagnostics table with VIX, Regime, Exposure, and Returns for the last N days.
    """
    st.subheader("Overlay Proof (Last 15 Days)")
    st.write("Detailed VIX/regime overlay diagnostics showing exposure adjustments and alpha generation.")
    
    # Check if diagnostics module is available
    if not VIX_DIAGNOSTICS_AVAILABLE:
        st.warning("‚ö†Ô∏è VIX overlay diagnostics module not available.")
        return
    
    # Get available waves
    available_waves = get_available_waves()
    
    if not available_waves:
        st.info("No wave data available for diagnostics.")
        return
    
    # Create UI controls in columns
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # Wave selector - default to S&P 500 Wave (SPY Wave)
        default_wave = "S&P 500 Wave" if "S&P 500 Wave" in available_waves else available_waves[0]
        selected_wave = st.selectbox(
            "Wave Selector",
            options=available_waves,
            index=available_waves.index(default_wave) if default_wave in available_waves else 0,
            key="overlay_proof_wave_selector",
            help="Select a wave to view VIX/regime overlay diagnostics"
        )
    
    with col2:
        # Mode selector - default to Standard
        mode_options = ["Standard", "Alpha-Minus-Beta", "Private Logic"]
        selected_mode = st.selectbox(
            "Mode Selector",
            options=mode_options,
            index=0,  # Default to Standard
            key="overlay_proof_mode_selector",
            help="Select the operating mode for diagnostics"
        )
    
    with col3:
        # Days selector - default to 15
        selected_days = st.number_input(
            "Days Selector",
            min_value=1,
            max_value=365,
            value=15,
            step=1,
            key="overlay_proof_days_selector",
            help="Number of days to display in diagnostics"
        )
    
    # Fetch diagnostics data
    try:
        with st.spinner(f"Fetching diagnostics for {selected_wave}..."):
            diagnostics_df = get_wave_diagnostics(
                wave_name=selected_wave,
                mode=selected_mode,
                days=selected_days
            )
        
        # Check if diagnostics are available
        if diagnostics_df is None or diagnostics_df.empty:
            st.info("Diagnostics unavailable for the selected wave and parameters.")
            return
        
        # Prepare the table with required columns
        # Reset index to make Date a column
        table_df = diagnostics_df.reset_index()
        
        # Ensure we have the required columns, create them if missing
        display_columns = []
        
        # Date column
        if 'date' in table_df.columns:
            display_columns.append('date')
            table_df = table_df.rename(columns={'date': 'Date'})
        elif 'Date' not in table_df.columns:
            table_df['Date'] = table_df.index
        
        if 'Date' not in display_columns:
            display_columns.append('Date')
        
        # Add other columns if they exist
        column_mapping = {
            'VIX': 'VIX',
            'Regime': 'Regime',
            'VIX_Exposure': 'VIX_Exposure',
            'Safe_Fraction': 'Safe_Fraction',
            'Exposure': 'Exposure',
            'Wave_Return': 'Wave_Return',
            'Benchmark_Return': 'Benchmark_Return'
        }
        
        for col_name in column_mapping.keys():
            if col_name in table_df.columns:
                display_columns.append(col_name)
        
        # Calculate Realized_Alpha if we have both returns
        if 'Wave_Return' in table_df.columns and 'Benchmark_Return' in table_df.columns:
            table_df['Realized_Alpha'] = table_df['Wave_Return'] - table_df['Benchmark_Return']
            display_columns.append('Realized_Alpha')
        
        # Filter to only the columns we want to display
        final_columns = [col for col in display_columns if col in table_df.columns]
        display_df = table_df[final_columns].copy()
        
        # Format numeric columns for better display
        if 'VIX' in display_df.columns:
            display_df['VIX'] = display_df['VIX'].apply(lambda x: f"{x:.2f}" if pd.notna(x) else "N/A")
        
        for col in ['VIX_Exposure', 'Safe_Fraction', 'Exposure', 'Wave_Return', 'Benchmark_Return', 'Realized_Alpha']:
            if col in display_df.columns:
                display_df[col] = display_df[col].apply(lambda x: f"{x:.4f}" if pd.notna(x) else "N/A")
        
        # Format Date column
        if 'Date' in display_df.columns:
            display_df['Date'] = pd.to_datetime(display_df['Date']).dt.strftime('%Y-%m-%d')
        
        # Display the table
        st.dataframe(display_df, use_container_width=True, hide_index=True)
        
        # Display summary statistics
        st.markdown("**Summary Statistics**")
        summary_col1, summary_col2, summary_col3 = st.columns(3)
        
        with summary_col1:
            if 'Wave_Return' in diagnostics_df.columns:
                total_wave_return = diagnostics_df['Wave_Return'].sum()
                st.metric("Total Wave Return", f"{total_wave_return:.2%}")
        
        with summary_col2:
            if 'Benchmark_Return' in diagnostics_df.columns:
                total_bm_return = diagnostics_df['Benchmark_Return'].sum()
                st.metric("Total Benchmark Return", f"{total_bm_return:.2%}")
        
        with summary_col3:
            if 'Wave_Return' in diagnostics_df.columns and 'Benchmark_Return' in diagnostics_df.columns:
                total_alpha = total_wave_return - total_bm_return
                st.metric("Total Alpha", f"{total_alpha:.2%}")
        
    except Exception as e:
        st.error(f"Error fetching diagnostics: {str(e)}")
        st.info("Diagnostics unavailable. The application continues to function.")


def render_overlays_tab():
    """Render the Overlays tab."""
    st.header("Analytics Overlays")
    
    # Capital-Weighted Alpha Section
    st.subheader("Capital-Weighted Alpha")
    st.write("Alpha attribution weighted by capital allocation across portfolio.")
    
    # Load wave history data
    df = safe_load_wave_history()
    
    # Always render the section, never blank
    if df is None or len(df) == 0:
        st.info("History not loaded - cannot compute capital-weighted alpha")
    else:
        # Compute capital-weighted alpha with fallback logic
        capital_alpha = compute_capital_weighted_alpha(df)
        
        if capital_alpha['data_available']:
            col1, col2 = st.columns(2)
            
            with col1:
                if capital_alpha['capital_weighted_alpha'] is not None:
                    st.metric(
                        "Portfolio Alpha",
                        f"{capital_alpha['capital_weighted_alpha']*100:.4f}%",
                        help="Capital-weighted or equal-weighted alpha"
                    )
                else:
                    st.metric("Portfolio Alpha", "N/A")
            
            with col2:
                method_label = capital_alpha['weighting_method']
                if method_label == 'equal-weight':
                    st.info("‚ÑπÔ∏è Equal-weight fallback ‚Äî no capital inputs found")
                else:
                    st.success(f"‚úÖ Using {method_label} weighting")
        else:
            st.info("Required columns not available (portfolio_return, benchmark_return)")
    
    st.divider()
    
    # Exposure-Adjusted Alpha Section
    st.subheader("Exposure-Adjusted Alpha")
    st.write("Alpha metrics adjusted for market exposure and beta.")
    
    # Load wave history data
    if df is None or len(df) == 0:
        st.info("History not loaded - cannot compute exposure-adjusted alpha")
    else:
        # Compute exposure-adjusted alpha with fallback logic
        exposure_alpha = compute_exposure_adjusted_alpha(df, days=30)
        
        if exposure_alpha['data_available']:
            col1, col2 = st.columns(2)
            
            with col1:
                if exposure_alpha['exposure_adj_alpha_30day'] is not None:
                    st.metric(
                        "30-Day Exposure-Adjusted Alpha",
                        f"{exposure_alpha['exposure_adj_alpha_30day']*100:.4f}%",
                        help="Cumulative 30-day exposure-adjusted or unadjusted alpha"
                    )
                else:
                    st.metric("30-Day Exposure-Adjusted Alpha", "N/A")
            
            with col2:
                if exposure_alpha['is_fallback']:
                    st.info("‚ÑπÔ∏è No exposure series found ‚Äî showing unadjusted alpha")
                else:
                    st.success("‚úÖ Using exposure-adjusted alpha")
        else:
            st.info("Required columns not available (portfolio_return, benchmark_return)")
    
    st.divider()
    
    # Risk-On vs Risk-Off Attribution Section
    st.subheader("Risk-On vs Risk-Off Attribution")
    st.write("Performance attribution segmented by market regime.")
    st.info("Data unavailable")
    
    st.divider()
    
    # Overlay Proof (Last 15 Days) Section
    render_overlay_proof_section()


def render_attribution_tab():
    """Render the Attribution tab with alpha decomposition."""
    st.header("üéØ Alpha Attribution Analysis")
    
    st.markdown("""
    **Precise, reconciled decomposition of Wave alpha into actionable components:**
    
    1Ô∏è‚É£ **Stock Selection Alpha** ‚Äî Wave return vs benchmark return differential  
    2Ô∏è‚É£ **Overlay Alpha** ‚Äî VIX gating, exposure scaling, and SmartSafe features  
    3Ô∏è‚É£ **Beta/Exposure Drift** ‚Äî Target vs realized exposure impact  
    4Ô∏è‚É£ **Residual Alpha** ‚Äî Unexplained deviation and other factors
    
    **Reconciliation:** All components sum to total realized Wave alpha.
    """)
    
    # Check if attribution module is available
    if not ALPHA_ATTRIBUTION_AVAILABLE:
        st.warning("‚ö†Ô∏è Alpha attribution module not available. Please ensure alpha_attribution.py is properly installed.")
        return
    
    # Load wave history data
    wave_df = safe_load_wave_history()
    
    if wave_df is None or wave_df.empty:
        st.error("‚ùå Wave history data is not available. Cannot compute attribution.")
        return
    
    # Check for 'wave' column and get available waves
    if 'wave' not in wave_df.columns:
        st.error("‚ùå Wave column is missing from wave history data. The data format may be incorrect.")
        return
    
    available_waves = sorted(wave_df['wave'].unique().tolist())
    
    if not available_waves:
        st.error("‚ùå No waves found in history data.")
        return
    
    # Configuration controls
    col1, col2, col3 = st.columns(3)
    
    with col1:
        selected_wave = st.selectbox(
            "Select Wave",
            available_waves,
            key="attr_wave_select"
        )
    
    with col2:
        timeframe_options = {
            "1 Day": 1,
            "5 Days": 5,
            "30 Days": 30,
            "60 Days": 60,
            "1 Year": 252
        }
        selected_timeframe = st.selectbox(
            "Timeframe",
            list(timeframe_options.keys()),
            index=2,  # Default to 30 Days
            key="attr_timeframe_select"
        )
        days = timeframe_options[selected_timeframe]
    
    with col3:
        show_all_waves = st.checkbox(
            "Show All Waves Comparison",
            value=False,
            key="attr_show_all"
        )
    
    st.divider()
    
    # Compute attribution for selected wave
    try:
        # Filter data for selected wave
        wave_data = wave_df[wave_df['wave'] == selected_wave].copy()
        wave_data = wave_data.sort_values('date')
        
        # Take last N days
        wave_data = wave_data.tail(days)
        
        if len(wave_data) == 0:
            st.warning(f"‚ö†Ô∏è No data available for {selected_wave} in the selected timeframe.")
            return
        
        # Prepare history DataFrame for attribution
        wave_data.set_index('date', inplace=True)
        history_df = pd.DataFrame({
            'wave_ret': wave_data['portfolio_return'],
            'bm_ret': wave_data['benchmark_return']
        })
        
        # Compute attribution
        with st.spinner(f"Computing attribution for {selected_wave}..."):
            daily_df, summary = compute_alpha_attribution_series(
                wave_name=selected_wave,
                mode="Standard",
                history_df=history_df,
                diagnostics_df=None,  # Use fallback defaults
                tilt_strength=0.8,
                base_exposure=1.0
            )
        
        # Display results
        st.success(f"‚úÖ Attribution computed successfully for {selected_wave} ({len(wave_data)} days)")
        
        # Summary section
        st.subheader("üìä Attribution Summary")
        
        # Create summary metrics
        col_m1, col_m2, col_m3, col_m4 = st.columns(4)
        
        with col_m1:
            st.metric(
                "Total Wave Return",
                f"{summary.total_wave_return * 100:+.2f}%"
            )
        
        with col_m2:
            st.metric(
                "Total Benchmark Return",
                f"{summary.total_benchmark_return * 100:+.2f}%"
            )
        
        with col_m3:
            st.metric(
                "Total Alpha",
                f"{summary.total_alpha * 100:+.2f}%",
                delta=None
            )
        
        with col_m4:
            recon_status = "‚úÖ PASS" if abs(summary.reconciliation_pct_error) < 0.01 else "‚ö†Ô∏è CHECK"
            st.metric(
                "Reconciliation",
                recon_status,
                delta=f"{summary.reconciliation_error * 100:.4f}%"
            )
        
        st.divider()
        
        # Component breakdown table
        st.subheader("üîç Alpha Component Breakdown")
        
        component_data = {
            "Component": [
                "1Ô∏è‚É£ Exposure & Timing Alpha",
                "2Ô∏è‚É£ Regime & VIX Overlay Alpha",
                "3Ô∏è‚É£ Momentum & Trend Alpha",
                "4Ô∏è‚É£ Volatility & Risk Control Alpha",
                "5Ô∏è‚É£ Asset Selection Alpha (Residual)",
                "**Total Alpha**"
            ],
            "Cumulative Alpha": [
                f"{summary.exposure_timing_alpha * 100:+.2f}%",
                f"{summary.regime_vix_alpha * 100:+.2f}%",
                f"{summary.momentum_trend_alpha * 100:+.2f}%",
                f"{summary.volatility_control_alpha * 100:+.2f}%",
                f"{summary.asset_selection_alpha * 100:+.2f}%",
                f"**{summary.total_alpha * 100:+.2f}%**"
            ],
            "Contribution to Total": [
                f"{summary.exposure_timing_contribution_pct:+.1f}%",
                f"{summary.regime_vix_contribution_pct:+.1f}%",
                f"{summary.momentum_trend_contribution_pct:+.1f}%",
                f"{summary.volatility_control_contribution_pct:+.1f}%",
                f"{summary.asset_selection_contribution_pct:+.1f}%",
                "**100.0%**"
            ]
        }
        
        st.dataframe(
            pd.DataFrame(component_data),
            hide_index=True,
            use_container_width=True
        )
        
        st.divider()
        
        # Visualization - Waterfall chart
        st.subheader("üìà Attribution Waterfall")
        
        # Create waterfall chart
        components = [
            "Start",
            "Exposure & Timing",
            "Regime & VIX",
            "Momentum & Trend",
            "Volatility Control",
            "Asset Selection",
            "Total Alpha"
        ]
        
        values = [
            0,  # Start at 0
            summary.exposure_timing_alpha * 100,
            summary.regime_vix_alpha * 100,
            summary.momentum_trend_alpha * 100,
            summary.volatility_control_alpha * 100,
            summary.asset_selection_alpha * 100,
            summary.total_alpha * 100
        ]
        
        # Build measures for waterfall
        measures = ["absolute", "relative", "relative", "relative", "relative", "relative", "total"]
        
        waterfall_fig = go.Figure(go.Waterfall(
            name="Attribution",
            orientation="v",
            measure=measures,
            x=components,
            y=values,
            connector={"line": {"color": "rgb(63, 63, 63)"}},
            increasing={"marker": {"color": "green"}},
            decreasing={"marker": {"color": "red"}},
            totals={"marker": {"color": "blue"}}
        ))
        
        waterfall_fig.update_layout(
            title=f"Alpha Attribution Waterfall - {selected_wave}",
            showlegend=False,
            height=500
        )
        
        safe_plotly_chart(waterfall_fig, use_container_width=True, key=f"attr_waterfall_{selected_wave}_{days}")
        
        st.divider()
        
        # Time series of cumulative components
        st.subheader("üìâ Cumulative Attribution Over Time")
        
        # Calculate cumulative sums
        cumulative_df = pd.DataFrame({
            'Date': daily_df.index,
            'Exposure & Timing': daily_df['ExposureTimingŒ±'].cumsum() * 100,
            'Regime & VIX': daily_df['RegimeVIXŒ±'].cumsum() * 100,
            'Momentum & Trend': daily_df['MomentumTrendŒ±'].cumsum() * 100,
            'Volatility Control': daily_df['VolatilityControlŒ±'].cumsum() * 100,
            'Asset Selection': daily_df['AssetSelectionŒ±'].cumsum() * 100,
            'Total Alpha': daily_df['TotalAlpha'].cumsum() * 100
        }).set_index('Date')
        
        ts_fig = go.Figure()
        
        for col in cumulative_df.columns:
            ts_fig.add_trace(go.Scatter(
                x=cumulative_df.index,
                y=cumulative_df[col],
                mode='lines',
                name=col,
                line=dict(width=2 if col == 'Total Alpha' else 1)
            ))
        
        ts_fig.update_layout(
            title=f"Cumulative Attribution Components - {selected_wave}",
            xaxis_title="Date",
            yaxis_title="Cumulative Alpha (%)",
            hovermode='x unified',
            height=500
        )
        
        safe_plotly_chart(ts_fig, use_container_width=True, key=f"attr_timeseries_{selected_wave}_{days}")
        
        st.divider()
        
        # Daily attribution sample
        with st.expander("üìã View Daily Attribution Details (Last 20 Days)", expanded=False):
            sample_df = daily_df.tail(20).copy()
            
            # Format for display
            display_df = pd.DataFrame({
                'Date': sample_df.index.strftime('%Y-%m-%d'),
                'VIX': sample_df['VIX'].round(1),
                'Regime': sample_df['Regime'],
                'Exp%': sample_df['Exposure (%)'].round(0).astype(int),
                'Safe%': sample_df['Safe (%)'].round(0).astype(int),
                'ExposTimŒ±': (sample_df['ExposureTimingŒ±'] * 100).round(2),
                'RegVIXŒ±': (sample_df['RegimeVIXŒ±'] * 100).round(2),
                'MomTrendŒ±': (sample_df['MomentumTrendŒ±'] * 100).round(2),
                'VolCtrlŒ±': (sample_df['VolatilityControlŒ±'] * 100).round(2),
                'AssetSelŒ±': (sample_df['AssetSelectionŒ±'] * 100).round(2),
                'TotalŒ±': (sample_df['TotalAlpha'] * 100).round(2),
                'WaveRet': (sample_df['WaveReturn'] * 100).round(2),
                'BmRet': (sample_df['BenchmarkReturn'] * 100).round(2)
            })
            
            st.dataframe(
                display_df,
                hide_index=True,
                use_container_width=True
            )
        
        # Download option
        csv_data = daily_df.to_csv()
        st.download_button(
            label="üì• Download Full Daily Attribution (CSV)",
            data=csv_data,
            file_name=f"{selected_wave.replace(' ', '_')}_attribution_{days}d.csv",
            mime="text/csv",
            key="download_attribution"
        )
        
    except Exception as e:
        st.error(f"‚ùå Error computing attribution: {str(e)}")
        st.exception(e)
    
    # All Waves Comparison
    if show_all_waves:
        st.divider()
        st.subheader("üåä All Waves Comparison")
        
        try:
            comparison_data = []
            
            with st.spinner("Computing attribution for all waves..."):
                for wave_name in available_waves:
                    try:
                        # Filter data for this wave
                        wave_data = wave_df[wave_df['wave'] == wave_name].copy()
                        wave_data = wave_data.sort_values('date')
                        wave_data = wave_data.tail(days)
                        
                        if len(wave_data) == 0:
                            continue
                        
                        # Prepare history DataFrame
                        wave_data.set_index('date', inplace=True)
                        history_df = pd.DataFrame({
                            'wave_ret': wave_data['portfolio_return'],
                            'bm_ret': wave_data['benchmark_return']
                        })
                        
                        # Compute attribution
                        _, summary = compute_alpha_attribution_series(
                            wave_name=wave_name,
                            mode="Standard",
                            history_df=history_df,
                            diagnostics_df=None,
                            tilt_strength=0.8,
                            base_exposure=1.0
                        )
                        
                        comparison_data.append({
                            'Wave': wave_name,
                            'Days': len(wave_data),
                            'Wave Return': f"{summary.total_wave_return * 100:+.2f}%",
                            'Benchmark Return': f"{summary.total_benchmark_return * 100:+.2f}%",
                            'Total Alpha': f"{summary.total_alpha * 100:+.2f}%",
                            'Exposure & Timing': f"{summary.exposure_timing_alpha * 100:+.2f}%",
                            'Regime & VIX': f"{summary.regime_vix_alpha * 100:+.2f}%",
                            'Momentum & Trend': f"{summary.momentum_trend_alpha * 100:+.2f}%",
                            'Volatility Control': f"{summary.volatility_control_alpha * 100:+.2f}%",
                            'Asset Selection': f"{summary.asset_selection_alpha * 100:+.2f}%",
                        })
                        
                    except Exception as wave_error:
                        st.warning(f"‚ö†Ô∏è Could not compute attribution for {wave_name}: {str(wave_error)}")
                        continue
            
            if comparison_data:
                comparison_df = pd.DataFrame(comparison_data)
                st.dataframe(
                    comparison_df,
                    hide_index=True,
                    use_container_width=True
                )
                
                # Download comparison
                comparison_csv = comparison_df.to_csv(index=False)
                st.download_button(
                    label="üì• Download All Waves Comparison (CSV)",
                    data=comparison_csv,
                    file_name=f"all_waves_attribution_{days}d.csv",
                    mime="text/csv",
                    key="download_all_waves"
                )
            else:
                st.warning("‚ö†Ô∏è No comparison data available.")
                
        except Exception as e:
            st.error(f"‚ùå Error computing all waves comparison: {str(e)}")


def generate_board_pack_pdf():
    """
    Generate Board Pack PDF report using reportlab.
    Returns PDF bytes or None if reportlab unavailable.
    """
    try:
        from reportlab.lib.pagesizes import letter, A4
        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
        from reportlab.lib.units import inch
        from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak
        from reportlab.lib import colors
        from io import BytesIO
        
        # Create PDF in memory
        buffer = BytesIO()
        doc = SimpleDocTemplate(buffer, pagesize=letter, rightMargin=72, leftMargin=72,
                                topMargin=72, bottomMargin=18)
        
        # Container for the 'Flowable' objects
        elements = []
        
        # Define styles
        styles = getSampleStyleSheet()
        title_style = ParagraphStyle(
            'CustomTitle',
            parent=styles['Heading1'],
            fontSize=24,
            textColor=colors.HexColor('#667eea'),
            spaceAfter=30,
            alignment=1  # Center
        )
        heading_style = ParagraphStyle(
            'CustomHeading',
            parent=styles['Heading2'],
            fontSize=16,
            textColor=colors.HexColor('#667eea'),
            spaceAfter=12,
            spaceBefore=12
        )
        
        # Get build info
        git_hash = get_git_commit_hash()
        git_branch = get_git_branch_name()
        deploy_timestamp = get_deploy_timestamp()
        current_datetime = datetime.now().strftime("%Y-%m-%d %H:%M:%S UTC")
        
        # ==================== COVER PAGE ====================
        elements.append(Spacer(1, 2*inch))
        elements.append(Paragraph("WAVES Intelligence‚Ñ¢", title_style))
        elements.append(Spacer(1, 0.3*inch))
        elements.append(Paragraph("Institutional Board Pack", styles['Heading2']))
        elements.append(Spacer(1, 0.3*inch))
        elements.append(Paragraph(f"Generated: {current_datetime}", styles['Normal']))
        elements.append(Spacer(1, 0.5*inch))
        
        # Build Information
        build_info = f"""
        <b>Build Information:</b><br/>
        Git Commit: {git_hash}<br/>
        Git Branch: {git_branch}<br/>
        Deployment: {deploy_timestamp}
        """
        elements.append(Paragraph(build_info, styles['Normal']))
        elements.append(PageBreak())
        
        # ==================== EXECUTIVE SUMMARY ====================
        elements.append(Paragraph("Executive Summary", heading_style))
        
        # Get Mission Control data
        mc_data = get_mission_control_data()
        
        if mc_data:
            exec_summary = f"""
            <b>Mission Control Snapshot:</b><br/>
            ‚Ä¢ Market Regime: {mc_data.get('market_regime', 'unknown')}<br/>
            ‚Ä¢ VIX Gate Status: {mc_data.get('vix_gate_status', 'unknown')}<br/>
            ‚Ä¢ 30-Day Alpha: {mc_data.get('alpha_30day', 'unknown')}<br/>
            ‚Ä¢ WaveScore Leader: {mc_data.get('wavescore_leader', 'unknown')} 
              (Score: {mc_data.get('wavescore_leader_score', 'N/A')})<br/>
            ‚Ä¢ Total Waves: {mc_data.get('total_waves', 0)}<br/>
            ‚Ä¢ Active Waves: {mc_data.get('active_waves', 0)}<br/>
            ‚Ä¢ System Status: {mc_data.get('system_status', 'unknown')}<br/>
            ‚Ä¢ Data Freshness: {mc_data.get('data_freshness', 'unknown')}
            """
            elements.append(Paragraph(exec_summary, styles['Normal']))
        else:
            elements.append(Paragraph("Executive summary data unavailable.", styles['Normal']))
        
        elements.append(Spacer(1, 0.3*inch))
        
        # Alerts
        alerts = get_system_alerts()
        if alerts and len(alerts) > 0:
            elements.append(Paragraph("<b>System Alerts:</b>", styles['Normal']))
            elements.append(Spacer(1, 0.1*inch))
            for alert in alerts[:5]:  # Top 5 alerts
                alert_text = f"‚Ä¢ [{alert.get('severity', 'info').upper()}] {alert.get('message', '')}"
                elements.append(Paragraph(alert_text, styles['Normal']))
        else:
            elements.append(Paragraph("<b>Alerts:</b> No alerts at this time.", styles['Normal']))
        
        elements.append(PageBreak())
        
        # ==================== PERFORMANCE TABLE ====================
        elements.append(Paragraph("Performance Table", heading_style))
        
        waves = get_available_waves()
        
        # Build performance table for multiple time periods
        perf_data = [['Wave', '30 Days', '60 Days', 'YTD']]
        
        if waves:
            for wave_name in waves[:10]:  # Top 10 waves
                wave_30d = get_wave_data_filtered(wave_name=wave_name, days=30)
                wave_60d = get_wave_data_filtered(wave_name=wave_name, days=60)
                wave_ytd = get_wave_data_filtered(wave_name=wave_name, days=252)  # Approx YTD
                
                metrics_30d = calculate_wave_metrics(wave_30d) if wave_30d is not None else {}
                metrics_60d = calculate_wave_metrics(wave_60d) if wave_60d is not None else {}
                metrics_ytd = calculate_wave_metrics(wave_ytd) if wave_ytd is not None else {}
                
                alpha_30d = metrics_30d.get('cumulative_alpha', 'N/A')
                alpha_60d = metrics_60d.get('cumulative_alpha', 'N/A')
                alpha_ytd = metrics_ytd.get('cumulative_alpha', 'N/A')
                
                alpha_30d_str = f"{alpha_30d*100:.2f}%" if isinstance(alpha_30d, (int, float)) else 'N/A'
                alpha_60d_str = f"{alpha_60d*100:.2f}%" if isinstance(alpha_60d, (int, float)) else 'N/A'
                alpha_ytd_str = f"{alpha_ytd*100:.2f}%" if isinstance(alpha_ytd, (int, float)) else 'N/A'
                
                perf_data.append([wave_name, alpha_30d_str, alpha_60d_str, alpha_ytd_str])
        else:
            perf_data.append(['No data', 'N/A', 'N/A', 'N/A'])
        
        perf_table = Table(perf_data, colWidths=[2.5*inch, 1.5*inch, 1.5*inch, 1.5*inch])
        perf_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#667eea')),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
            ('FONTSIZE', (0, 0), (-1, 0), 12),
            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
            ('GRID', (0, 0), (-1, -1), 1, colors.black)
        ]))
        
        elements.append(perf_table)
        elements.append(PageBreak())
        
        # ==================== RISK ANALYSIS ====================
        elements.append(Paragraph("Risk Analysis", heading_style))
        
        if waves:
            sample_wave = waves[0]
            wave_data = get_wave_data_filtered(wave_name=sample_wave, days=30)
            
            if wave_data is not None:
                metrics = calculate_wave_metrics(wave_data)
                
                risk_text = f"""
                <b>Sample Wave: {sample_wave}</b><br/>
                ‚Ä¢ Volatility: {f"{metrics.get('volatility', 'N/A')*100:.2f}%" if isinstance(metrics.get('volatility'), (int, float)) else 'N/A'}<br/>
                ‚Ä¢ Max Drawdown: {f"{metrics.get('max_drawdown', 'N/A')*100:.2f}%" if isinstance(metrics.get('max_drawdown'), (int, float)) else 'N/A'}<br/>
                ‚Ä¢ Sharpe Ratio: {f"{metrics.get('sharpe_ratio', 'N/A'):.2f}" if isinstance(metrics.get('sharpe_ratio'), (int, float)) else 'N/A'}<br/>
                ‚Ä¢ Win Rate: {f"{metrics.get('win_rate', 'N/A')*100:.2f}%" if isinstance(metrics.get('win_rate'), (int, float)) else 'N/A'}
                """
                elements.append(Paragraph(risk_text, styles['Normal']))
                
                # Correlation insights
                if len(waves) > 1:
                    wave2_name = waves[1]
                    wave2_data = get_wave_data_filtered(wave_name=wave2_name, days=30)
                    correlation = calculate_wave_correlation(wave_data, wave2_data)
                    
                    if correlation is not None:
                        corr_text = f"""
                        <br/><b>Correlation Insight:</b><br/>
                        Correlation between {sample_wave} and {wave2_name}: {correlation:.3f}
                        """
                        elements.append(Paragraph(corr_text, styles['Normal']))
            else:
                elements.append(Paragraph("Risk analysis data unavailable.", styles['Normal']))
        else:
            elements.append(Paragraph("No waves available for risk analysis.", styles['Normal']))
        
        elements.append(PageBreak())
        
        # ==================== ALPHA PROOF SECTION ====================
        elements.append(Paragraph("Alpha Proof - Components Breakdown", heading_style))
        
        if waves:
            sample_wave = waves[0]
            wave_data = get_wave_data_filtered(wave_name=sample_wave, days=30)
            
            if wave_data is not None:
                alpha_components = calculate_alpha_components(wave_data, sample_wave)
                
                if alpha_components:
                    alpha_text = f"""
                    <b>Sample Wave: {sample_wave}</b><br/>
                    ‚Ä¢ Total Alpha: {alpha_components.get('total_alpha', 0)*100:.2f}%<br/>
                    ‚Ä¢ Selection Alpha: {alpha_components.get('selection_alpha', 0)*100:.2f}%<br/>
                    ‚Ä¢ Overlay Alpha: {alpha_components.get('overlay_alpha', 0)*100:.2f}%<br/>
                    ‚Ä¢ Risk-Off Alpha (Cash): {alpha_components.get('cash_contribution', 0)*100:.2f}%<br/>
                    <br/>
                    Wave Return: {alpha_components.get('wave_return', 0)*100:.2f}%<br/>
                    Benchmark Return: {alpha_components.get('benchmark_return', 0)*100:.2f}%
                    """
                    elements.append(Paragraph(alpha_text, styles['Normal']))
                else:
                    elements.append(Paragraph("Alpha components unavailable.", styles['Normal']))
            else:
                elements.append(Paragraph("Wave data unavailable for alpha proof.", styles['Normal']))
        else:
            elements.append(Paragraph("No waves available for alpha proof.", styles['Normal']))
        
        elements.append(PageBreak())
        
        # ==================== WAVE ANALYSIS - VECTOR EXPLAIN ====================
        elements.append(Paragraph("Wave Analysis - Vector Explain Narrative", heading_style))
        
        if waves:
            sample_wave = waves[0]
            wave_data = get_wave_data_filtered(wave_name=sample_wave, days=30)
            
            if wave_data is not None and len(wave_data) > 0:
                # Generate narrative
                narrative = generate_wave_narrative(sample_wave, wave_data)
                
                if narrative:
                    # Convert markdown narrative to PDF-friendly format
                    narrative_lines = narrative.split('\n')
                    for line in narrative_lines:
                        if line.strip():
                            # Handle headers
                            if line.startswith('# '):
                                elements.append(Paragraph(line.replace('# ', ''), heading_style))
                            elif line.startswith('## '):
                                elements.append(Paragraph(line.replace('## ', ''), styles['Heading3']))
                            elif line.startswith('**') and line.endswith('**'):
                                # Bold text
                                elements.append(Paragraph(line, styles['Normal']))
                            else:
                                # Regular text
                                elements.append(Paragraph(line, styles['Normal']))
                        else:
                            elements.append(Spacer(1, 0.1*inch))
                else:
                    elements.append(Paragraph("Vector Explain narrative unavailable.", styles['Normal']))
            else:
                elements.append(Paragraph("Insufficient wave data for narrative generation.", styles['Normal']))
        else:
            elements.append(Paragraph("No waves available for narrative analysis.", styles['Normal']))
        
        elements.append(PageBreak())
        
        # ==================== TOP MOVERS ====================
        elements.append(Paragraph("Top Movers - Biggest WaveScore Changes", heading_style))
        
        # Get leaderboard data
        leaderboard_data = []
        if waves:
            for wave_name in waves:
                wave_data = get_wave_data_filtered(wave_name=wave_name, days=30)
                if wave_data is not None and len(wave_data) > 0:
                    wavescore = calculate_wavescore(wave_data)
                    leaderboard_data.append({
                        'wave': wave_name,
                        'wavescore': wavescore
                    })
            
            # Sort by wavescore
            leaderboard_data.sort(key=lambda x: x['wavescore'] if isinstance(x['wavescore'], (int, float)) else 0, reverse=True)
        
        if leaderboard_data and len(leaderboard_data) >= 3:
            movers_text = "<b>Top Performers:</b><br/>"
            for i, item in enumerate(leaderboard_data[:5], 1):
                movers_text += f"{i}. {item['wave']} - WaveScore: {item['wavescore']:.1f}<br/>"
            elements.append(Paragraph(movers_text, styles['Normal']))
        else:
            elements.append(Paragraph("Insufficient data for movers analysis.", styles['Normal']))
        
        elements.append(PageBreak())
        
        # ==================== APPENDIX ====================
        elements.append(Paragraph("Appendix - System Metadata", heading_style))
        
        appendix_text = f"""
        <b>Data Timestamps:</b><br/>
        ‚Ä¢ Latest Data: {mc_data.get('data_freshness', 'unknown')}<br/>
        ‚Ä¢ Data Age: {mc_data.get('data_age_days', 'N/A')} days<br/>
        ‚Ä¢ Report Generated: {current_datetime}<br/>
        <br/>
        <b>System Information:</b><br/>
        ‚Ä¢ Total Waves Tracked: {mc_data.get('total_waves', 0)}<br/>
        ‚Ä¢ Active Waves: {mc_data.get('active_waves', 0)}<br/>
        ‚Ä¢ System Status: {mc_data.get('system_status', 'unknown')}<br/>
        <br/>
        <b>Version Control:</b><br/>
        ‚Ä¢ Git Commit Hash: {git_hash}<br/>
        ‚Ä¢ Git Branch: {git_branch}<br/>
        ‚Ä¢ Deployment Timestamp: {deploy_timestamp}
        """
        elements.append(Paragraph(appendix_text, styles['Normal']))
        
        # Build PDF
        doc.build(elements)
        
        # Get PDF bytes
        pdf_bytes = buffer.getvalue()
        buffer.close()
        
        return pdf_bytes
        
    except ImportError:
        return None
    except Exception as e:
        st.error(f"Error generating PDF: {str(e)}")
        return None


def render_board_pack_tab():
    """
    Render the Board Pack (PDF) tab with PDF download functionality.
    Uses reportlab for PDF generation with graceful error handling.
    """
    st.header("üìä Board Pack (PDF)")
    st.write("Generate a comprehensive PDF report including all key metrics, performance data, and analytics.")
    
    st.divider()
    
    # Introduction
    st.markdown("""
    ### Institutional Board Pack Report
    
    The Board Pack includes:
    
    1. **Cover Page** - Title, current date/time, and build information
    2. **Executive Summary** - Mission Control metrics, WaveScore leader, and alerts
    3. **Performance Table** - Key portfolio metrics for 30 days, 60 days, and Year-To-Date
    4. **Risk Analysis** - Volatility, max drawdown, and correlation insights
    5. **Alpha Proof** - Selection Alpha, Overlay Alpha, and Risk-Off Alpha breakdown
    6. **Wave Analysis** - WaveScore, top holdings, and Vector Explain narrative
    7. **Top Movers** - Biggest WaveScore changes during the period
    8. **Appendix** - Data timestamps and system metadata
    """)
    
    st.divider()
    
    # Preview Data On-Screen (before generation)
    st.subheader("üìã Report Preview - Metrics to Include")
    st.write("The following metrics and sections will be included in the PDF:")
    
    # Build preview table showing availability of data
    preview_data = []
    
    # Mission Control data
    mc_data = get_mission_control_data()
    mc_available = mc_data is not None and mc_data.get('market_regime') is not None
    preview_data.append({
        'Section': 'Mission Control',
        'Metrics': 'Market Regime, VIX Gate Status, 30-Day Alpha, WaveScore Leader',
        'Status': '‚úÖ Available' if mc_available else '‚ö†Ô∏è Unavailable'
    })
    
    # Wave data
    waves = get_available_waves()
    waves_available = waves is not None and len(waves) > 0
    preview_data.append({
        'Section': 'Wave Performance',
        'Metrics': '30/60/YTD Returns, Volatility, Sharpe Ratio',
        'Status': '‚úÖ Available' if waves_available else '‚ö†Ô∏è Unavailable'
    })
    
    # Alpha Proof
    alpha_available = False
    if waves_available:
        sample_wave_data = get_wave_data_filtered(wave_name=waves[0], days=30)
        alpha_available = sample_wave_data is not None
    preview_data.append({
        'Section': 'Alpha Proof Decomposition',
        'Metrics': 'Selection Alpha, Overlay Alpha, Risk-Off Alpha',
        'Status': '‚úÖ Available' if alpha_available else '‚ö†Ô∏è Unavailable'
    })
    
    # Risk Metrics
    preview_data.append({
        'Section': 'Risk Metrics',
        'Metrics': 'Max Drawdown, Win Rate, Correlation Analysis',
        'Status': '‚úÖ Available' if alpha_available else '‚ö†Ô∏è Unavailable'
    })
    
    # WaveScore & Top Holdings
    preview_data.append({
        'Section': 'WaveScore & Leaderboard',
        'Metrics': 'Top 10 Waves by WaveScore, Performance Rankings',
        'Status': '‚úÖ Available' if waves_available else '‚ö†Ô∏è Unavailable'
    })
    
    # Vector Explain
    preview_data.append({
        'Section': 'Vector Explain Narrative',
        'Metrics': 'Institutional narrative, performance drivers, insights',
        'Status': '‚úÖ Available' if alpha_available else '‚ö†Ô∏è Unavailable'
    })
    
    # Display preview table
    preview_df = pd.DataFrame(preview_data)
    st.dataframe(preview_df, use_container_width=True, hide_index=True)
    
    st.divider()
    
    # Check if reportlab is available
    try:
        import reportlab
        reportlab_available = True
    except ImportError:
        reportlab_available = False
    
    if not reportlab_available:
        st.error("‚ùå PDF Generation not available.")
        st.warning("The `reportlab` library is required for PDF generation but is not installed.")
        st.info("Please install reportlab to enable PDF downloads: `pip install reportlab`")
        st.info("The app will continue to function normally. Other tabs remain accessible.")
        return
    
    # Control buttons
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("üì• Generate Board Pack", type="primary", use_container_width=True):
            with st.spinner("Generating Board Pack PDF..."):
                try:
                    pdf_bytes = generate_board_pack_pdf()
                    
                    if pdf_bytes is None:
                        st.error("‚ùå Report generation unavailable.")
                        st.warning("Unable to generate PDF report. Please check system configuration.")
                    else:
                        # Generate filename
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        filename = f"WAVES_Board_Pack_{timestamp}.pdf"
                        
                        # Provide download
                        st.download_button(
                            label="üíæ Download PDF",
                            data=pdf_bytes,
                            file_name=filename,
                            mime="application/pdf",
                            use_container_width=True
                        )
                        
                        st.success("‚úÖ PDF generated successfully! Click 'Download PDF' above to save.")
                        
                except Exception as e:
                    st.error("‚ùå Report generation unavailable.")
                    st.warning(f"An error occurred while generating the report: {str(e)}")
                    st.info("The application continues to function normally. Other tabs remain accessible.")
    
    with col2:
        if st.button("üîÑ Force Reload Data", use_container_width=True):
            # Clear cache
            st.cache_data.clear()
            st.cache_resource.clear()
            
            # Remove wave-related session keys
            keys_to_remove = [key for key in st.session_state.keys() if 'wave' in key.lower()]
            for key in keys_to_remove:
                del st.session_state[key]
            
            st.success("‚úÖ Cache cleared and data reloaded!")
            st.info("Please click 'Generate Board Pack' again to create PDF with fresh data.")
            # Mark user interaction
            st.session_state.user_interaction_detected = True
            trigger_rerun("boardpack_snapshot_refresh")
    
    # Footer
    st.markdown("---")
    st.caption("Board Pack PDF generation uses live data from the Institutional Console. All sections gracefully handle unavailable data.")


def render_ic_pack_tab():
    """
    Render the IC Pack (Investment Committee Pack) tab.
    
    This comprehensive executive report includes:
    - Mission Control Summary Tiles
    - WaveScore Details (leaderboard, alerts, biggest movers)
    - Overlay Proof Table (VIX/Regime diagnostics)
    - Alpha Proof Decomposition
    - Vector Brief Narrative
    - HTML Download capability
    """
    st.header("üìä IC Pack - Investment Committee Executive Report")
    st.write("Comprehensive executive intelligence pack for investment committee review.")
    
    st.divider()
    
    # ========================================================================
    # SECTION 1: MISSION CONTROL SUMMARY TILES
    # ========================================================================
    st.subheader("üéØ Mission Control Summary")
    
    try:
        mc_data = get_mission_control_data()
        
        # First row: Primary metrics (5 columns)
        col1, col2, col3, col4, col5 = st.columns(5)
        
        with col1:
            regime_value = mc_data.get('market_regime', 'Data unavailable')
            if 'Risk-On' in str(regime_value):
                regime_display = f"üìà {regime_value}"
            elif 'Risk-Off' in str(regime_value):
                regime_display = f"üìâ {regime_value}"
            else:
                regime_display = f"‚ûñ {regime_value}"
            
            st.metric(
                label="Market Regime",
                value=regime_display,
                help="Current market regime based on portfolio performance"
            )
        
        with col2:
            vix_value = mc_data.get('vix_gate_status', 'Data unavailable')
            if 'GREEN' in str(vix_value):
                vix_display = f"üü¢ {vix_value}"
            elif 'YELLOW' in str(vix_value):
                vix_display = f"üü° {vix_value}"
            elif 'RED' in str(vix_value):
                vix_display = f"üî¥ {vix_value}"
            else:
                vix_display = str(vix_value)
            
            st.metric(
                label="VIX Gate Status",
                value=vix_display,
                help="Volatility-based risk gate"
            )
        
        with col3:
            alpha_today_str = mc_data.get('alpha_today', 'Data unavailable')
            alpha_30day_str = mc_data.get('alpha_30day', 'Data unavailable')
            
            st.markdown("**Alpha Captured**")
            st.write(f"Latest: {alpha_today_str}")
            st.write(f"30-Day: {alpha_30day_str}")
        
        with col4:
            st.markdown("**WaveScore Leader**")
            leader_name = mc_data.get('wavescore_leader', 'Data unavailable')
            leader_score = mc_data.get('wavescore_leader_score', 'N/A')
            st.write(f"{leader_name}")
            st.write(f"Score: {leader_score}")
        
        with col5:
            st.metric(
                label="System Status",
                value=mc_data.get('system_status', 'Data unavailable'),
                help="Data freshness and system health"
            )
            if mc_data.get('data_age_days') is not None:
                st.caption(f"Data age: {mc_data['data_age_days']} days")
        
        # Second row: Additional metrics
        col_a, col_b, col_c, col_d = st.columns(4)
        
        with col_a:
            st.metric("Total Waves", mc_data.get('total_waves', 0))
        
        with col_b:
            st.metric("Active Waves", mc_data.get('active_waves', 0))
        
        with col_c:
            st.metric("Data Freshness", mc_data.get('data_freshness', 'unknown'))
        
        with col_d:
            pass  # Reserved for future use
    
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Mission Control data unavailable: {str(e)}")
    
    st.divider()
    
    # ========================================================================
    # SECTION 2: WAVESCORE DETAILS
    # ========================================================================
    st.subheader("üèÜ WaveScore Details")
    
    try:
        waves = get_available_waves()
        
        if not waves:
            st.info("üìä Data unavailable - No wave data found")
        else:
            # Build leaderboard
            leaderboard_data = []
            
            for wave_name in waves:
                wave_data = get_wave_data_filtered(wave_name, days=30)
                if wave_data is not None and len(wave_data) > 0:
                    metrics = calculate_wave_metrics(wave_data)
                    
                    leaderboard_data.append({
                        'Wave': wave_name,
                        'WaveScore': metrics.get('wavescore', 0),
                        'Cumulative Alpha': metrics.get('cumulative_alpha', 'N/A'),
                        'Volatility': metrics.get('volatility', 'N/A'),
                        'Sharpe Ratio': metrics.get('sharpe_ratio', 'N/A')
                    })
            
            if leaderboard_data:
                leaderboard_df = pd.DataFrame(leaderboard_data)
                leaderboard_df = leaderboard_df.sort_values('WaveScore', ascending=False)
                
                # Top scoring waves
                st.markdown("**üìà Top Scoring Waves (Last 30 Days)**")
                top_waves = leaderboard_df.head(5)
                st.dataframe(top_waves, use_container_width=True, hide_index=True)
                
                # Biggest movers (highest absolute change in score)
                st.markdown("**üîÑ Biggest Movers**")
                st.caption("Waves with highest WaveScore volatility indicate significant performance changes")
                
                # Calculate movers based on score variance
                movers_data = []
                for wave_name in waves[:10]:  # Limit to top 10 for performance
                    wave_data = get_wave_data_filtered(wave_name, days=30)
                    if wave_data is not None and len(wave_data) > 5:
                        if 'alpha' in wave_data.columns:
                            alpha_std = wave_data['alpha'].std()
                            cumulative_alpha = wave_data['alpha'].sum()
                            movers_data.append({
                                'Wave': wave_name,
                                'Alpha Volatility': f"{alpha_std * 100:.3f}%",
                                'Cumulative Alpha': f"{cumulative_alpha * 100:.2f}%",
                                'Movement Score': alpha_std * 1000
                            })
                
                if movers_data:
                    movers_df = pd.DataFrame(movers_data)
                    movers_df = movers_df.sort_values('Movement Score', ascending=False).head(5)
                    movers_df = movers_df.drop(columns=['Movement Score'])
                    st.dataframe(movers_df, use_container_width=True, hide_index=True)
                else:
                    st.info("Movement data unavailable")
                
                # Alerts section
                st.markdown("**‚ö†Ô∏è WaveScore Alerts**")
                low_score_waves = leaderboard_df[leaderboard_df['WaveScore'] < 40]
                
                if len(low_score_waves) > 0:
                    st.warning(f"üî¥ {len(low_score_waves)} wave(s) with low WaveScore (<40)")
                    st.dataframe(low_score_waves, use_container_width=True, hide_index=True)
                else:
                    st.success("‚úÖ No low-scoring waves detected")
            else:
                st.info("üìä Data unavailable - Could not calculate WaveScores")
    
    except Exception as e:
        st.warning(f"‚ö†Ô∏è WaveScore details unavailable: {str(e)}")
    
    st.divider()
    
    # ========================================================================
    # SECTION 3: OVERLAY PROOF TABLE
    # ========================================================================
    st.subheader("üî¨ Overlay Proof Table")
    st.caption("VIX/Regime overlay diagnostics showing exposure scaling activity")
    
    try:
        if not VIX_DIAGNOSTICS_AVAILABLE:
            st.warning("‚ö†Ô∏è VIX diagnostics module not available")
        else:
            # Selectors
            col_sel1, col_sel2, col_sel3 = st.columns(3)
            
            with col_sel1:
                waves = get_available_waves()
                if waves and 'SPY Wave' in waves:
                    default_wave = 'SPY Wave'
                elif waves:
                    default_wave = waves[0]
                else:
                    default_wave = 'SPY Wave'
                
                selected_wave = st.selectbox(
                    "Wave",
                    options=waves if waves else [default_wave],
                    index=waves.index(default_wave) if waves and default_wave in waves else 0,
                    key="ic_overlay_wave"
                )
            
            with col_sel2:
                selected_mode = st.selectbox(
                    "Mode",
                    options=["Standard", "Aggressive", "Conservative"],
                    index=0,
                    key="ic_overlay_mode"
                )
            
            with col_sel3:
                selected_days = st.selectbox(
                    "Days",
                    options=[15, 30, 60, 90],
                    index=0,
                    key="ic_overlay_days"
                )
            
            # Fetch diagnostics
            try:
                diagnostics_df = get_wave_diagnostics(
                    wave_name=selected_wave,
                    mode=selected_mode,
                    days=selected_days
                )
                
                if diagnostics_df is None or len(diagnostics_df) == 0:
                    st.info("üìä Data unavailable for selected parameters")
                else:
                    # Format and display the table
                    display_df = diagnostics_df.copy()
                    
                    # Ensure required columns exist
                    required_cols = ['Date', 'VIX', 'Regime', 'VIX_Exposure', 'Safe_Fraction', 
                                   'Exposure', 'Wave_Return', 'Benchmark_Return']
                    
                    # Calculate Realized_Alpha if possible
                    if 'Wave_Return' in display_df.columns and 'Benchmark_Return' in display_df.columns:
                        display_df['Realized_Alpha'] = display_df['Wave_Return'] - display_df['Benchmark_Return']
                    
                    # Select columns for display (use available columns)
                    display_cols = []
                    for col in required_cols + ['Realized_Alpha']:
                        if col in display_df.columns:
                            display_cols.append(col)
                    
                    if display_cols:
                        display_df = display_df[display_cols]
                        
                        # Format percentages
                        pct_cols = ['VIX_Exposure', 'Safe_Fraction', 'Exposure', 'Wave_Return', 
                                   'Benchmark_Return', 'Realized_Alpha']
                        for col in pct_cols:
                            if col in display_df.columns:
                                display_df[col] = display_df[col].apply(
                                    lambda x: f"{x * 100:.2f}%" if pd.notna(x) else "N/A"
                                )
                        
                        # Format date
                        if 'Date' in display_df.columns:
                            display_df['Date'] = pd.to_datetime(display_df['Date']).dt.strftime('%Y-%m-%d')
                        
                        # Display table
                        st.dataframe(display_df, use_container_width=True, hide_index=True)
                        
                        # Summary statistics
                        st.markdown("**Summary Statistics**")
                        col_stat1, col_stat2, col_stat3 = st.columns(3)
                        
                        with col_stat1:
                            if 'Realized_Alpha' in diagnostics_df.columns:
                                avg_alpha = diagnostics_df['Realized_Alpha'].mean()
                                st.metric("Avg Daily Alpha", f"{avg_alpha * 100:.3f}%")
                        
                        with col_stat2:
                            if 'VIX_Exposure' in diagnostics_df.columns:
                                avg_vix_exp = diagnostics_df['VIX_Exposure'].mean()
                                st.metric("Avg VIX Exposure", f"{avg_vix_exp * 100:.1f}%")
                        
                        with col_stat3:
                            if 'Safe_Fraction' in diagnostics_df.columns:
                                avg_safe = diagnostics_df['Safe_Fraction'].mean()
                                st.metric("Avg Safe Fraction", f"{avg_safe * 100:.1f}%")
                    else:
                        st.info("üìä Data unavailable - required columns missing")
            
            except Exception as diag_err:
                st.info(f"üìä Data unavailable: {str(diag_err)}")
    
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Overlay proof table unavailable: {str(e)}")
    
    st.divider()
    
    # ========================================================================
    # SECTION 4: ALPHA PROOF DECOMPOSITION
    # ========================================================================
    st.subheader("üîç Alpha Proof Decomposition")
    st.caption("Breakdown of alpha sources from decision attribution")
    
    try:
        if not ALPHA_ATTRIBUTION_AVAILABLE:
            st.warning("‚ö†Ô∏è Alpha attribution module not available")
        else:
            # Wave selector for alpha decomposition
            waves = get_available_waves()
            if waves:
                selected_alpha_wave = st.selectbox(
                    "Select Wave for Alpha Decomposition",
                    options=waves,
                    index=waves.index('SPY Wave') if 'SPY Wave' in waves else 0,
                    key="ic_alpha_wave"
                )
                
                # Get wave data
                wave_data = get_wave_data_filtered(selected_alpha_wave, days=30)
                
                if wave_data is not None and len(wave_data) > 0:
                    try:
                        # Prepare data for attribution - transform column names
                        wave_data_copy = wave_data.copy()
                        
                        # Ensure date is index
                        if 'date' in wave_data_copy.columns:
                            wave_data_copy = wave_data_copy.set_index('date')
                        
                        # Transform column names to match expected format
                        if 'portfolio_return' in wave_data_copy.columns and 'benchmark_return' in wave_data_copy.columns:
                            wave_data_copy['wave_ret'] = wave_data_copy['portfolio_return']
                            wave_data_copy['bm_ret'] = wave_data_copy['benchmark_return']
                        elif 'return' in wave_data_copy.columns and 'benchmark_return' in wave_data_copy.columns:
                            wave_data_copy['wave_ret'] = wave_data_copy['return']
                            wave_data_copy['bm_ret'] = wave_data_copy['benchmark_return']
                        else:
                            raise ValueError("Missing required return columns (wave_ret/bm_ret)")
                        
                        # Use alpha attribution module with correct parameters
                        daily_df, summary = compute_alpha_attribution_series(
                            wave_name=selected_alpha_wave,
                            mode="Standard",
                            history_df=wave_data_copy
                        )
                        
                        if summary is not None:
                            
                            # Display alpha components
                            st.markdown("**Alpha Components (Last 30 Days)**")
                            
                            col_alpha1, col_alpha2, col_alpha3, col_alpha4 = st.columns(4)
                            
                            with col_alpha1:
                                # Asset Selection Alpha
                                selection_val = getattr(summary, 'asset_selection_alpha', None)
                                if selection_val is not None:
                                    st.metric("Selection Alpha", f"{selection_val * 100:.3f}%")
                                else:
                                    st.metric("Selection Alpha", "Data unavailable")
                            
                            with col_alpha2:
                                # Exposure & Timing Alpha (previously called "overlay")
                                overlay_val = getattr(summary, 'exposure_timing_alpha', None)
                                if overlay_val is not None:
                                    st.metric("Timing Alpha", f"{overlay_val * 100:.3f}%")
                                else:
                                    st.metric("Timing Alpha", "Data unavailable")
                            
                            with col_alpha3:
                                # Regime & VIX Alpha (previously called "risk-off")
                                risk_off_val = getattr(summary, 'regime_vix_alpha', None)
                                if risk_off_val is not None:
                                    st.metric("Regime Alpha", f"{risk_off_val * 100:.3f}%")
                                else:
                                    st.metric("Regime Alpha", "Data unavailable")
                            
                            with col_alpha4:
                                # Momentum & Trend Alpha
                                momentum_val = getattr(summary, 'momentum_trend_alpha', None)
                                if momentum_val is not None:
                                    st.metric("Momentum Alpha", f"{momentum_val * 100:.3f}%")
                                else:
                                    st.metric("Momentum Alpha", "Data unavailable")
                            
                            # Display total alpha and reconciliation
                            total_alpha = getattr(summary, 'total_alpha', None)
                            if total_alpha is not None:
                                st.metric("Total Alpha", f"{total_alpha * 100:.3f}%")
                            
                            # Display reconciliation error as a progress bar (lower is better)
                            recon_error_pct = abs(getattr(summary, 'reconciliation_pct_error', 0))
                            if recon_error_pct < 0.01:  # Less than 1% error is excellent
                                st.success(f"‚úÖ Reconciliation: {(1 - recon_error_pct) * 100:.1f}% accurate")
                            
                        else:
                            # Fallback: Try to use DecisionAttributionEngine
                            st.info("üìä Using alternative attribution calculation")
                            
                            engine = DecisionAttributionEngine()
                            components = engine.compute_attribution(wave_data, selected_alpha_wave)
                            
                            if components:
                                col_alpha1, col_alpha2, col_alpha3, col_alpha4 = st.columns(4)
                                
                                with col_alpha1:
                                    if components.selection_available:
                                        st.metric("Selection", f"{components.selection_alpha * 100:.3f}%")
                                    else:
                                        st.metric("Selection", "Data unavailable")
                                
                                with col_alpha2:
                                    if components.overlay_available:
                                        st.metric("Overlay", f"{components.overlay_alpha * 100:.3f}%")
                                    else:
                                        st.metric("Overlay", "Data unavailable")
                                
                                with col_alpha3:
                                    if components.risk_off_available:
                                        st.metric("Risk-Off", f"{components.risk_off_alpha * 100:.3f}%")
                                    else:
                                        st.metric("Risk-Off", "Data unavailable")
                                
                                with col_alpha4:
                                    if components.residual_available:
                                        st.metric("Residual", f"{components.residual_alpha * 100:.3f}%")
                                    else:
                                        st.metric("Residual", "Data unavailable")
                                
                                st.progress(components.data_completeness, 
                                          text=f"Data Completeness: {components.data_completeness * 100:.0f}%")
                            else:
                                st.info("üìä Data unavailable for attribution calculation")
                    
                    except Exception as attr_err:
                        st.info(f"üìä Data unavailable: {str(attr_err)}")
                else:
                    st.info("üìä Data unavailable for selected wave")
            else:
                st.info("üìä Data unavailable - No waves found")
    
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Alpha decomposition unavailable: {str(e)}")
    
    st.divider()
    
    # ========================================================================
    # SECTION 5: VECTOR BRIEF NARRATIVE
    # ========================================================================
    st.subheader("üìù Vector Brief - Executive Narrative")
    st.caption("AI-generated summary of key metrics, drivers, and recommended actions")
    
    try:
        # Generate narrative based on available data
        narrative_sections = []
        
        # Section 1: Current State
        try:
            mc_data = get_mission_control_data()
            regime = mc_data.get('market_regime', 'unknown')
            vix_status = mc_data.get('vix_gate_status', 'unknown')
            alpha_30day = mc_data.get('alpha_30day', 'unknown')
            
            if regime != 'unknown':
                narrative_sections.append(
                    f"**Current Market State:** The portfolio is operating in a {regime} environment "
                    f"with VIX gate status at {vix_status}. "
                    f"30-day alpha performance stands at {alpha_30day}."
                )
            else:
                narrative_sections.append("**Current Market State:** Data unavailable for current market assessment.")
        except:
            narrative_sections.append("**Current Market State:** Data unavailable for current market assessment.")
        
        # Section 2: Performance Drivers
        try:
            waves = get_available_waves()
            if waves:
                top_performers = []
                for wave in waves[:5]:  # Top 5 waves
                    wave_data = get_wave_data_filtered(wave, days=30)
                    if wave_data is not None and len(wave_data) > 0:
                        metrics = calculate_wave_metrics(wave_data)
                        wavescore = metrics.get('wavescore', 0)
                        if wavescore > 60:
                            top_performers.append((wave, wavescore))
                
                if top_performers:
                    top_performers.sort(key=lambda x: x[1], reverse=True)
                    top_wave = top_performers[0]
                    narrative_sections.append(
                        f"**Major Performance Drivers:** {top_wave[0]} leads with a WaveScore of {top_wave[1]:.1f}, "
                        f"indicating strong relative performance. "
                        f"{len(top_performers)} wave(s) currently scoring above 60."
                    )
                else:
                    narrative_sections.append("**Major Performance Drivers:** No strong performers identified in current period.")
            else:
                narrative_sections.append("**Major Performance Drivers:** Data unavailable for performance analysis.")
        except:
            narrative_sections.append("**Major Performance Drivers:** Data unavailable for performance analysis.")
        
        # Section 3: Risk Assessment
        try:
            mc_data = get_mission_control_data()
            vix_status = mc_data.get('vix_gate_status', 'unknown')
            
            if 'RED' in str(vix_status):
                risk_level = "elevated"
                risk_action = "Recommend maintaining defensive positioning and monitoring for regime shifts."
            elif 'YELLOW' in str(vix_status):
                risk_level = "moderate"
                risk_action = "Continue normal operations with heightened awareness of volatility signals."
            elif 'GREEN' in str(vix_status):
                risk_level = "low"
                risk_action = "Favorable environment for growth-oriented positioning."
            else:
                risk_level = "unknown"
                risk_action = "Data unavailable for risk assessment."
            
            narrative_sections.append(
                f"**Risk Assessment:** Current volatility regime indicates {risk_level} risk levels. "
                f"{risk_action}"
            )
        except:
            narrative_sections.append("**Risk Assessment:** Data unavailable for risk assessment.")
        
        # Section 4: Recommended Actions
        try:
            mc_data = get_mission_control_data()
            system_status = mc_data.get('system_status', 'unknown')
            data_age = mc_data.get('data_age_days', None)
            
            actions = []
            
            if data_age is not None and data_age > 3:
                actions.append(f"Update data feeds (currently {data_age} days old)")
            
            if system_status == 'Stale':
                actions.append("Refresh system data to ensure accurate reporting")
            
            # Check for low-scoring waves
            waves = get_available_waves()
            low_score_count = 0
            if waves:
                for wave in waves:
                    wave_data = get_wave_data_filtered(wave, days=30)
                    if wave_data is not None:
                        metrics = calculate_wave_metrics(wave_data)
                        if metrics.get('wavescore', 100) < 40:
                            low_score_count += 1
            
            if low_score_count > 0:
                actions.append(f"Review {low_score_count} underperforming wave(s)")
            
            if actions:
                action_text = "; ".join(actions) + "."
            else:
                action_text = "No immediate actions required. Continue monitoring key metrics."
            
            narrative_sections.append(f"**Recommended Actions:** {action_text}")
        except:
            narrative_sections.append("**Recommended Actions:** Data unavailable for action recommendations.")
        
        # Display narrative
        for section in narrative_sections:
            st.markdown(section)
            st.write("")
        
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Vector brief unavailable: {str(e)}")
    
    st.divider()
    
    # ========================================================================
    # SECTION 6: HTML DOWNLOAD
    # ========================================================================
    st.subheader("üì• Download IC Pack Report")
    
    try:
        col_dl1, col_dl2, col_dl3 = st.columns([1, 2, 1])
        
        with col_dl2:
            if st.button("üì• Download IC Pack (HTML)", type="primary", use_container_width=True):
                with st.spinner("Generating HTML report..."):
                    try:
                        html_content = generate_ic_pack_html()
                        
                        if html_content:
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            filename = f"IC_Pack_{timestamp}.html"
                            
                            st.download_button(
                                label="üíæ Download HTML Report",
                                data=html_content,
                                file_name=filename,
                                mime="text/html",
                                use_container_width=True
                            )
                            
                            st.success("‚úÖ HTML report generated successfully!")
                        else:
                            st.error("‚ùå Report generation failed")
                    
                    except Exception as gen_err:
                        st.error(f"‚ùå Report generation error: {str(gen_err)}")
    
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Download functionality unavailable: {str(e)}")
    
    st.divider()
    
    # Footer
    st.markdown("---")
    st.caption("IC Pack report powered by WAVES Intelligence‚Ñ¢ - Real-time institutional analytics")


def generate_ic_pack_html():
    """
    Generate HTML version of the IC Pack report for download.
    
    Returns:
        HTML string containing the full IC Pack report
    """
    try:
        # Start HTML document
        html_parts = []
        
        html_parts.append("""
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>IC Pack - Investment Committee Report</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 40px;
            background-color: #f5f5f5;
            color: #333;
        }
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 10px;
            margin-bottom: 30px;
        }
        h1 {
            margin: 0;
            font-size: 32px;
        }
        .timestamp {
            font-size: 14px;
            opacity: 0.9;
            margin-top: 10px;
        }
        .section {
            background: white;
            padding: 25px;
            margin-bottom: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        h2 {
            color: #667eea;
            border-bottom: 2px solid #667eea;
            padding-bottom: 10px;
            margin-top: 0;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
        }
        th {
            background-color: #667eea;
            color: white;
            padding: 12px;
            text-align: left;
        }
        td {
            padding: 10px;
            border-bottom: 1px solid #ddd;
        }
        tr:hover {
            background-color: #f5f5f5;
        }
        .metric-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }
        .metric-card {
            background: #f8f9fa;
            padding: 15px;
            border-radius: 5px;
            border-left: 4px solid #667eea;
        }
        .metric-label {
            font-size: 12px;
            color: #666;
            text-transform: uppercase;
        }
        .metric-value {
            font-size: 24px;
            font-weight: bold;
            color: #333;
            margin-top: 5px;
        }
        .narrative {
            line-height: 1.8;
            color: #555;
        }
        .footer {
            text-align: center;
            color: #999;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #ddd;
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>üìä IC Pack - Investment Committee Executive Report</h1>
        <div class="timestamp">Generated: """ + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + """</div>
    </div>
""")
        
        # Mission Control Section
        html_parts.append('<div class="section"><h2>üéØ Mission Control Summary</h2>')
        
        try:
            mc_data = get_mission_control_data()
            html_parts.append('<div class="metric-grid">')
            
            metrics = [
                ('Market Regime', mc_data.get('market_regime', 'N/A')),
                ('VIX Gate Status', mc_data.get('vix_gate_status', 'N/A')),
                ('30-Day Alpha', mc_data.get('alpha_30day', 'N/A')),
                ('WaveScore Leader', mc_data.get('wavescore_leader', 'N/A')),
                ('System Status', mc_data.get('system_status', 'N/A')),
                ('Total Waves', mc_data.get('total_waves', 0)),
            ]
            
            for label, value in metrics:
                html_parts.append(f'''
                <div class="metric-card">
                    <div class="metric-label">{label}</div>
                    <div class="metric-value">{value}</div>
                </div>
                ''')
            
            html_parts.append('</div>')
        except:
            html_parts.append('<p>Data unavailable</p>')
        
        html_parts.append('</div>')
        
        # WaveScore Leaderboard
        html_parts.append('<div class="section"><h2>üèÜ WaveScore Leaderboard</h2>')
        
        try:
            waves = get_available_waves()
            if waves:
                leaderboard_data = []
                for wave in waves:
                    wave_data = get_wave_data_filtered(wave, days=30)
                    if wave_data is not None:
                        metrics = calculate_wave_metrics(wave_data)
                        leaderboard_data.append({
                            'Wave': wave,
                            'WaveScore': metrics.get('wavescore', 0),
                            'Alpha': metrics.get('cumulative_alpha', 'N/A')
                        })
                
                if leaderboard_data:
                    leaderboard_data.sort(key=lambda x: x['WaveScore'], reverse=True)
                    
                    html_parts.append('<table><tr><th>Wave</th><th>WaveScore</th><th>Cumulative Alpha</th></tr>')
                    for item in leaderboard_data[:10]:
                        html_parts.append(f'''
                        <tr>
                            <td>{item['Wave']}</td>
                            <td>{item['WaveScore']:.1f}</td>
                            <td>{item['Alpha']}</td>
                        </tr>
                        ''')
                    html_parts.append('</table>')
                else:
                    html_parts.append('<p>Data unavailable</p>')
            else:
                html_parts.append('<p>Data unavailable</p>')
        except:
            html_parts.append('<p>Data unavailable</p>')
        
        html_parts.append('</div>')
        
        # Vector Brief
        html_parts.append('<div class="section"><h2>üìù Vector Brief - Executive Narrative</h2>')
        html_parts.append('<div class="narrative">')
        
        try:
            mc_data = get_mission_control_data()
            regime = mc_data.get('market_regime', 'unknown')
            alpha = mc_data.get('alpha_30day', 'unknown')
            
            html_parts.append(f'<p><strong>Current State:</strong> The portfolio is operating in a {regime} environment with 30-day alpha of {alpha}.</p>')
            html_parts.append('<p><strong>Risk Assessment:</strong> Monitor key volatility indicators and maintain appropriate positioning based on current regime.</p>')
            html_parts.append('<p><strong>Recommended Actions:</strong> Continue monitoring performance metrics and adjust exposure as market conditions evolve.</p>')
        except:
            html_parts.append('<p>Data unavailable for narrative generation.</p>')
        
        html_parts.append('</div></div>')
        
        # Footer
        html_parts.append('''
    <div class="footer">
        <p>IC Pack Report - WAVES Intelligence‚Ñ¢</p>
        <p>Institutional-grade analytics for investment committees</p>
    </div>
</body>
</html>
''')
        
        return ''.join(html_parts)
    
    except Exception as e:
        return f"<html><body><h1>Error generating report</h1><p>{str(e)}</p></body></html>"


def render_alpha_capture_tab():
    """
    Render the Alpha Capture tab with Alpha Drivers breakdown.
    
    This tab shows:
    - Left column: All Waves table with 30D returns, alpha, and exposure
    - Right column: Alpha Drivers breakdown for selected wave showing 3-bucket attribution
    
    Features:
    - Simple table-based layout (no HTML components)
    - Mobile-friendly design
    - Timeframe selector (default 30D)
    - 3-bucket alpha breakdown with percentages and return points
    - Graceful fallback when exposure/diagnostics data is missing
    """
    try:
        # ========================================================================
        # TOP SECTION: Introductory Header
        # ========================================================================
        st.markdown("# Alpha Drivers.")
        st.markdown("### A defensible breakdown of where alpha comes from.")
        
        st.markdown("""
        **Key Definitions:**
        - **Total Alpha = Wave Return ‚àí Benchmark Return.**
        - **Stock Selection**: Alpha from choosing which assets to hold.
        - **Risk Overlay**: Alpha from VIX/SafeSmart/Cash shift strategies.
        - **Residual/Other**: Remaining alpha not attributed to the above.
        """)
        
        st.markdown("---")
        
        # Timeframe selector
        timeframe_options = {
            "30 Days": 30,
            "60 Days": 60,
            "90 Days": 90,
            "365 Days": 365
        }
        
        selected_timeframe_label = st.selectbox(
            "Select Timeframe:",
            options=list(timeframe_options.keys()),
            index=0,  # Default to 30 Days
            key=k("AlphaCapture", "timeframe")
        )
        
        timeframe_days = timeframe_options[selected_timeframe_label]
        
        # Compute alpha metrics for all waves
        with st.spinner(f"Computing alpha metrics for {selected_timeframe_label}..."):
            all_metrics = compute_alpha_metrics_all_waves()
        
        if not all_metrics:
            st.warning("‚ö†Ô∏è No alpha data available. Please ensure wave_history.csv contains valid data.")
            return
        
        st.markdown("---")
        
        # ========================================================================
        # TWO-COLUMN LAYOUT: All Waves Table (Left) + Alpha Drivers (Right)
        # ========================================================================
        
        # Build DataFrame for All Waves table
        table_data = []
        for metrics in all_metrics:
            # Get the appropriate alpha based on selected timeframe
            if timeframe_days == 30:
                alpha_val = metrics.get('alpha_30d')
                exposure_val = metrics.get('exposure_30d', 1.0)
            elif timeframe_days == 60:
                alpha_val = metrics.get('alpha_60d')
                exposure_val = metrics.get('exposure_60d', 1.0)
            elif timeframe_days == 90:
                # Use 60d as approximation for 90d
                alpha_val = metrics.get('alpha_60d')
                exposure_val = metrics.get('exposure_60d', 1.0)
            else:  # 365 days
                alpha_val = metrics.get('alpha_365d')
                exposure_val = metrics.get('exposure_365d', 1.0)
            
            # Format values for display
            def fmt_pct(val):
                return f"{val*100:.2f}%" if val is not None else "N/A"
            
            def fmt_exp(val):
                return f"{val:.2f}" if val is not None else "1.00"
            
            # Get wave data to calculate actual returns for the timeframe
            wave_universe_version = st.session_state.get("wave_universe_version", 1)
            wave_data = get_wave_data_filtered(
                wave_name=metrics['wave_name'],
                days=timeframe_days,
                _wave_universe_version=wave_universe_version
            )
            
            wave_return = 0.0
            benchmark_return = 0.0
            
            if wave_data is not None and len(wave_data) > 0:
                if 'portfolio_return' in wave_data.columns:
                    wave_return = wave_data['portfolio_return'].sum()
                if 'benchmark_return' in wave_data.columns:
                    benchmark_return = wave_data['benchmark_return'].sum()
            
            table_data.append({
                'Wave Name': metrics['wave_name'],
                f'{selected_timeframe_label} Wave Return': fmt_pct(wave_return),
                f'{selected_timeframe_label} Benchmark Return': fmt_pct(benchmark_return),
                f'{selected_timeframe_label} Alpha': fmt_pct(alpha_val),
                'Exposure (Optional)': fmt_exp(exposure_val),
                '_alpha_sort': alpha_val if alpha_val is not None else -9999
            })
        
        if table_data:
            df_display = pd.DataFrame(table_data)
            
            # Sort by Alpha (descending) - default sorting
            df_display = df_display.sort_values('_alpha_sort', ascending=False)
            df_display = df_display.drop(columns=['_alpha_sort'])
            
            # Create two columns: 50% for table, 50% for alpha drivers
            col_table, col_drivers = st.columns([5, 5])
            
            # ================================================================
            # LEFT COLUMN: All Waves Table
            # ================================================================
            with col_table:
                st.markdown("### üìã All Waves")
                st.caption(f"Sorted by {selected_timeframe_label} Alpha (descending)")
                
                # Display table
                st.dataframe(
                    df_display,
                    use_container_width=True,
                    height=600,
                    hide_index=True
                )
            
            # ================================================================
            # RIGHT COLUMN: Alpha Drivers for Selected Wave
            # ================================================================
            with col_drivers:
                st.markdown("### üéØ Alpha Drivers")
                
                # Wave selector
                wave_names = [m['wave_name'] for m in all_metrics]
                
                # Default to wave with highest alpha in the selected timeframe
                if timeframe_days == 30:
                    valid_metrics = [m for m in all_metrics if m.get('alpha_30d') is not None]
                    if valid_metrics:
                        default_wave = max(valid_metrics, key=lambda x: x['alpha_30d'])['wave_name']
                    else:
                        default_wave = wave_names[0]
                elif timeframe_days == 60:
                    valid_metrics = [m for m in all_metrics if m.get('alpha_60d') is not None]
                    if valid_metrics:
                        default_wave = max(valid_metrics, key=lambda x: x['alpha_60d'])['wave_name']
                    else:
                        default_wave = wave_names[0]
                else:
                    valid_metrics = [m for m in all_metrics if m.get('alpha_365d') is not None]
                    if valid_metrics:
                        default_wave = max(valid_metrics, key=lambda x: x['alpha_365d'])['wave_name']
                    else:
                        default_wave = wave_names[0]
                
                selected_wave_name = st.selectbox(
                    label="Select a Wave:",
                    options=wave_names,
                    index=wave_names.index(default_wave) if default_wave in wave_names else 0,
                    key=k("AlphaCapture", "wave_selector")
                )
                
                st.markdown("---")
                
                # Compute Alpha Drivers for selected wave
                with st.spinner(f"Computing alpha drivers for {selected_wave_name}..."):
                    drivers = compute_alpha_drivers(
                        wave_name=selected_wave_name,
                        timeframe_days=timeframe_days
                    )
                
                # Display Total Alpha
                st.markdown("#### üìä Total Alpha")
                total_alpha_pct = drivers['total_alpha'] * 100
                st.metric(
                    label=f"Total Alpha ({selected_timeframe_label})",
                    value=f"{total_alpha_pct:.2f}%",
                    help="Wave Return minus Benchmark Return"
                )
                
                st.markdown("---")
                
                # Display Alpha Drivers Breakdown
                st.markdown("#### üîç Alpha Drivers Breakdown")
                st.caption("3-bucket percentage breakdown")
                
                # Check if we can display percentages
                if drivers['selection_percent'] is not None:
                    # Build drivers table
                    drivers_table_data = [
                        {
                            'Driver': 'üìà Stock Selection',
                            'Contribution (pts)': f"{drivers['selection_contribution']*100:.2f}%",
                            'Share': f"{drivers['selection_percent']:.1f}%"
                        },
                        {
                            'Driver': 'üõ°Ô∏è Risk Overlay',
                            'Contribution (pts)': f"{drivers['overlay_contribution']*100:.2f}%",
                            'Share': f"{drivers['overlay_percent']:.1f}%"
                        },
                        {
                            'Driver': '‚ö™ Residual/Other',
                            'Contribution (pts)': f"{drivers['residual_contribution']*100:.2f}%",
                            'Share': f"{drivers['residual_percent']:.1f}%"
                        }
                    ]
                    
                    df_drivers = pd.DataFrame(drivers_table_data)
                    
                    st.dataframe(
                        df_drivers,
                        use_container_width=True,
                        hide_index=True
                    )
                    
                    # Verification that shares sum to ~100%
                    total_share = drivers['selection_percent'] + drivers['overlay_percent'] + drivers['residual_percent']
                    st.caption(f"‚úì Total: {total_share:.1f}% (should be ~100%)")
                    
                else:
                    # Total alpha is effectively zero - display N/A
                    st.info("üìä Total Alpha is near zero. Share percentages: N/A")
                    
                    # Still show contributions in return points
                    drivers_table_data = [
                        {
                            'Driver': 'üìà Stock Selection',
                            'Contribution (pts)': f"{drivers['selection_contribution']*100:.2f}%",
                            'Share': 'N/A'
                        },
                        {
                            'Driver': 'üõ°Ô∏è Risk Overlay',
                            'Contribution (pts)': f"{drivers['overlay_contribution']*100:.2f}%",
                            'Share': 'N/A'
                        },
                        {
                            'Driver': '‚ö™ Residual/Other',
                            'Contribution (pts)': f"{drivers['residual_contribution']*100:.2f}%",
                            'Share': 'N/A'
                        }
                    ]
                    
                    df_drivers = pd.DataFrame(drivers_table_data)
                    
                    st.dataframe(
                        df_drivers,
                        use_container_width=True,
                        hide_index=True
                    )
                
                st.markdown("---")
                
                # Display additional metrics
                st.markdown("#### üìà Wave Metrics")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.metric(
                        label="Wave Return",
                        value=f"{drivers['wave_return']*100:.2f}%",
                        help=f"Total return over {selected_timeframe_label}"
                    )
                
                with col2:
                    st.metric(
                        label="Benchmark Return",
                        value=f"{drivers['benchmark_return']*100:.2f}%",
                        help=f"Benchmark return over {selected_timeframe_label}"
                    )
                
                # Show average exposure
                st.metric(
                    label="Avg Exposure",
                    value=f"{drivers['avg_exposure']:.2f}",
                    help="Average exposure over the period (1.0 = fully invested)"
                )
                
                # Data source indicator
                if drivers['has_diagnostics']:
                    st.caption("‚úì Using diagnostics data for overlay calculation")
                else:
                    st.caption("‚ÑπÔ∏è Using fallback method (diagnostics unavailable)")
                
                # Last updated timestamp
                if drivers['last_updated'] is not None:
                    st.caption(f"**Last Updated:** {drivers['last_updated'].strftime('%Y-%m-%d') if hasattr(drivers['last_updated'], 'strftime') else drivers['last_updated']}")
                else:
                    st.caption(f"**Last Updated:** {datetime.now().strftime('%Y-%m-%d')}")
        
        else:
            st.info("üìä No data available for display")
        
        # ========================================================================
        # COLLAPSIBLE SECTION: Method & Notes
        # ========================================================================
        st.markdown("---")
        
        with st.expander("üìö Method & Notes", expanded=False):
            st.markdown("### Understanding Alpha Drivers")
            
            st.markdown("""
            #### How We Calculate Alpha Drivers
            
            **Total Alpha** is calculated as:
            ```
            Total Alpha = Wave Return ‚àí Benchmark Return
            ```
            
            We then break down this alpha into three buckets:
            
            ---
            
            #### The 3-Bucket Breakdown
            
            **1. Stock Selection (Portfolio vs Benchmark)**
            - Alpha from choosing which assets to hold
            - Represents the value of stock picking and sector allocation
            - Calculated by comparing portfolio composition to benchmark
            
            **2. Risk Overlay (VIX/SafeSmart/Cash Shift)**
            - Alpha from dynamic risk management strategies
            - Includes VIX-driven exposure adjustments and safe asset allocation
            - Based on diagnostics data when available, otherwise uses exposure-based fallback
            
            **3. Residual/Other**
            - Alpha not attributed to the above two buckets
            - Should be close to 0% for transparency and accountability
            - May include rounding differences or untracked factors
            
            ---
            
            #### Computation Method
            
            **Preferred Method (when diagnostics available):**
            - Extract daily exposure and overlay activity from diagnostics
            - Calculate return with and without overlay effects
            - `Overlay Contribution = Return_with_overlay ‚àí Return_without_overlay`
            - `Selection Contribution = Total_Alpha ‚àí Overlay_Contribution`
            
            **Fallback Method (when diagnostics unavailable):**
            - Use exposure data from wave_history
            - `Risky Sleeve Return = Wave_Return / max(Exposure, 0.01)`
            - `Selection Alpha = Risky_Sleeve_Return ‚àí Benchmark_Return`
            - `Overlay Alpha = Total_Alpha ‚àí Selection_Alpha`
            
            **When exposure data is missing:**
            - `Overlay Alpha = 0`
            - `Selection Alpha = Total_Alpha`
            
            ---
            
            #### Percentage Shares
            
            If `Total Alpha ‚â† 0`:
            ```
            Share % = (Contribution / Total Alpha) √ó 100
            ```
            
            If `Total Alpha == 0`:
            - Display "N/A" for share percentages
            - Still show contributions in return points
            
            ---
            
            #### Important Notes
            
            **Data Sources:**
            - Wave returns and benchmark returns from wave_history.csv
            - Diagnostics from VIX overlay diagnostics module (when available)
            - Exposure data from wave_history.csv
            
            **Verification:**
            - All three buckets must sum to ~100%
            - Any deviation indicates calculation transparency
            - Residual bucket captures rounding and unattributed effects
            
            **Mobile-Friendly Design:**
            - Clean table-based layout
            - No HTML components
            - Responsive columns
            """)
        
        st.markdown("---")
        
    except Exception as e:
        st.error(f"‚ö†Ô∏è Error rendering Alpha Drivers tab: {str(e)}")
        with st.expander("üîç View Error Details"):
            st.code(traceback.format_exc(), language="python")


# ============================================================================
# SECTION 7.5: BOTTOM TICKER BAR FUNCTIONALITY
# ============================================================================

@st.cache_data(ttl=300)
def get_portfolio_tickers():
    """
    Get portfolio tickers from Master_Stock_Sheet.csv.
    Returns top tickers by weight or empty list if unavailable.
    """
    try:
        master_sheet_path = os.path.join(os.path.dirname(__file__), 'Master_Stock_Sheet.csv')
        
        if not os.path.exists(master_sheet_path):
            return []
        
        df = pd.read_csv(master_sheet_path)
        
        if df is None or len(df) == 0:
            return []
        
        # Get top tickers by weight
        if 'Ticker' in df.columns and 'Weight' in df.columns:
            # Sort by weight and get top 10
            df = df.sort_values('Weight', ascending=False)
            tickers = df.head(10)['Ticker'].dropna().tolist()
            return tickers
        elif 'Ticker' in df.columns:
            # Just get first 10 tickers
            tickers = df.head(10)['Ticker'].dropna().tolist()
            return tickers
        
        return []
        
    except Exception:
        return []


@st.cache_data(ttl=3600)
def get_next_earnings_date(ticker):
    """
    Get next earnings date for a ticker using yfinance (free API).
    Returns formatted date string or None if unavailable.
    """
    try:
        import yfinance as yf
        
        stock = yf.Ticker(ticker)
        calendar = stock.calendar
        
        if calendar is not None and not calendar.empty:
            # Get earnings date
            if 'Earnings Date' in calendar.index:
                earnings_date = calendar.loc['Earnings Date']
                if pd.notna(earnings_date) and hasattr(earnings_date, 'strftime'):
                    return earnings_date.strftime('%Y-%m-%d')
                elif isinstance(earnings_date, str):
                    return earnings_date
        
        return None
        
    except Exception:
        return None


@st.cache_data(ttl=86400)
def get_fed_decision_info():
    """
    Get next Federal Reserve decision date and current rate.
    Uses hardcoded schedule for 2024-2025 (no paid API required).
    Returns dict with 'next_date' and 'current_rate' or None values if unavailable.
    """
    try:
        # Federal Reserve FOMC meeting dates for 2024-2025
        # Source: federalreserve.gov (publicly available schedule)
        fomc_dates = [
            datetime(2024, 12, 17),
            datetime(2024, 12, 18),
            datetime(2025, 1, 28),
            datetime(2025, 1, 29),
            datetime(2025, 3, 18),
            datetime(2025, 3, 19),
            datetime(2025, 5, 6),
            datetime(2025, 5, 7),
            datetime(2025, 6, 17),
            datetime(2025, 6, 18),
            datetime(2025, 7, 29),
            datetime(2025, 7, 30),
            datetime(2025, 9, 16),
            datetime(2025, 9, 17),
            datetime(2025, 10, 28),
            datetime(2025, 10, 29),
            datetime(2025, 12, 9),
            datetime(2025, 12, 10),
        ]
        
        # Find next FOMC date after today
        now = datetime.now()
        next_date = None
        
        for date in fomc_dates:
            if date > now:
                next_date = date
                break
        
        # Current Federal Funds Rate (as of Dec 2024)
        # This is a static value - could be updated manually or via API
        current_rate = "4.25-4.50%"
        
        return {
            'next_date': next_date.strftime('%Y-%m-%d') if next_date else None,
            'current_rate': current_rate
        }
        
    except Exception:
        return {'next_date': None, 'current_rate': None}


# ============================================================================
# SECTION 7.4B: GOVERNANCE & AUDIT TAB
# ============================================================================

def render_governance_audit_tab():
    """
    Render the Governance & Audit tab.
    
    This tab provides comprehensive governance and audit information including:
    - Platform version and Git commit hash
    - Snapshot build timestamp and metadata
    - Data regime (LIVE/SANDBOX/HYBRID)
    - Wave registry version
    - Benchmark registry version
    - Safe Mode status
    - Last successful full snapshot
    - Count of degraded/partial waves
    - Count of broken tickers
    
    The panel is read-only and always loads properly.
    """
    st.markdown("# üõ°Ô∏è Governance & Audit")
    st.markdown("### Transparency and accountability layer for system state and data provenance")
    
    st.markdown("---")
    
    # ========================================================================
    # SECTION 1: System Identification
    # ========================================================================
    st.subheader("üîç System Identification")
    
    try:
        from governance_metadata import get_current_governance_info
        gov_info = get_current_governance_info()
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            platform_version = gov_info.get('platform_version', 'unknown')
            st.metric(
                "Platform Version",
                platform_version,
                help="Git commit hash of the current software version"
            )
            
            git_branch = gov_info.get('git_branch', 'unknown')
            st.metric(
                "Git Branch",
                git_branch,
                help="Current Git branch"
            )
        
        with col2:
            snapshot_time = gov_info.get('snapshot_timestamp')
            if snapshot_time:
                if isinstance(snapshot_time, str):
                    snapshot_display = snapshot_time
                else:
                    snapshot_display = snapshot_time.strftime('%Y-%m-%d %H:%M:%S')
            else:
                snapshot_display = 'Not available'
            
            st.metric(
                "Snapshot Build Time",
                snapshot_display,
                help="Timestamp of the last successful snapshot generation"
            )
        
        with col3:
            data_regime = gov_info.get('data_regime', 'UNKNOWN')
            
            # Color-code based on regime
            if data_regime == 'LIVE':
                regime_color = 'üü¢'
            elif data_regime == 'SANDBOX':
                regime_color = 'üü°'
            elif data_regime == 'HYBRID':
                regime_color = 'üü†'
            else:
                regime_color = '‚ö™'
            
            st.metric(
                "Data Regime",
                f"{regime_color} {data_regime}",
                help="LIVE = >80% full data | HYBRID = mixed data | SANDBOX = >70% partial/unavailable"
            )
        
        st.markdown("---")
        
        # ========================================================================
        # SECTION 2: Registry Versions
        # ========================================================================
        st.subheader("üìã Registry Versions")
        
        col1, col2 = st.columns(2)
        
        with col1:
            wave_registry_version = gov_info.get('wave_registry_version', 'unknown')
            st.metric(
                "Wave Registry Version",
                wave_registry_version,
                help="Version identifier for the wave registry configuration"
            )
        
        with col2:
            benchmark_version = gov_info.get('benchmark_registry_version', 'unknown')
            st.metric(
                "Benchmark Registry Version",
                benchmark_version,
                help="Version identifier for the benchmark configuration"
            )
        
        st.markdown("---")
        
        # ========================================================================
        # SECTION 3: System Status
        # ========================================================================
        st.subheader("‚öôÔ∏è System Status")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            safe_mode = gov_info.get('safe_mode_status', 'UNKNOWN')
            safe_mode_display = f"{'üî¥' if safe_mode == 'ON' else 'üü¢'} {safe_mode}"
            st.metric(
                "Safe Mode",
                safe_mode_display,
                help="Whether Safe Mode is currently active"
            )
        
        with col2:
            total_waves = gov_info.get('total_wave_count', 0)
            st.metric(
                "Total Waves",
                total_waves,
                help="Total number of waves in the registry"
            )
        
        with col3:
            degraded_count = gov_info.get('degraded_wave_count', 0)
            degraded_pct = (degraded_count / total_waves * 100) if total_waves > 0 else 0
            st.metric(
                "Degraded Waves",
                f"{degraded_count} ({degraded_pct:.1f}%)",
                help="Number of waves with partial or unavailable data"
            )
        
        with col4:
            broken_tickers = gov_info.get('broken_ticker_count', 0)
            st.metric(
                "Broken Tickers",
                broken_tickers,
                help="Number of tickers that failed to load or have data issues"
            )
        
        st.markdown("---")
        
        # ========================================================================
        # SECTION 4: Snapshot Metadata (if available)
        # ========================================================================
        st.subheader("üì∏ Current Snapshot Metadata")
        
        try:
            from snapshot_ledger import get_snapshot_metadata
            snapshot_metadata = get_snapshot_metadata()
            
            if snapshot_metadata.get('exists'):
                col1, col2 = st.columns(2)
                
                with col1:
                    # Snapshot ID (if available)
                    snapshot_id = snapshot_metadata.get('snapshot_id', 'N/A')
                    st.text_input(
                        "Snapshot ID",
                        snapshot_id,
                        disabled=True,
                        help="Unique identifier for this snapshot (immutable)"
                    )
                    
                    # Generation reason (if available)
                    generation_reason = snapshot_metadata.get('generation_reason', 'N/A')
                    st.text_input(
                        "Generation Reason",
                        generation_reason,
                        disabled=True,
                        help="Reason for snapshot generation (auto/manual/fallback)"
                    )
                
                with col2:
                    # Snapshot hash (if available)
                    snapshot_hash = snapshot_metadata.get('snapshot_hash', 'N/A')
                    st.text_input(
                        "Snapshot Hash",
                        snapshot_hash,
                        disabled=True,
                        help="Content hash for snapshot integrity verification"
                    )
                    
                    # Age
                    age_hours = snapshot_metadata.get('age_hours', 0)
                    is_stale = snapshot_metadata.get('is_stale', True)
                    age_display = f"{age_hours:.1f}h {'(STALE)' if is_stale else '(FRESH)'}"
                    st.text_input(
                        "Snapshot Age",
                        age_display,
                        disabled=True,
                        help="Time since snapshot was last generated"
                    )
                
                # Show full metadata in expander
                with st.expander("üîç View Full Snapshot Metadata", expanded=False):
                    st.json(snapshot_metadata)
            else:
                st.warning("‚ö†Ô∏è No snapshot metadata available. Snapshot may not have been generated yet.")
                
                if st.button("üîÑ Generate Snapshot Now", help="Manually trigger snapshot generation"):
                    with st.spinner("Generating snapshot..."):
                        try:
                            from snapshot_ledger import generate_snapshot
                            generate_snapshot(force_refresh=True, generation_reason='manual')
                            st.success("‚úÖ Snapshot generated successfully!")
                            # Mark user interaction
                            st.session_state.user_interaction_detected = True
                            trigger_rerun("planb_snapshot_generated")
                        except Exception as e:
                            st.error(f"‚ùå Failed to generate snapshot: {str(e)}")
                            with st.expander("üîç View Error Details"):
                                st.code(traceback.format_exc(), language="python")
        
        except Exception as e:
            st.error(f"‚ö†Ô∏è Error loading snapshot metadata: {str(e)}")
            with st.expander("üîç View Error Details"):
                st.code(traceback.format_exc(), language="python")
        
        st.markdown("---")
        
        # ========================================================================
        # SECTION 5: Governance Notes
        # ========================================================================
        st.subheader("üìù Governance Notes")
        
        st.info("""
        **What am I looking at?**
        
        This Governance & Audit panel provides complete transparency into the state 
        of your Waves-Simple application:
        
        - **Platform Version**: Identifies the exact software version running
        - **Data Regime**: Shows whether you're looking at live, sandbox, or hybrid data
        - **Registry Versions**: Tracks changes to wave and benchmark configurations
        - **System Status**: Monitors safe mode, degraded waves, and broken tickers
        - **Snapshot Metadata**: Provides immutable identification and provenance tracking
        
        **Key Benefits:**
        - Answer "What am I looking at?" in 10 seconds
        - No ambiguity about data state or provenance
        - Every metric traceable to a specific snapshot
        - Immutable snapshots prevent silent data overwrites
        
        **For Managers:** Understand system health at a glance  
        **For Investors:** Verify data quality and regime  
        **For Developers:** Debug issues with precise version tracking
        """)
        
    except Exception as e:
        st.error(f"‚ö†Ô∏è Error rendering Governance & Audit tab: {str(e)}")
        st.warning("The Governance & Audit panel could not load properly. Some modules may not be available.")
        with st.expander("üîç View Error Details"):
            st.code(traceback.format_exc(), language="python")


# ============================================================================
# SECTION 7.5: DIAGNOSTICS TAB
# ============================================================================

def render_wave_monitor_tab():
    """
    ROUND 7 Phase 5: Wave Monitor Tab
    
    Render the Wave Monitor tab with:
    - Dropdown to choose individual waves
    - Context strip showing wave name, readiness, exposure, cash
    - Static metrics table with returns, alphas, coverage
    - Toggle between "Show ALL vs Operational"
    
    This tab ensures meaningful rendering even with no live data downloads.
    """
    st.markdown("# üåä Wave Monitor")
    st.markdown("### Individual Wave Analytics and Diagnostics")
    
    st.markdown("---")
    
    # ========================================================================
    # SECTION 1: Wave Selector
    # ========================================================================
    from waves_engine import get_all_waves_universe, get_wave_id_from_display_name
    from analytics_pipeline import resolve_wave_metrics
    
    try:
        universe = get_all_waves_universe()
        all_waves = universe.get('waves', [])
        
        # Filter toggle
        col_toggle1, col_toggle2 = st.columns([3, 1])
        
        with col_toggle2:
            show_all = st.checkbox("Show All Waves", value=True, key="wave_monitor_show_all")
        
        # If not showing all, filter to operational+ waves only
        if not show_all:
            # Get wave statuses
            operational_waves = []
            for wave_name in all_waves:
                wave_id = get_wave_id_from_display_name(wave_name)
                if wave_id:
                    metrics = resolve_wave_metrics(wave_id, mode="Standard")
                    readiness = metrics.get('readiness_status', 'unavailable')
                    if readiness in ['operational', 'partial', 'full']:
                        operational_waves.append(wave_name)
            waves_to_show = operational_waves if operational_waves else all_waves
        else:
            waves_to_show = all_waves
        
        with col_toggle1:
            selected_wave = st.selectbox(
                "Select Wave",
                options=waves_to_show,
                key="wave_monitor_selected_wave",
                help=f"Showing {len(waves_to_show)} of {len(all_waves)} waves"
            )
        
        if not selected_wave:
            st.info("Please select a wave to view its metrics.")
            return
        
        # Get wave_id and metrics
        wave_id = get_wave_id_from_display_name(selected_wave)
        if not wave_id:
            st.error(f"Could not find wave_id for {selected_wave}")
            return
        
        metrics = resolve_wave_metrics(wave_id, mode="Standard")
        
        st.markdown("---")
        
        # ========================================================================
        # SECTION 2: Context Strip
        # ========================================================================
        st.subheader("üìä Wave Context")
        
        col1, col2, col3, col4, col5 = st.columns(5)
        
        with col1:
            st.metric("Wave", selected_wave.replace(" Wave", ""))
        
        with col2:
            readiness = metrics.get('readiness_status', 'unavailable')
            readiness_emoji = {
                'full': 'üü¢',
                'partial': 'üü°',
                'operational': 'üü†',
                'unavailable': 'üî¥'
            }.get(readiness, '‚ö™')
            st.metric("Readiness", f"{readiness_emoji} {readiness.title()}")
        
        with col3:
            exposure = metrics.get('exposure_pct', 1.0)
            exposure_pct = exposure * 100 if exposure is not None else 100.0
            st.metric("Exposure", f"{exposure_pct:.1f}%")
        
        with col4:
            cash = metrics.get('cash_pct', 0.0)
            cash_pct = cash * 100 if cash is not None else 0.0
            st.metric("Cash", f"{cash_pct:.1f}%")
        
        with col5:
            # Show snapshot timestamp if available
            snapshot_age = st.session_state.get('snapshot_age_minutes')
            if snapshot_age is not None and snapshot_age < 60:
                freshness_text = f"{snapshot_age:.0f}m"
            elif snapshot_age is not None and snapshot_age < 1440:
                freshness_text = f"{snapshot_age/60:.1f}h"
            else:
                freshness_text = "Stale"
            st.metric("Data Age", freshness_text)
        
        st.markdown("---")
        
        # ========================================================================
        # SECTION 3: Returns & Alpha Table
        # ========================================================================
        st.subheader("üìà Returns & Alpha Analysis")
        
        # Build metrics table
        timeframes = [
            ('1D', '1d'),
            ('30D', '30d'),
            ('60D', '60d'),
            ('365D', '365d')
        ]
        
        table_data = []
        for label, key in timeframes:
            wave_return = metrics.get(f'return_{key}')
            bm_return = metrics.get(f'benchmark_return_{key}')
            alpha = metrics.get(f'alpha_{key}')
            
            # Format values
            wave_return_str = f"{wave_return*100:.2f}%" if wave_return is not None else "N/A"
            bm_return_str = f"{bm_return*100:.2f}%" if bm_return is not None else "N/A"
            alpha_str = f"{alpha*100:.2f}%" if alpha is not None else "N/A"
            
            table_data.append({
                'Timeframe': label,
                'Wave Return': wave_return_str,
                'Benchmark Return': bm_return_str,
                'Alpha': alpha_str
            })
        
        import pandas as pd
        metrics_df = pd.DataFrame(table_data)
        st.dataframe(metrics_df, use_container_width=True, hide_index=True)
        
        st.markdown("---")
        
        # ========================================================================
        # SECTION 4: Coverage & Readiness Diagnostics
        # ========================================================================
        st.subheader("üîç Coverage & Readiness Diagnostics")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            coverage_pct = metrics.get('coverage_pct', 0.0)
            st.metric("Coverage %", f"{coverage_pct:.1f}%")
        
        with col2:
            coverage_score = metrics.get('coverage_score', 0)
            st.metric("Coverage Score", coverage_score)
        
        with col3:
            has_prices = metrics.get('has_prices', False)
            prices_status = "‚úÖ Yes" if has_prices else "‚ùå No"
            st.metric("Has Prices", prices_status)
        
        with col4:
            has_benchmark = metrics.get('has_benchmark', False)
            benchmark_status = "‚úÖ Yes" if has_benchmark else "‚ùå No"
            st.metric("Has Benchmark", benchmark_status)
        
        # ========================================================================
        # SECTION 5: Alerts & Degradation Info
        # ========================================================================
        alerts = metrics.get('alerts', [])
        degradation_cause = metrics.get('degradation_cause')
        source = metrics.get('source', 'unknown')
        
        if alerts or degradation_cause:
            st.markdown("---")
            st.subheader("‚ö†Ô∏è Alerts & Diagnostics")
            
            # Show data source
            source_emoji = {
                'pipeline': 'üü¢',
                'snapshot': 'üü°',
                'degraded': 'üî¥'
            }.get(source, '‚ö™')
            st.info(f"**Data Source:** {source_emoji} {source.title()}")
            
            if degradation_cause:
                st.warning(f"**Degradation Cause:** {degradation_cause}")
            
            if alerts:
                with st.expander(f"üìã View Alerts ({len(alerts)})", expanded=False):
                    for i, alert in enumerate(alerts, 1):
                        st.text(f"{i}. {alert}")
        
    except Exception as e:
        st.error(f"Error rendering Wave Monitor tab: {str(e)}")
        import traceback
        with st.expander("View Error Details", expanded=False):
            st.code(traceback.format_exc(), language="python")


def render_planb_monitor_tab():
    """
    Render the Plan B Monitor tab.
    
    This tab provides:
    - Canonical metrics table for all 28 waves
    - Completely decoupled from live ticker dependencies
    - Graceful degradation with status flags
    - Wave selector with diagnostics
    - No blockers or indefinite retries
    """
    try:
        from planb_pipeline import build_planb_snapshot, get_planb_diagnostics, check_planb_files
        
        st.markdown("# üìä Plan B Monitor")
        st.markdown("### Canonical Metrics - Fully Decoupled Analytics")
        
        st.markdown("---")
        
        # ========================================================================
        # SECTION 1: Status Banner
        # ========================================================================
        
        # Check file status
        file_status = check_planb_files()
        all_files_exist = all(file_status.values())
        
        if not all_files_exist:
            missing_files = [k for k, v in file_status.items() if not v]
            st.warning(f"‚ö†Ô∏è **Plan B Data Incomplete:** Missing files: {', '.join(missing_files)}.csv - All waves marked UNAVAILABLE until data is populated.")
        
        # ========================================================================
        # SECTION 2: Snapshot Controls
        # ========================================================================
        
        st.subheader("‚öôÔ∏è Snapshot Controls")
        
        col1, col2, col3 = st.columns([2, 2, 3])
        
        with col1:
            # Days selector
            days_options = [30, 60, 90, 180, 365]
            selected_days = st.selectbox("History Window", days_options, index=4, help="Number of days of history to use for calculations")
        
        with col2:
            # Refresh button
            if st.button("üîÑ Refresh Snapshot", help="Rebuild the Plan B snapshot with current data"):
                st.session_state.planb_snapshot_cache_bust = datetime.now()
                # Mark user interaction
                st.session_state.user_interaction_detected = True
                trigger_rerun("planb_snapshot_cache_bust")
        
        with col3:
            # Timestamp display
            snapshot_timestamp = st.session_state.get('planb_snapshot_timestamp', datetime.now())
            st.info(f"üìÖ Snapshot Age: {(datetime.now() - snapshot_timestamp).total_seconds() / 60:.1f} minutes")
        
        st.markdown("---")
        
        # ========================================================================
        # SECTION 3: Build Snapshot
        # ========================================================================
        
        with st.spinner("Building Plan B snapshot..."):
            snapshot_df = build_planb_snapshot(days=selected_days)
            st.session_state.planb_snapshot_timestamp = datetime.now()
        
        # ========================================================================
        # SECTION 4: Summary Metrics
        # ========================================================================
        
        st.subheader("üìà Summary Metrics")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            status_counts = snapshot_df['status'].value_counts()
            full_count = status_counts.get('FULL', 0)
            st.metric("FULL Status", f"{full_count}/28", delta=None, help="Waves with complete data")
        
        with col2:
            partial_count = status_counts.get('PARTIAL', 0)
            st.metric("PARTIAL Status", f"{partial_count}/28", delta=None, help="Waves with partial data")
        
        with col3:
            unavailable_count = status_counts.get('UNAVAILABLE', 0)
            st.metric("UNAVAILABLE", f"{unavailable_count}/28", delta=None, help="Waves with no data")
        
        with col4:
            # Count waves with alerts
            alerts_count = snapshot_df['alerts'].apply(lambda x: len(x) > 0).sum()
            st.metric("Alerts", f"{alerts_count}", delta=None, help="Waves with active alerts")
        
        st.markdown("---")
        
        # ========================================================================
        # SECTION 5: Dynamic Table View
        # ========================================================================
        
        st.subheader("üìä All Waves Snapshot (28 Waves)")
        
        # Prepare display DataFrame
        display_df = snapshot_df.copy()
        
        # Format percentage columns
        pct_cols = [
            'return_1d', 'return_30d', 'return_60d', 'return_365d',
            'bm_return_1d', 'bm_return_30d', 'bm_return_60d', 'bm_return_365d',
            'alpha_1d', 'alpha_30d', 'alpha_60d', 'alpha_365d',
            'exposure_pct', 'cash_pct', 'vol_365d', 'maxdd_365d', 'turnover_30d'
        ]
        
        for col in pct_cols:
            if col in display_df.columns:
                display_df[col] = display_df[col].apply(
                    lambda x: f"{x:.2f}%" if pd.notna(x) else "N/A"
                )
        
        # Format beta
        if 'beta_est' in display_df.columns:
            display_df['beta_est'] = display_df['beta_est'].apply(
                lambda x: f"{x:.2f}" if pd.notna(x) else "N/A"
            )
        
        # Format NAV
        if 'nav_latest' in display_df.columns:
            display_df['nav_latest'] = display_df['nav_latest'].apply(
                lambda x: f"${x:,.2f}" if pd.notna(x) else "N/A"
            )
        
        # Format alerts
        if 'alerts' in display_df.columns:
            display_df['alerts'] = display_df['alerts'].apply(
                lambda x: f"{len(x)} alerts" if len(x) > 0 else "‚úÖ"
            )
        
        # Select columns for display
        display_columns = [
            'display_name', 'status', 'nav_latest',
            'return_1d', 'return_30d', 'return_365d',
            'alpha_1d', 'alpha_30d', 'alpha_365d',
            'exposure_pct', 'cash_pct', 'beta_est',
            'vol_365d', 'maxdd_365d', 'alerts'
        ]
        
        # Filter to available columns
        display_columns = [c for c in display_columns if c in display_df.columns]
        
        # Show table with color coding
        st.dataframe(
            display_df[display_columns],
            use_container_width=True,
            hide_index=True,
            height=600
        )
        
        st.markdown("---")
        
        # ========================================================================
        # SECTION 6: Wave Selector with Diagnostics
        # ========================================================================
        
        st.subheader("üîç Wave Diagnostics")
        
        # Wave selector
        selected_wave_name = st.selectbox(
            "Select Wave for Detailed Diagnostics",
            options=snapshot_df['display_name'].tolist(),
            help="Choose a wave to view detailed diagnostics"
        )
        
        # Get selected wave data
        wave_data = snapshot_df[snapshot_df['display_name'] == selected_wave_name].iloc[0]
        
        # Display diagnostics in expandable section
        with st.expander(f"üìã Diagnostics for {selected_wave_name}", expanded=True):
            
            # Status and reason
            col1, col2 = st.columns(2)
            
            with col1:
                status = wave_data['status']
                status_color = {
                    'FULL': 'üü¢',
                    'PARTIAL': 'üü°',
                    'UNAVAILABLE': 'üî¥'
                }.get(status, '‚ö™')
                
                st.markdown(f"**Status:** {status_color} {status}")
                st.markdown(f"**Wave ID:** `{wave_data['wave_id']}`")
                st.markdown(f"**Mode:** {wave_data['mode']}")
            
            with col2:
                st.markdown(f"**Latest NAV:** {display_df[display_df['display_name'] == selected_wave_name]['nav_latest'].iloc[0]}")
                st.markdown(f"**365D Return:** {display_df[display_df['display_name'] == selected_wave_name]['return_365d'].iloc[0]}")
                st.markdown(f"**365D Alpha:** {display_df[display_df['display_name'] == selected_wave_name]['alpha_365d'].iloc[0]}")
            
            # Reason for degradation
            if wave_data['reason']:
                st.warning(f"**Degradation Reason:** {wave_data['reason']}")
            
            # Alerts
            if len(wave_data['alerts']) > 0:
                st.error("**Active Alerts:**")
                for alert in wave_data['alerts']:
                    st.markdown(f"- {alert}")
            else:
                st.success("‚úÖ No active alerts")
            
            # Detailed metrics
            st.markdown("---")
            st.markdown("**Detailed Metrics:**")
            
            metrics_col1, metrics_col2, metrics_col3 = st.columns(3)
            
            with metrics_col1:
                st.markdown("**Returns:**")
                st.markdown(f"- 1D: {display_df[display_df['display_name'] == selected_wave_name]['return_1d'].iloc[0]}")
                st.markdown(f"- 30D: {display_df[display_df['display_name'] == selected_wave_name]['return_30d'].iloc[0]}")
                st.markdown(f"- 60D: {display_df[display_df['display_name'] == selected_wave_name]['return_60d'].iloc[0]}")
                st.markdown(f"- 365D: {display_df[display_df['display_name'] == selected_wave_name]['return_365d'].iloc[0]}")
            
            with metrics_col2:
                st.markdown("**Alpha vs Benchmark:**")
                st.markdown(f"- 1D: {display_df[display_df['display_name'] == selected_wave_name]['alpha_1d'].iloc[0]}")
                st.markdown(f"- 30D: {display_df[display_df['display_name'] == selected_wave_name]['alpha_30d'].iloc[0]}")
                st.markdown(f"- 60D: {display_df[display_df['display_name'] == selected_wave_name]['alpha_60d'].iloc[0]}")
                st.markdown(f"- 365D: {display_df[display_df['display_name'] == selected_wave_name]['alpha_365d'].iloc[0]}")
            
            with metrics_col3:
                st.markdown("**Risk Metrics:**")
                st.markdown(f"- Beta: {display_df[display_df['display_name'] == selected_wave_name]['beta_est'].iloc[0]}")
                st.markdown(f"- Volatility: {display_df[display_df['display_name'] == selected_wave_name]['vol_365d'].iloc[0]}")
                st.markdown(f"- Max Drawdown: {display_df[display_df['display_name'] == selected_wave_name]['maxdd_365d'].iloc[0]}")
                st.markdown(f"- Exposure: {display_df[display_df['display_name'] == selected_wave_name]['exposure_pct'].iloc[0]}")
        
        st.markdown("---")
        
        # ========================================================================
        # SECTION 7: System Diagnostics
        # ========================================================================
        
        st.subheader("üîß System Diagnostics")
        
        diagnostics = get_planb_diagnostics()
        
        with st.expander("üìã View System Diagnostics", expanded=False):
            st.markdown(f"**Timestamp:** {diagnostics['timestamp']}")
            st.markdown(f"**All Files Exist:** {'‚úÖ Yes' if diagnostics['all_files_exist'] else '‚ùå No'}")
            
            if diagnostics['missing_files']:
                st.warning(f"**Missing Files:** {', '.join(diagnostics['missing_files'])}")
            
            st.markdown("---")
            st.markdown("**File Information:**")
            
            for file_key, file_info in diagnostics.get('file_info', {}).items():
                if file_info.get('exists'):
                    if 'error' in file_info:
                        st.error(f"- **{file_key}.csv:** Error loading - {file_info['error']}")
                    else:
                        st.markdown(f"- **{file_key}.csv:** {file_info.get('row_count', 0)} rows, {file_info.get('size_bytes', 0)} bytes")
                else:
                    st.error(f"- **{file_key}.csv:** ‚ùå Missing")
    
    except ImportError:
        st.error("‚ö†Ô∏è **Plan B Monitor Unavailable:** The `planb_pipeline` module is not available.")
        st.info("To enable this feature, ensure `planb_pipeline.py` is present in the application directory.")
    
    except Exception as e:
        st.error(f"Error rendering Plan B Monitor tab: {str(e)}")
        import traceback
        with st.expander("View Error Details", expanded=False):
            st.code(traceback.format_exc(), language="python")


def render_wave_intelligence_planb_tab():
    """
    Render the Wave Intelligence (Plan B) tab.
    
    This tab provides:
    - Proxy-based analytics for all 28 waves
    - Safe Mode toggle for snapshot-only rendering
    - Build control with 2-minute lock
    - Diagnostics box
    - Snapshot-first rendering (no blocking)
    - Wave selector dropdown
    - Wave Identity fact sheet with proxy returns and diagnostics
    - Universe Table with general stats for all waves
    """
    try:
        from planb_proxy_pipeline import (
            load_proxy_snapshot, 
            get_snapshot_freshness, 
            build_proxy_snapshot,
            should_trigger_build,
            load_diagnostics,
            BUILD_LOCK_MINUTES
        )
        from helpers.proxy_registry_validator import validate_proxy_registry, get_enabled_proxy_waves
        
        st.markdown("# üìä Wave Intelligence (Plan B)")
        st.markdown("### Proxy-Based Analytics - Consistent Data for All 28 Waves")
        
        st.markdown("---")
        
        # ========================================================================
        # SECTION 0: Initialize Session State
        # ========================================================================
        
        # Initialize session state variables for build control
        if 'planb_build_in_progress' not in st.session_state:
            st.session_state.planb_build_in_progress = False
        
        if 'planb_last_build_attempt' not in st.session_state:
            st.session_state.planb_last_build_attempt = None
        
        if 'planb_safe_mode' not in st.session_state:
            st.session_state.planb_safe_mode = False
        
        # ========================================================================
        # SECTION 1: Load Snapshot First (Snapshot-First Rendering)
        # ========================================================================
        
        # ALWAYS load existing snapshot first for immediate rendering
        snapshot_df = load_proxy_snapshot()
        freshness = get_snapshot_freshness()
        diagnostics = load_diagnostics()
        
        # ========================================================================
        # SECTION 2: Diagnostics Box
        # ========================================================================
        
        st.subheader("üìã Snapshot Diagnostics")
        
        diag_col1, diag_col2, diag_col3, diag_col4 = st.columns(4)
        
        with diag_col1:
            if freshness['exists']:
                age_min = freshness.get('age_minutes', 0)
                st.metric("Snapshot Age", f"{age_min:.1f}m")
            else:
                st.metric("Snapshot Age", "N/A")
        
        with diag_col2:
            if freshness['exists']:
                status = "üü¢ Fresh" if freshness['fresh'] else "üü° Stale"
                st.metric("Freshness", status)
            else:
                st.metric("Freshness", "üî¥ Missing")
        
        with diag_col3:
            if diagnostics:
                build_status = "‚úÖ Success" if diagnostics.get('snapshot_saved', False) else "‚ö†Ô∏è Partial"
                if diagnostics.get('timeout_exceeded', False):
                    build_status = "‚è±Ô∏è Timeout"
                st.metric("Last Build", build_status)
            else:
                st.metric("Last Build", "N/A")
        
        with diag_col4:
            if diagnostics:
                failed_count = diagnostics.get('failed_fetches', 0)
                st.metric("Failed Tickers", failed_count)
            else:
                st.metric("Failed Tickers", "N/A")
        
        # Build lock status
        if st.session_state.planb_last_build_attempt:
            minutes_since = (datetime.now() - st.session_state.planb_last_build_attempt).total_seconds() / 60
            if minutes_since < BUILD_LOCK_MINUTES:
                st.warning(f"‚è±Ô∏è **Build Lock Active:** Last build {minutes_since:.1f}m ago. Must wait {BUILD_LOCK_MINUTES - minutes_since:.1f}m more.")
        
        # STEP 6: Show build in progress / suppressed status
        try:
            from helpers.compute_gate import get_build_diagnostics
            
            planb_diag = get_build_diagnostics(st.session_state, "planb_snapshot")
            engine_diag = get_build_diagnostics(st.session_state, "engine_snapshot")
            
            if planb_diag.get('build_in_progress', False):
                st.info("üîÑ **Plan B Build Running** - Please wait...")
            elif engine_diag.get('build_in_progress', False):
                st.info("üîÑ **Engine Build Running** - Please wait...")
            elif st.session_state.get('safe_demo_mode', False):
                st.success("üõ°Ô∏è **SAFE DEMO MODE** - All builds suppressed")
        except ImportError:
            pass
        
        st.markdown("---")
        
        # ========================================================================
        # SECTION 3: Safe Mode & Action Buttons
        # ========================================================================
        
        col1, col2, col3, col4 = st.columns([2, 2, 2, 3])
        
        with col1:
            # Safe Mode Toggle
            safe_mode = st.checkbox(
                "üõ°Ô∏è Safe Mode (Snapshot Only)",
                value=st.session_state.planb_safe_mode,
                help="When enabled, prevents all online fetches and only renders from the latest snapshot"
            )
            st.session_state.planb_safe_mode = safe_mode
        
        with col2:
            # Rebuild button (disabled in Safe Mode)
            rebuild_disabled = safe_mode or st.session_state.planb_build_in_progress
            
            if st.button("üîÑ Rebuild Snapshot Now", disabled=rebuild_disabled, help="Fetch latest proxy data and rebuild snapshot (disabled in Safe Mode)"):
                # Mark button click for run tracking
                st.session_state._last_button_clicked = "planb_rebuild"
                
                # Mark build in progress
                st.session_state.planb_build_in_progress = True
                st.session_state.planb_last_build_attempt = datetime.now()
                
                with st.spinner("Rebuilding proxy snapshot (max 15s timeout)..."):
                    try:
                        snapshot_df = build_proxy_snapshot(
                            days=365, 
                            enforce_timeout=True,
                            session_state=st.session_state,
                            explicit_button_click=True  # User explicitly clicked button
                        )
                        
                        if not snapshot_df.empty:
                            st.success(f"‚úÖ Snapshot rebuilt with {len(snapshot_df)} waves")
                        else:
                            st.warning(f"‚ö†Ô∏è Build completed but returned empty snapshot")
                        
                        st.session_state.planb_build_in_progress = False
                        # Mark user interaction
                        st.session_state.user_interaction_detected = True
                        trigger_rerun("planb_build_complete")
                    except Exception as e:
                        st.error(f"Failed to rebuild snapshot: {str(e)}")
                        st.session_state.planb_build_in_progress = False
        
        with col3:
            # Download button
            if not snapshot_df.empty:
                csv_data = snapshot_df.to_csv(index=False)
                st.download_button(
                    label="üì• Download CSV",
                    data=csv_data,
                    file_name=f"wave_proxy_snapshot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv",
                    help="Download the current proxy snapshot as CSV"
                )
        
        with col4:
            # Registry validation status
            validation = validate_proxy_registry(strict=False)
            if validation['valid']:
                st.info(f"‚úÖ Registry: {validation['enabled_count']}/28 waves enabled")
            else:
                st.warning(f"‚ö†Ô∏è Registry: {validation['enabled_count']}/28 waves (degraded)")
        
        # Safe Mode banner
        if safe_mode:
            st.info("üõ°Ô∏è **Safe Mode Active:** Rendering from snapshot only. No online fetches will be performed.")
        
        # ========================================================================
        # STEP 8: Diagnostics - Why is it rerunning?
        # ========================================================================
        with st.expander("üîç Diagnostics: Why is it rerunning?", expanded=False):
            st.markdown("### Rerun & Build Diagnostics")
            
            # Import compute gate for diagnostics
            try:
                from helpers.compute_gate import get_build_diagnostics
                
                diag_col1, diag_col2 = st.columns(2)
                
                with diag_col1:
                    st.markdown("#### App Rerun Status")
                    st.markdown(f"**Run ID:** {st.session_state.get('run_id', 'N/A')}")
                    st.markdown(f"**Trigger:** {st.session_state.get('run_trigger', 'unknown')}")
                    st.markdown(f"**SAFE DEMO MODE:** {'üü¢ ON' if st.session_state.get('safe_demo_mode', False) else 'üî¥ OFF'}")
                    st.markdown(f"**Plan B Safe Mode:** {'üü¢ ON' if safe_mode else 'üî¥ OFF'}")
                
                with diag_col2:
                    st.markdown("#### Plan B Build Status")
                    planb_diag = get_build_diagnostics(st.session_state, "planb_snapshot")
                    
                    st.markdown(f"**Build In Progress:** {planb_diag.get('build_in_progress', False)}")
                    st.markdown(f"**Last Build Attempt:** {planb_diag.get('last_build_attempt', 'Never')}")
                    st.markdown(f"**Last Build Success:** {planb_diag.get('last_build_success', 'N/A')}")
                    st.markdown(f"**Minutes Since Last:** {planb_diag.get('minutes_since_last_attempt', 'N/A')}")
                    st.markdown(f"**Last Build Run ID:** {planb_diag.get('last_build_run_id', 'N/A')}")
                
                # Engine build diagnostics
                st.markdown("#### Engine Build Status")
                engine_diag = get_build_diagnostics(st.session_state, "engine_snapshot")
                
                diag_col3, diag_col4 = st.columns(2)
                
                with diag_col3:
                    st.markdown(f"**Build In Progress:** {engine_diag.get('build_in_progress', False)}")
                    st.markdown(f"**Last Build Attempt:** {engine_diag.get('last_build_attempt', 'Never')}")
                    st.markdown(f"**Last Build Success:** {engine_diag.get('last_build_success', 'N/A')}")
                
                with diag_col4:
                    st.markdown(f"**Minutes Since Last:** {engine_diag.get('minutes_since_last_attempt', 'N/A')}")
                    st.markdown(f"**Last Build Run ID:** {engine_diag.get('last_build_run_id', 'N/A')}")
                
                # Show ticker counts from diagnostics
                if diagnostics:
                    st.markdown("#### Last Build Summary")
                    st.markdown(f"**Total Waves Processed:** {diagnostics.get('total_waves', 'N/A')}")
                    st.markdown(f"**Successful Fetches:** {diagnostics.get('successful_fetches', 'N/A')}")
                    st.markdown(f"**Failed Fetches:** {diagnostics.get('failed_fetches', 'N/A')}")
                    st.markdown(f"**Build Duration:** {diagnostics.get('build_duration_seconds', 'N/A')}s")
                    st.markdown(f"**Timeout Exceeded:** {diagnostics.get('timeout_exceeded', False)}")
                
            except ImportError:
                st.warning("Compute gate diagnostics not available")
        
        st.markdown("---")
        
        # ========================================================================
        # SECTION 4: Check if snapshot is available
        # ========================================================================
        
        if snapshot_df.empty:
            st.warning("‚ö†Ô∏è No proxy snapshot available. Click 'Rebuild Snapshot Now' to generate.")
            st.info("The snapshot is required to display the 28-Wave Universe Table and individual wave analytics.")
            return
        
        # ========================================================================
        # SECTION 4: Wave Selector
        # ========================================================================
        
        st.subheader("üéØ Wave Selector")
        
        # Get list of waves
        wave_options = snapshot_df['display_name'].tolist()
        wave_ids = snapshot_df['wave_id'].tolist()
        
        # Create wave selector
        selected_wave_name = st.selectbox(
            "Select Wave",
            wave_options,
            index=0,
            help="Choose a wave to view detailed proxy analytics"
        )
        
        # Get selected wave data
        selected_wave_idx = wave_options.index(selected_wave_name)
        selected_wave_id = wave_ids[selected_wave_idx]
        selected_wave_data = snapshot_df[snapshot_df['wave_id'] == selected_wave_id].iloc[0]
        
        st.markdown("---")
        
        # ========================================================================
        # SECTION 5: Wave Identity Fact Sheet
        # ========================================================================
        
        st.subheader("üìã Wave Identity")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown(f"**Wave Name:** {selected_wave_data['display_name']}")
            st.markdown(f"**Wave ID:** `{selected_wave_data['wave_id']}`")
            st.markdown(f"**Category:** {selected_wave_data['category']}")
        
        with col2:
            st.markdown(f"**Proxy Ticker:** {selected_wave_data['proxy_ticker']}")
            st.markdown(f"**Benchmark:** {selected_wave_data['benchmark_ticker']}")
            confidence = selected_wave_data['confidence']
            confidence_emoji = "üü¢" if confidence == "FULL" else "üü°" if confidence == "PARTIAL" else "üî¥"
            st.markdown(f"**Confidence:** {confidence_emoji} {confidence}")
        
        with col3:
            # Returns summary
            ret_1d = selected_wave_data['return_1D']
            ret_30d = selected_wave_data['return_30D']
            ret_365d = selected_wave_data['return_365D']
            
            if pd.notna(ret_1d):
                st.metric("1D Return", f"{ret_1d*100:.2f}%")
            else:
                st.metric("1D Return", "N/A")
            
            if pd.notna(ret_30d):
                st.metric("30D Return", f"{ret_30d*100:.2f}%")
            else:
                st.metric("30D Return", "N/A")
        
        st.markdown("---")
        
        # ========================================================================
        # SECTION 6: Proxy Returns & Diagnostics
        # ========================================================================
        
        st.subheader("üìà Proxy Returns & Alpha")
        
        # Create returns table
        returns_data = {
            'Period': ['1 Day', '30 Days', '60 Days', '365 Days'],
            'Proxy Return': [
                f"{selected_wave_data['return_1D']*100:.2f}%" if pd.notna(selected_wave_data['return_1D']) else "N/A",
                f"{selected_wave_data['return_30D']*100:.2f}%" if pd.notna(selected_wave_data['return_30D']) else "N/A",
                f"{selected_wave_data['return_60D']*100:.2f}%" if pd.notna(selected_wave_data['return_60D']) else "N/A",
                f"{selected_wave_data['return_365D']*100:.2f}%" if pd.notna(selected_wave_data['return_365D']) else "N/A"
            ],
            'Benchmark Return': [
                f"{selected_wave_data['benchmark_1D']*100:.2f}%" if pd.notna(selected_wave_data['benchmark_1D']) else "N/A",
                f"{selected_wave_data['benchmark_30D']*100:.2f}%" if pd.notna(selected_wave_data['benchmark_30D']) else "N/A",
                f"{selected_wave_data['benchmark_60D']*100:.2f}%" if pd.notna(selected_wave_data['benchmark_60D']) else "N/A",
                f"{selected_wave_data['benchmark_365D']*100:.2f}%" if pd.notna(selected_wave_data['benchmark_365D']) else "N/A"
            ],
            'Alpha': [
                f"{selected_wave_data['alpha_1D']*100:.2f}%" if pd.notna(selected_wave_data['alpha_1D']) else "N/A",
                f"{selected_wave_data['alpha_30D']*100:.2f}%" if pd.notna(selected_wave_data['alpha_30D']) else "N/A",
                f"{selected_wave_data['alpha_60D']*100:.2f}%" if pd.notna(selected_wave_data['alpha_60D']) else "N/A",
                f"{selected_wave_data['alpha_365D']*100:.2f}%" if pd.notna(selected_wave_data['alpha_365D']) else "N/A"
            ]
        }
        
        returns_df = pd.DataFrame(returns_data)
        st.dataframe(returns_df, use_container_width=True, hide_index=True)
        
        st.markdown("---")
        
        # ========================================================================
        # SECTION 7: Universe Table - All Waves
        # ========================================================================
        
        st.subheader("üåê Universe Table - All 28 Waves")
        
        # Prepare display table
        display_df = snapshot_df.copy()
        
        # Format percentage columns
        for col in ['return_1D', 'return_30D', 'return_60D', 'return_365D']:
            display_df[col] = display_df[col].apply(
                lambda x: f"{x*100:.2f}%" if pd.notna(x) else "N/A"
            )
        
        # Select columns for display
        universe_table = display_df[[
            'display_name',
            'category',
            'proxy_ticker',
            'confidence',
            'return_1D',
            'return_30D',
            'return_60D',
            'return_365D'
        ]].copy()
        
        # Rename columns
        universe_table.columns = [
            'Wave Name',
            'Category',
            'Proxy',
            'Confidence',
            '1D Return',
            '30D Return',
            '60D Return',
            '365D Return'
        ]
        
        # Display table
        st.dataframe(universe_table, use_container_width=True, hide_index=True)
        
        # Summary metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            full_count = len(snapshot_df[snapshot_df['confidence'] == 'FULL'])
            st.metric("FULL Confidence", f"{full_count}/28")
        
        with col2:
            partial_count = len(snapshot_df[snapshot_df['confidence'] == 'PARTIAL'])
            st.metric("PARTIAL Confidence", f"{partial_count}/28")
        
        with col3:
            unavail_count = len(snapshot_df[snapshot_df['confidence'] == 'UNAVAILABLE'])
            st.metric("UNAVAILABLE", f"{unavail_count}/28")
        
        with col4:
            # Average 30D return (only for available data)
            valid_30d = snapshot_df[pd.notna(snapshot_df['return_30D'])]['return_30D']
            if len(valid_30d) > 0:
                avg_30d = valid_30d.mean()
                st.metric("Avg 30D Return", f"{avg_30d*100:.2f}%")
            else:
                st.metric("Avg 30D Return", "N/A")
        
        # Timestamp
        st.caption(f"üìÖ Snapshot generated at: {freshness.get('modified_at', 'Unknown')}")
    
    except ImportError as e:
        st.error("‚ö†Ô∏è **Wave Intelligence (Plan B) Unavailable:** Required modules are not available.")
        st.info("To enable this feature, ensure `planb_proxy_pipeline.py` and proxy registry validator are present.")
        with st.expander("View Error Details", expanded=False):
            st.code(str(e), language="python")
    
    except Exception as e:
        st.error(f"Error rendering Wave Intelligence (Plan B) tab: {str(e)}")
        import traceback
        with st.expander("View Error Details", expanded=False):
            st.code(traceback.format_exc(), language="python")


def render_operator_panel_tab():
    """
    Render the Operator Panel tab.
    
    This tab provides:
    - System state proof (entrypoint file, UTC timestamp, build marker)
    - Data diagnostics (cache path, existence, max date, missing symbols, coverage stats)
    - Canonical Return Ledger preview (top/bottom 5 rows, date range, row count)
    """
    st.markdown("# üõ†Ô∏è Operator Panel")
    st.markdown("### System state, data diagnostics, and ledger preview")
    
    st.markdown("---")
    
    # ========================================================================
    # SECTION 1: System State Proof
    # ========================================================================
    st.subheader("üîç System State Proof")
    
    try:
        # Entrypoint file
        entrypoint = os.path.abspath(__file__)
        entrypoint_basename = os.path.basename(__file__)
        
        # UTC timestamp
        utc_now = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')
        
        # Build marker
        try:
            build_info = get_build_info()
            build_marker = f"{build_info.get('sha', 'unknown')}"
            branch = build_info.get('branch', 'unknown')
        except Exception:
            build_marker = "SHA unavailable"
            branch = "unknown"
        
        # Escape HTML to prevent XSS (html module imported at top of file)
        entrypoint_basename_escaped = html.escape(entrypoint_basename)
        entrypoint_escaped = html.escape(entrypoint)
        utc_now_escaped = html.escape(utc_now)
        build_marker_escaped = html.escape(build_marker)
        branch_escaped = html.escape(branch)
        
        # Display in structured format
        st.markdown(f"""
        <div style="background-color: #1e1e1e; padding: 16px; border-radius: 8px; border: 1px solid #00d9ff; font-family: monospace;">
            <div style="color: #00d9ff; font-weight: bold; margin-bottom: 8px;">SYSTEM STATE</div>
            <div style="color: #e0e0e0;">
                <strong>Entrypoint:</strong> {entrypoint_basename_escaped}<br>
                <strong>Full Path:</strong> <code>{entrypoint_escaped}</code><br>
                <strong>UTC Timestamp:</strong> {utc_now_escaped}<br>
                <strong>Build Marker:</strong> {build_marker_escaped}<br>
                <strong>Branch:</strong> {branch_escaped}
            </div>
        </div>
        """, unsafe_allow_html=True)
        
    except Exception as e:
        st.error(f"Error displaying system state: {str(e)}")
    
    st.markdown("---")
    
    # ========================================================================
    # SECTION 2: Data Diagnostics
    # ========================================================================
    st.subheader("üìä Data Diagnostics")
    
    try:
        if get_price_book is not None and PRICE_BOOK_CONSTANTS_AVAILABLE:
            # Load price book
            price_book = get_cached_price_book()
            
            if price_book is not None and not price_book.empty:
                # Cache path (imported at top of file)
                cache_exists = os.path.exists(CANONICAL_CACHE_PATH)
                
                # Max date
                max_date = price_book.index.max()
                max_date_str = max_date.strftime('%Y-%m-%d') if hasattr(max_date, 'strftime') else str(max_date)
                
                # Min date
                min_date = price_book.index.min()
                min_date_str = min_date.strftime('%Y-%m-%d') if hasattr(min_date, 'strftime') else str(min_date)
                
                # Shape
                num_rows, num_cols = price_book.shape
                
                # Missing required symbols
                required_symbols = ['SPY', 'QQQ', 'IWM']
                missing_required = [sym for sym in required_symbols if sym not in price_book.columns]
                
                # VIX proxies
                vix_proxies = ['^VIX', 'VIXY', 'VXX']
                vix_available = [v for v in vix_proxies if v in price_book.columns]
                
                # Safe assets
                safe_assets = ['BIL', 'SHY']
                safe_available = [s for s in safe_assets if s in price_book.columns]
                
                # Display diagnostics
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("Cache Path", "Valid" if cache_exists else "Missing")
                    st.caption(f"Path: `{CANONICAL_CACHE_PATH}`")
                    st.caption(f"Exists: {'‚úÖ' if cache_exists else '‚ùå'}")
                
                with col2:
                    st.metric("Shape (rows √ó cols)", f"{num_rows} √ó {num_cols}")
                    st.caption(f"Date range: {min_date_str} to {max_date_str}")
                
                with col3:
                    st.metric("Required Symbols", f"{len(required_symbols) - len(missing_required)}/{len(required_symbols)}")
                    if missing_required:
                        st.caption(f"‚ö†Ô∏è Missing: {', '.join(missing_required)}")
                    else:
                        st.caption("‚úÖ All required symbols present")
                
                # Coverage statistics
                st.markdown("#### Coverage Statistics")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**VIX Proxies:**")
                    if vix_available:
                        st.success(f"‚úÖ Available: {', '.join(vix_available)}")
                    else:
                        st.warning("‚ö†Ô∏è No VIX proxies found")
                
                with col2:
                    st.markdown("**Safe Assets:**")
                    if safe_available:
                        st.success(f"‚úÖ Available: {', '.join(safe_available)}")
                    else:
                        st.warning("‚ö†Ô∏è No safe assets found")
                
                # List of missing required symbols (if any)
                if missing_required:
                    st.warning(f"‚ö†Ô∏è **Missing Required Symbols:** {', '.join(missing_required)}")
                
            else:
                st.warning("‚ö†Ô∏è Price book is empty or not available")
                st.info("**Reason:** Cache does not exist or contains no data")
        else:
            st.error("‚ùå Price book module not available")
            st.info("**Reason:** PRICE_BOOK_CONSTANTS_AVAILABLE is False or get_price_book is None")
            
    except Exception as e:
        st.error(f"Error loading data diagnostics: {str(e)}")
        st.info(f"**Reason:** {type(e).__name__}: {str(e)}")
    
    st.markdown("---")
    
    # ========================================================================
    # SECTION 3: Canonical Return Ledger Preview
    # ========================================================================
    st.subheader("üìà Canonical Return Ledger Preview")
    
    try:
        # Check if ledger exists in session state
        if 'portfolio_alpha_ledger' in st.session_state:
            ledger = st.session_state['portfolio_alpha_ledger']
            
            if ledger and isinstance(ledger, dict) and ledger.get('success'):
                ledger_df = ledger.get('ledger')
                
                if ledger_df is not None and not ledger_df.empty:
                    # Ledger metadata
                    num_rows = len(ledger_df)
                    date_range_start = ledger_df.index.min()
                    date_range_end = ledger_df.index.max()
                    
                    date_start_str = date_range_start.strftime('%Y-%m-%d') if hasattr(date_range_start, 'strftime') else str(date_range_start)
                    date_end_str = date_range_end.strftime('%Y-%m-%d') if hasattr(date_range_end, 'strftime') else str(date_range_end)
                    
                    # Display metadata
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric("Row Count", num_rows)
                    
                    with col2:
                        st.metric("Date Range Start", date_start_str)
                    
                    with col3:
                        st.metric("Date Range End", date_end_str)
                    
                    # Preview top 5 rows
                    st.markdown("#### Top 5 Rows (Most Recent)")
                    st.dataframe(ledger_df.head(5), use_container_width=True)
                    
                    # Preview bottom 5 rows
                    st.markdown("#### Bottom 5 Rows (Oldest)")
                    st.dataframe(ledger_df.tail(5), use_container_width=True)
                    
                else:
                    st.warning("‚ö†Ô∏è Ledger dataframe is empty")
                    st.info("**Reason:** Ledger computation returned empty dataframe")
            else:
                st.warning("‚ö†Ô∏è Ledger computation failed or not successful")
                reason = ledger.get('reason', 'Unknown reason') if isinstance(ledger, dict) else 'Ledger is not a dict'
                st.info(f"**Reason:** {reason}")
        else:
            st.info("‚ÑπÔ∏è Canonical Return Ledger not yet computed")
            st.info("**Reason:** Ledger has not been computed in this session. Navigate to a tab that uses the ledger to trigger computation.")
            
    except Exception as e:
        st.error(f"Error loading ledger preview: {str(e)}")
        st.info(f"**Reason:** {type(e).__name__}: {str(e)}")
    
    st.markdown("---")
    
    # ========================================================================
    # SECTION 4: S&P 500 Wave (Flagship) Proof Block
    # ========================================================================
    st.subheader("üéØ S&P 500 Wave (Flagship) Proof")
    st.markdown("### Volatility Regime, Exposure, and Reconciliation Status")
    
    try:
        # Import volatility regime helper
        from helpers.wave_performance import compute_volatility_regime_and_exposure, compute_portfolio_alpha_ledger
        
        # Get price book for volatility regime computation
        if get_price_book is not None and PRICE_BOOK_CONSTANTS_AVAILABLE:
            price_book = get_cached_price_book()
            
            if price_book is not None and not price_book.empty:
                # Compute volatility regime and exposure
                vix_result = compute_volatility_regime_and_exposure(price_book)
                
                # Display VIX and Exposure information
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    if vix_result['available']:
                        st.metric("VIX Ticker Used", vix_result['vix_ticker_used'])
                        st.caption(f"Last VIX Date: {vix_result['last_vix_date']}")
                        st.caption(f"Last VIX Value: {vix_result['last_vix_value']:.2f}")
                    else:
                        st.metric("VIX Ticker Used", "N/A")
                        st.caption(f"Reason: {vix_result.get('reason', 'Unknown')}")
                
                with col2:
                    if vix_result['available']:
                        st.metric("Current Regime", vix_result['current_regime'])
                        st.caption(f"Based on VIX: {vix_result['last_vix_value']:.2f}")
                    else:
                        st.metric("Current Regime", "N/A")
                        st.caption("VIX data unavailable")
                
                with col3:
                    if vix_result['available']:
                        exposure_pct = vix_result['current_exposure'] * 100
                        st.metric("Current Exposure", f"{exposure_pct:.0f}%")
                        st.caption(f"Raw value: {vix_result['current_exposure']:.3f}")
                    else:
                        st.metric("Current Exposure", "100%")
                        st.caption("Default (no VIX overlay)")
                
                # Compute ledger to get reconciliation status
                st.markdown("#### Daily Ledger Status")
                
                try:
                    # Try to compute ledger for S&P 500 Wave
                    ledger_result = compute_portfolio_alpha_ledger(
                        price_book,
                        periods=[1, 30],  # Just compute for quick check
                        vix_exposure_enabled=True
                    )
                    
                    if ledger_result['success']:
                        # Get ledger info
                        daily_ledger = ledger_result.get('daily_ledger')
                        
                        if daily_ledger is not None:
                            # Display ledger status
                            col1, col2, col3 = st.columns(3)
                            
                            with col1:
                                last_date = daily_ledger.index[-1].strftime('%Y-%m-%d') if hasattr(daily_ledger.index[-1], 'strftime') else str(daily_ledger.index[-1])
                                st.metric("Ledger Last Date", last_date)
                            
                            with col2:
                                st.metric("Ledger Row Count", len(daily_ledger))
                            
                            with col3:
                                # Reconciliation status
                                recon_passed = ledger_result.get('reconciliation_passed', False)
                                if recon_passed:
                                    st.metric("Reconciliation", "‚úÖ PASS")
                                    max_diff_1 = ledger_result.get('reconciliation_1_max_diff', 0)
                                    max_diff_2 = ledger_result.get('reconciliation_2_max_diff', 0)
                                    st.caption(f"Max diff 1: {max_diff_1:.8f}")
                                    st.caption(f"Max diff 2: {max_diff_2:.8f}")
                                else:
                                    st.metric("Reconciliation", "‚ùå FAIL")
                                    st.caption(f"Reason: {ledger_result.get('failure_reason', 'Unknown')}")
                            
                            # Display reconciliation details
                            st.markdown("#### Reconciliation Checks")
                            
                            if recon_passed:
                                st.success("‚úÖ All reconciliation checks passed (tolerance < 0.10%)")
                                
                                col1, col2 = st.columns(2)
                                
                                with col1:
                                    st.markdown("**Check 1:** `realized_return - benchmark_return == alpha_total`")
                                    max_diff_1 = ledger_result.get('reconciliation_1_max_diff', 0)
                                    tolerance = 0.0010
                                    st.caption(f"Max diff: {max_diff_1:.8f} < {tolerance:.8f} ‚úì")
                                
                                with col2:
                                    st.markdown("**Check 2:** `alpha_selection + alpha_overlay + alpha_residual == alpha_total`")
                                    max_diff_2 = ledger_result.get('reconciliation_2_max_diff', 0)
                                    st.caption(f"Max diff: {max_diff_2:.8f} < {tolerance:.8f} ‚úì")
                            else:
                                st.error("‚ùå Reconciliation failed - tolerance exceeded (>0.10%)")
                                st.caption(f"Failure reason: {ledger_result.get('failure_reason', 'Unknown')}")
                        else:
                            st.warning("‚ö†Ô∏è Daily ledger not available")
                            st.caption("Ledger DataFrame is None")
                    else:
                        st.warning("‚ö†Ô∏è Ledger computation failed")
                        st.caption(f"Reason: {ledger_result.get('failure_reason', 'Unknown')}")
                
                except Exception as ledger_error:
                    st.error(f"Error computing ledger: {str(ledger_error)}")
                    st.caption(f"Exception: {type(ledger_error).__name__}")
            else:
                st.warning("‚ö†Ô∏è Price book is empty - cannot compute S&P 500 Wave proof")
        else:
            st.error("‚ùå Price book module not available - cannot compute S&P 500 Wave proof")
    
    except Exception as e:
        st.error(f"Error loading S&P 500 Wave proof: {str(e)}")
        st.info(f"**Reason:** {type(e).__name__}: {str(e)}")


def render_diagnostics_tab():
    """
    Render the Diagnostics / Health tab.
    
    This tab provides:
    - System health status
    - Data availability checks
    - Safe Mode status and error history
    - Wave universe diagnostics
    - Performance diagnostics
    """
    st.markdown("# üè• Health & Diagnostics")
    st.markdown("### System health, data availability, and diagnostics")
    
    st.markdown("---")
    
    # ========================================================================
    # SECTION 1: System Health Overview
    # ========================================================================
    st.subheader("üìä System Health Overview")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        # Check if in Safe Mode
        safe_mode_active = st.session_state.get("safe_mode_enabled", False)
        safe_mode_status = "üî¥ Active" if safe_mode_active else "üü¢ Normal"
        st.metric("Safe Mode", safe_mode_status)
    
    with col2:
        # Check wave universe status
        wave_universe = st.session_state.get("wave_universe", {})
        wave_count = len(wave_universe.get("waves", []))
        st.metric("Waves Loaded", wave_count)
    
    with col3:
        # Check data freshness
        last_refresh = st.session_state.get("last_successful_refresh_time")
        if last_refresh:
            time_since = datetime.now() - last_refresh
            freshness = "üü¢ Fresh" if time_since.total_seconds() < 300 else "üü° Stale"
        else:
            freshness = "‚ö™ Unknown"
        st.metric("Data Freshness", freshness)
    
    with col4:
        # Check auto-refresh status
        auto_refresh = st.session_state.get("auto_refresh_enabled", False)
        refresh_status = "üü¢ On" if auto_refresh else "üî¥ Off"
        st.metric("Auto-Refresh", refresh_status)
    
    st.markdown("---")
    
    # ========================================================================
    # SECTION 1.5: PRICE SOURCE STAMP (ALWAYS VISIBLE)
    # ========================================================================
    st.subheader("üí∞ PRICE SOURCE STAMP")
    st.caption("Canonical price cache information - single source of truth for all price data")
    
    try:
        from helpers.price_loader import get_cache_info, check_cache_readiness, collect_required_tickers
        
        # Get cache info
        cache_info = get_cache_info()
        
        # Get readiness check
        readiness = check_cache_readiness(active_only=True)
        
        # Display in columns
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # Use status_code for display
            status_code = readiness.get('status_code', 'UNKNOWN')
            
            st.metric(
                "Cache Status",
                status_code,
                delta=None,
                help=readiness['status']
            )
            
            # Cache path
            st.caption(f"**Path:** `{cache_info['path']}`")
            
        with col2:
            # Dataframe shape
            shape_str = f"{readiness['num_days']} √ó {readiness['num_tickers']}"
            st.metric("Cache Shape (rows √ó cols)", shape_str)
            
            # Max date
            if readiness['max_date']:
                st.caption(f"**Max Date:** {readiness['max_date']}")
            else:
                st.caption("**Max Date:** N/A")
        
        with col3:
            # Ticker counts
            st.metric("Required Tickers", readiness['required_tickers'])
            st.metric("Cached Tickers", readiness['num_tickers'])
            
            # Missing tickers count
            missing_count = len(readiness.get('missing_tickers', []))
            if missing_count > 0:
                st.caption(f"‚ö†Ô∏è **Missing:** {missing_count} tickers")
            else:
                st.caption("‚úÖ **Missing:** 0 tickers")
            
            # Extra tickers count (informational only)
            extra_count = len(readiness.get('extra_tickers', []))
            if extra_count > 0:
                st.caption(f"‚ÑπÔ∏è **Extra:** {extra_count} tickers (harmless)")
        
        # Show detailed info in expander
        with st.expander("üìä Detailed Cache Information", expanded=False):
            st.markdown("### Cache Details")
            
            detail_col1, detail_col2 = st.columns(2)
            
            with detail_col1:
                st.markdown("**Cache File:**")
                st.text(f"Exists: {'Yes' if cache_info['exists'] else 'No'}")
                st.text(f"Size: {cache_info['size_mb']:.2f} MB")
                st.text(f"Rows: {readiness['num_days']} trading days")
                st.text(f"Columns: {readiness['num_tickers']} tickers")
                
                if cache_info['date_range'][0] and cache_info['date_range'][1]:
                    st.text(f"Date Range: {cache_info['date_range'][0]} to {cache_info['date_range'][1]}")
            
            with detail_col2:
                st.markdown("**Readiness:**")
                st.text(f"Status: {readiness['status']}")
                st.text(f"Ready: {'Yes' if readiness['ready'] else 'No'}")
                
                if readiness['days_stale'] is not None:
                    st.text(f"Days Stale: {readiness['days_stale']}")
                
                st.text(f"Required Tickers: {readiness['required_tickers']}")
                st.text(f"Missing Tickers: {len(readiness.get('missing_tickers', []))}")
                st.text(f"Extra Tickers: {len(readiness.get('extra_tickers', []))}")
                
                # Show failed tickers count if available
                failed_count = len(readiness.get('failed_tickers', []))
                if failed_count > 0:
                    st.text(f"Failed Downloads: {failed_count}")
            
            # Show missing tickers if any
            if readiness.get('missing_tickers'):
                st.markdown("### Missing Tickers")
                missing = readiness['missing_tickers']
                if len(missing) <= 20:
                    st.text(", ".join(missing))
                else:
                    st.text(", ".join(missing[:20]) + f", ... and {len(missing) - 20} more")
                    
                    with st.expander("View all missing tickers", expanded=False):
                        st.text(", ".join(missing))
            
            # Show extra tickers info (informational only - these are harmless)
            if readiness.get('extra_tickers'):
                st.markdown("### Extra Tickers (Informational)")
                st.caption("These tickers are in the cache but not required by active waves. This is harmless.")
                extra = readiness['extra_tickers']
                if len(extra) <= 20:
                    st.text(", ".join(extra))
                else:
                    st.text(", ".join(extra[:20]) + f", ... and {len(extra) - 20} more")
                    
                    with st.expander("View all extra tickers", expanded=False):
                        st.text(", ".join(extra))
            
            # Show failed tickers if any
            if readiness.get('failed_tickers'):
                st.markdown("### Failed Tickers")
                st.caption("These required tickers failed to download. Check failed_tickers.csv for details.")
                failed = readiness['failed_tickers']
                if len(failed) <= 20:
                    st.text(", ".join(failed))
                else:
                    st.text(", ".join(failed[:20]) + f", ... and {len(failed) - 20} more")
                    
                    with st.expander("View all failed tickers", expanded=False):
                        st.text(", ".join(failed))
        
        # Warning if cache is not ready
        if not readiness['ready']:
            if not readiness['exists']:
                st.warning(
                    "‚ö†Ô∏è **Cache file is missing!** Click 'üí∞ Rebuild Price Cache (Active Tickers Only)' in the sidebar to build the cache.",
                    icon="‚ö†Ô∏è"
                )
            else:
                st.warning(
                    f"‚ö†Ô∏è **Cache is not ready:** {readiness['status']}\n\n"
                    "Click 'üí∞ Rebuild Price Cache (Active Tickers Only)' in the sidebar to update.",
                    icon="‚ö†Ô∏è"
                )
        
    except Exception as e:
        st.error(f"Error loading price cache information: {str(e)}")
        import traceback
        with st.expander("View Error Details", expanded=False):
            st.code(traceback.format_exc(), language="python")
    
    st.markdown("---")
    
    # ========================================================================
    # SECTION 2: Safe Mode Status
    # ========================================================================
    st.subheader("‚ö†Ô∏è Safe Mode Status")
    
    if safe_mode_active:
        st.error("**Safe Mode is currently active.** The application is running in fallback mode with limited functionality.")
        
        # Show error details if available
        with st.expander("üîç View Safe Mode Error Details", expanded=False):
            error_msg = st.session_state.get("safe_mode_error_message", "No error message available")
            error_trace = st.session_state.get("safe_mode_error_traceback", "No traceback available")
            
            st.error(f"**Error Message:** {error_msg}")
            st.code(error_trace, language="python")
        
        # Retry button
        if st.button("üîÑ Retry Full Mode", help="Clear Safe Mode and attempt to run the full application"):
            st.session_state.safe_mode_enabled = False
            st.session_state.safe_mode_error_shown = False
            if "safe_mode_error_message" in st.session_state:
                del st.session_state.safe_mode_error_message
            if "safe_mode_error_traceback" in st.session_state:
                del st.session_state.safe_mode_error_traceback
            # Mark user interaction
            st.session_state.user_interaction_detected = True
            trigger_rerun("clear_safe_mode_error")
    else:
        st.success("**Safe Mode is not active.** The application is running normally.")
        
        # Show if Safe Mode was previously triggered
        if st.session_state.get("safe_mode_error_shown", False):
            st.info("**Note:** Safe Mode was triggered earlier in this session but has been cleared.")
    
    st.markdown("---")
    
    # ========================================================================
    # SECTION 2.5: Component Errors History
    # ========================================================================
    st.subheader("üîç Component Errors History")
    
    component_errors = st.session_state.get("component_errors", [])
    
    if component_errors:
        st.warning(f"**{len(component_errors)} component error(s) logged in this session**")
        
        with st.expander(f"üìã View Component Errors ({len(component_errors)})", expanded=False):
            for i, error in enumerate(reversed(component_errors), 1):
                st.markdown(f"### Error {i}: {error['component']}")
                st.text(f"Timestamp: {error['timestamp']}")
                st.error(f"**Error:** {error['error']}")
                
                with st.expander("View Traceback", expanded=False):
                    st.code(error['traceback'], language="python")
                
                st.markdown("---")
        
        # Clear errors button
        if st.button("üóëÔ∏è Clear Error History", help="Clear all logged component errors"):
            st.session_state.component_errors = []
            st.success("Error history cleared.")
            # Mark user interaction
            st.session_state.user_interaction_detected = True
            trigger_rerun("clear_error_history")
    else:
        st.success("**No component errors logged.** All components are functioning normally.")
    
    st.markdown("---")
    
    # ========================================================================
    # SECTION 3: Data Availability Checks
    # ========================================================================
    st.subheader("üìÅ Data Availability")
    
    # Check for key data files
    data_checks = []
    
    # wave_history.csv
    wave_history_path = os.path.join(os.path.dirname(__file__), 'wave_history.csv')
    wave_history_exists = os.path.exists(wave_history_path)
    wave_history_size = os.path.getsize(wave_history_path) if wave_history_exists else 0
    data_checks.append({
        "File": "wave_history.csv",
        "Status": "‚úÖ Found" if wave_history_exists else "‚ùå Missing",
        "Size": f"{wave_history_size / 1024:.1f} KB" if wave_history_exists else "N/A"
    })
    
    # wave_config.csv
    wave_config_path = os.path.join(os.path.dirname(__file__), 'wave_config.csv')
    wave_config_exists = os.path.exists(wave_config_path)
    wave_config_size = os.path.getsize(wave_config_path) if wave_config_exists else 0
    data_checks.append({
        "File": "wave_config.csv",
        "Status": "‚úÖ Found" if wave_config_exists else "‚ùå Missing",
        "Size": f"{wave_config_size / 1024:.1f} KB" if wave_config_exists else "N/A"
    })
    
    # Master_Stock_Sheet.csv
    master_sheet_path = os.path.join(os.path.dirname(__file__), 'Master_Stock_Sheet.csv')
    master_sheet_exists = os.path.exists(master_sheet_path)
    master_sheet_size = os.path.getsize(master_sheet_path) if master_sheet_exists else 0
    data_checks.append({
        "File": "Master_Stock_Sheet.csv",
        "Status": "‚úÖ Found" if master_sheet_exists else "‚ùå Missing",
        "Size": f"{master_sheet_size / 1024:.1f} KB" if master_sheet_exists else "N/A"
    })
    
    # Display data checks
    df_data_checks = pd.DataFrame(data_checks)
    st.dataframe(df_data_checks, use_container_width=True, hide_index=True)
    
    st.markdown("---")
    
    # ========================================================================
    # SECTION 4: Wave Universe Diagnostics
    # ========================================================================
    st.subheader("üåä Wave Universe Diagnostics")
    
    wave_universe = st.session_state.get("wave_universe", {})
    
    if wave_universe:
        col1, col2 = st.columns(2)
        
        with col1:
            st.metric("Total Waves", len(wave_universe.get("waves", [])))
            st.metric("Duplicates Removed", len(wave_universe.get("removed_duplicates", [])))
        
        with col2:
            st.metric("Data Source", wave_universe.get("source", "Unknown"))
            st.metric("Last Updated", wave_universe.get("timestamp", "Unknown"))
        
        # Show removed duplicates if any
        removed_duplicates = wave_universe.get("removed_duplicates", [])
        if removed_duplicates:
            with st.expander(f"üìã View Removed Duplicates ({len(removed_duplicates)})", expanded=False):
                for dup in removed_duplicates:
                    st.text(f"‚Ä¢ {dup}")
        
        # Show full wave list
        with st.expander(f"üìã View All Waves ({len(wave_universe.get('waves', []))})", expanded=False):
            waves = wave_universe.get("waves", [])
            if waves:
                # Display in 3 columns
                cols = st.columns(3)
                for i, wave in enumerate(sorted(waves)):
                    with cols[i % 3]:
                        st.text(f"‚Ä¢ {wave}")
    else:
        st.warning("Wave universe data not available.")
    
    st.markdown("---")
    
    # ========================================================================
    # SECTION 5: Module Availability
    # ========================================================================
    st.subheader("üì¶ Module Availability")
    
    module_checks = []
    
    # Alpha Attribution
    module_checks.append({
        "Module": "Alpha Attribution",
        "Status": "‚úÖ Available" if ALPHA_ATTRIBUTION_AVAILABLE else "‚ùå Unavailable"
    })
    
    # VIX Diagnostics
    module_checks.append({
        "Module": "VIX Diagnostics",
        "Status": "‚úÖ Available" if VIX_DIAGNOSTICS_AVAILABLE else "‚ùå Unavailable"
    })
    
    # Waves Engine
    module_checks.append({
        "Module": "Waves Engine",
        "Status": "‚úÖ Available" if WAVES_ENGINE_AVAILABLE else "‚ùå Unavailable"
    })
    
    # Ticker V3
    module_checks.append({
        "Module": "Ticker V3",
        "Status": "‚úÖ Available" if TICKER_V3_AVAILABLE else "‚ùå Unavailable"
    })
    
    # Auto Refresh Config
    module_checks.append({
        "Module": "Auto Refresh Config",
        "Status": "‚úÖ Available" if AUTO_REFRESH_CONFIG_AVAILABLE else "‚ùå Unavailable"
    })
    
    df_module_checks = pd.DataFrame(module_checks)
    st.dataframe(df_module_checks, use_container_width=True, hide_index=True)
    
    st.markdown("---")
    
    # ========================================================================
    # SECTION 6: Performance Diagnostics
    # ========================================================================
    st.subheader("‚ö° Performance Diagnostics")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Session start time
        if "session_start_time" not in st.session_state:
            st.session_state.session_start_time = datetime.now()
        
        session_duration = datetime.now() - st.session_state.session_start_time
        st.metric("Session Duration", f"{session_duration.total_seconds() / 60:.1f} min")
    
    with col2:
        # Auto-refresh error count
        error_count = st.session_state.get("auto_refresh_error_count", 0)
        st.metric("Auto-Refresh Errors", error_count)
    
    # Last refresh time
    last_refresh = st.session_state.get("last_successful_refresh_time")
    if last_refresh:
        time_since_refresh = datetime.now() - last_refresh
        st.info(f"**Last Successful Refresh:** {time_since_refresh.total_seconds():.0f} seconds ago")
    else:
        st.warning("**Last Successful Refresh:** Unknown")
    
    st.markdown("---")
    
    # ========================================================================
    # SECTION 6.5: ROUND 7 Phase 6 - Enhanced Diagnostics
    # ========================================================================
    st.subheader("üîç ROUND 7 Enhanced Diagnostics")
    
    with st.expander("üìä Snapshot Diagnostics", expanded=False):
        st.markdown("### Snapshot Health & Freshness")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            snapshot_exists = st.session_state.get('snapshot_exists', False)
            exists_status = "‚úÖ Yes" if snapshot_exists else "‚ùå No"
            st.metric("Snapshot Exists", exists_status)
        
        with col2:
            snapshot_fresh = st.session_state.get('snapshot_fresh', False)
            fresh_status = "üü¢ Fresh" if snapshot_fresh else "üî¥ Stale"
            st.metric("Snapshot Fresh", fresh_status)
        
        with col3:
            snapshot_age = st.session_state.get('snapshot_age_minutes')
            if snapshot_age is not None:
                if snapshot_age < 60:
                    age_str = f"{snapshot_age:.0f}m"
                elif snapshot_age < 1440:
                    age_str = f"{snapshot_age/60:.1f}h"
                else:
                    age_str = f"{snapshot_age/1440:.1f}d"
            else:
                age_str = "N/A"
            st.metric("Snapshot Age", age_str)
        
        # Snapshot rebuild status
        if st.session_state.get('snapshot_rebuilt', False):
            st.success("‚úÖ Snapshot was rebuilt on startup")
        
        # Snapshot error
        snapshot_error = st.session_state.get('snapshot_error')
        if snapshot_error:
            st.error(f"‚ö†Ô∏è Snapshot Error: {snapshot_error}")
        
        # Snapshot stale fallback
        if st.session_state.get('snapshot_stale_fallback', False):
            st.warning("‚ö†Ô∏è Using stale snapshot due to rebuild failure")
    
    with st.expander("üåä Broken Wave Diagnostics", expanded=False):
        st.markdown("### Waves with Data Issues")
        
        try:
            from analytics_pipeline import get_broken_tickers_report, resolve_wave_metrics
            from waves_engine import get_all_wave_ids, get_display_name_from_wave_id
            
            # Get all waves and check their status
            broken_waves = []
            degraded_waves = []
            operational_waves = []
            
            for wave_id in get_all_wave_ids():
                metrics = resolve_wave_metrics(wave_id, mode="Standard")
                wave_name = get_display_name_from_wave_id(wave_id) or wave_id
                readiness = metrics.get('readiness_status', 'unavailable')
                source = metrics.get('source', 'unknown')
                coverage = metrics.get('coverage_pct', 0.0)
                
                if readiness == 'unavailable':
                    broken_waves.append({
                        'Wave': wave_name,
                        'Readiness': readiness,
                        'Coverage': f"{coverage:.1f}%",
                        'Source': source
                    })
                elif source == 'degraded':
                    degraded_waves.append({
                        'Wave': wave_name,
                        'Readiness': readiness,
                        'Coverage': f"{coverage:.1f}%",
                        'Source': source
                    })
                elif readiness in ['operational', 'partial']:
                    operational_waves.append({
                        'Wave': wave_name,
                        'Readiness': readiness,
                        'Coverage': f"{coverage:.1f}%",
                        'Source': source
                    })
            
            # Display summaries
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Unavailable Waves", len(broken_waves))
            with col2:
                st.metric("Degraded Waves", len(degraded_waves))
            with col3:
                st.metric("Operational Waves", len(operational_waves))
            
            # Show broken waves
            if broken_waves:
                st.markdown("#### ‚ùå Unavailable Waves")
                st.dataframe(pd.DataFrame(broken_waves), use_container_width=True, hide_index=True)
            
            # Show degraded waves
            if degraded_waves:
                st.markdown("#### ‚ö†Ô∏è Degraded Waves")
                st.dataframe(pd.DataFrame(degraded_waves), use_container_width=True, hide_index=True)
                
        except Exception as e:
            st.error(f"Error checking wave diagnostics: {str(e)}")
    
    with st.expander("üîÑ Circuit Breaker Status", expanded=False):
        st.markdown("### Retry Limits & Timeouts")
        
        try:
            from analytics_pipeline import (
                MAX_BATCH_RETRIES,
                MAX_TICKER_RETRIES,
                MAX_TOTAL_TICKERS_PER_RUN,
                MAX_WAVE_COMPUTE_SECONDS
            )
            
            limits = [
                {"Parameter": "Max Batch Retries", "Value": MAX_BATCH_RETRIES},
                {"Parameter": "Max Ticker Retries", "Value": MAX_TICKER_RETRIES},
                {"Parameter": "Max Total Tickers Per Run", "Value": MAX_TOTAL_TICKERS_PER_RUN},
                {"Parameter": "Max Wave Compute Seconds", "Value": MAX_WAVE_COMPUTE_SECONDS},
            ]
            
            st.dataframe(pd.DataFrame(limits), use_container_width=True, hide_index=True)
            st.info("These limits prevent infinite loops and ensure bounded execution times.")
            
        except Exception as e:
            st.error(f"Error loading circuit breaker constants: {str(e)}")
    
    with st.expander("üìã Snapshot Validation Checklist", expanded=False):
        st.markdown("### Snapshot Quality Checks")
        
        checklist = []
        
        # Check 1: Snapshot exists
        snapshot_exists = st.session_state.get('snapshot_exists', False)
        checklist.append({
            "Check": "Snapshot file exists",
            "Status": "‚úÖ Pass" if snapshot_exists else "‚ùå Fail"
        })
        
        # Check 2: Snapshot is fresh
        snapshot_fresh = st.session_state.get('snapshot_fresh', False)
        checklist.append({
            "Check": "Snapshot is fresh (<15 min)",
            "Status": "‚úÖ Pass" if snapshot_fresh else "‚ö†Ô∏è Warning"
        })
        
        # Check 3: No snapshot errors
        snapshot_error = st.session_state.get('snapshot_error')
        checklist.append({
            "Check": "No generation errors",
            "Status": "‚úÖ Pass" if not snapshot_error else "‚ùå Fail"
        })
        
        # Check 4: Not using stale fallback
        stale_fallback = st.session_state.get('snapshot_stale_fallback', False)
        checklist.append({
            "Check": "Not using stale fallback",
            "Status": "‚úÖ Pass" if not stale_fallback else "‚ö†Ô∏è Warning"
        })
        
        # Check 5: Snapshot data completeness
        try:
            from analytics_pipeline import load_live_snapshot
            snapshot_df = load_live_snapshot(fallback=False)
            if snapshot_df is not None:
                row_count = len(snapshot_df)
                complete = row_count == 28
                checklist.append({
                    "Check": f"Snapshot has 28 rows (actual: {row_count})",
                    "Status": "‚úÖ Pass" if complete else "‚ö†Ô∏è Warning"
                })
        except Exception:
            checklist.append({
                "Check": "Snapshot has 28 rows",
                "Status": "‚ùå Fail"
            })
        
        st.dataframe(pd.DataFrame(checklist), use_container_width=True, hide_index=True)
    
    st.markdown("---")
    
    # ========================================================================
    # SECTION 6.75: Failed Tickers (Top 50)
    # ========================================================================
    st.subheader("‚ùå Failed Tickers Diagnostics")
    
    with st.expander("üìã Failed Tickers (Top 50) - Click to expand", expanded=False):
        try:
            from helpers.ticker_diagnostics import load_broken_tickers_from_csv, export_failed_tickers_to_cache
            
            # Load broken tickers from CSV
            broken_tickers = load_broken_tickers_from_csv("data/broken_tickers.csv")
            
            if not broken_tickers:
                st.info("‚úÖ No failed tickers found. All tickers are loading successfully.")
            else:
                # Summary metrics
                total_failed = len(broken_tickers)
                total_occurrences = sum(ticker['failure_count'] for ticker in broken_tickers)
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Total Failed Tickers", total_failed)
                with col2:
                    st.metric("Total Failure Occurrences", total_occurrences)
                with col3:
                    avg_failures = total_occurrences / total_failed if total_failed > 0 else 0
                    st.metric("Avg Failures per Ticker", f"{avg_failures:.1f}")
                
                st.markdown("---")
                
                # Display top 50 failed tickers
                top_50 = broken_tickers[:50]
                
                st.markdown(f"### Top {len(top_50)} Failed Tickers")
                st.caption("Sorted by number of impacted waves (highest first)")
                
                # Create display table
                display_data = []
                for i, ticker in enumerate(top_50, 1):
                    # Sanitize error message for display
                    error_msg = ticker.get('error_message', 'Unknown error')
                    if len(error_msg) > 50:
                        error_msg = error_msg[:47] + "..."
                    
                    # Truncate waves list for display
                    waves = ticker.get('impacted_waves', [])
                    wave_count = len(waves)
                    if wave_count > 3:
                        waves_display = ', '.join(waves[:3]) + f", ... (+{wave_count - 3} more)"
                    else:
                        waves_display = ', '.join(waves) if waves else 'None'
                    
                    display_data.append({
                        'Rank': i,
                        'Ticker': ticker.get('ticker_original', 'N/A'),
                        'Failure Type': ticker.get('failure_type', 'UNKNOWN'),
                        'Error Reason': error_msg,
                        'Wave Count': wave_count,
                        'Sample Waves': waves_display
                    })
                
                # Display as dataframe
                df_display = pd.DataFrame(display_data)
                st.dataframe(df_display, use_container_width=True, hide_index=True)
                
                # Detailed view option
                st.markdown("---")
                st.markdown("### Detailed Ticker Information")
                
                # Selector for detailed view
                ticker_names = [t.get('ticker_original', 'N/A') for t in top_50]
                selected_ticker = st.selectbox(
                    "Select a ticker to view full details:",
                    options=ticker_names,
                    key=k("Diagnostics", "failed_ticker_selector")
                )
                
                # Show detailed info for selected ticker
                if selected_ticker:
                    ticker_info = next((t for t in top_50 if t.get('ticker_original') == selected_ticker), None)
                    if ticker_info:
                        st.markdown(f"#### Details for {selected_ticker}")
                        
                        detail_col1, detail_col2 = st.columns(2)
                        
                        with detail_col1:
                            st.markdown("**Basic Information:**")
                            st.text(f"Original Symbol: {ticker_info.get('ticker_original', 'N/A')}")
                            st.text(f"Normalized Symbol: {ticker_info.get('ticker_normalized', 'N/A')}")
                            st.text(f"Failure Type: {ticker_info.get('failure_type', 'UNKNOWN')}")
                            st.text(f"Is Fatal: {ticker_info.get('is_fatal', True)}")
                        
                        with detail_col2:
                            st.markdown("**Timestamps:**")
                            st.text(f"First Seen: {ticker_info.get('first_seen', 'N/A')}")
                            st.text(f"Last Seen: {ticker_info.get('last_seen', 'N/A')}")
                            st.text(f"Impacted Waves: {ticker_info.get('failure_count', 0)}")
                        
                        st.markdown("**Error Message:**")
                        st.code(ticker_info.get('error_message', 'No error message available'), language=None)
                        
                        st.markdown("**Suggested Fix:**")
                        st.info(ticker_info.get('suggested_fix', 'No fix suggestion available'))
                        
                        st.markdown("**All Impacted Waves:**")
                        waves = ticker_info.get('impacted_waves', [])
                        if waves:
                            # Display waves in a grid
                            wave_cols = st.columns(3)
                            for idx, wave in enumerate(waves):
                                with wave_cols[idx % 3]:
                                    st.text(f"‚Ä¢ {wave}")
                        else:
                            st.text("No impacted waves recorded")
                
                st.markdown("---")
                
                # Export button
                st.markdown("### Export Options")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    # Export to cache directory
                    if st.button("üíæ Export to data/cache/failed_tickers.csv", 
                                help="Export all failed tickers to data/cache/failed_tickers.csv"):
                        success = export_failed_tickers_to_cache(broken_tickers, "data/cache/failed_tickers.csv")
                        if success:
                            st.success("‚úÖ Successfully exported to data/cache/failed_tickers.csv")
                        else:
                            st.error("‚ùå Failed to export. Check logs for details.")
                
                with col2:
                    # Download button for immediate download
                    try:
                        # Create CSV content for download using csv module
                        import io
                        import csv as csv_module
                        
                        output = io.StringIO()
                        fieldnames = [
                            'ticker_original', 'ticker_normalized', 'failure_type', 
                            'error_message', 'failure_count', 'impacted_waves',
                            'suggested_fix', 'first_seen', 'last_seen', 'is_fatal'
                        ]
                        writer = csv_module.DictWriter(output, fieldnames=fieldnames, extrasaction='ignore')
                        writer.writeheader()
                        
                        for ticker in broken_tickers:
                            # Create a copy and convert impacted_waves list to string
                            row = ticker.copy()
                            row['impacted_waves'] = row.get('impacted_waves_str', 
                                                            ', '.join(row.get('impacted_waves', [])))
                            writer.writerow(row)
                        
                        csv_content = output.getvalue()
                        
                        st.download_button(
                            label="üì• Download Failed Tickers CSV",
                            data=csv_content,
                            file_name=f"failed_tickers_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                            mime="text/csv",
                            help="Download all failed tickers as CSV"
                        )
                    except Exception as e:
                        st.error(f"Error creating download: {str(e)}")
                
        except Exception as e:
            st.error(f"Error loading failed tickers diagnostics: {str(e)}")
            import traceback
            with st.expander("View Error Details", expanded=False):
                st.code(traceback.format_exc(), language="python")
    
    st.markdown("---")
    
    # ========================================================================
    # SECTION 6.9: Active Price Source Diagnostics
    # ========================================================================
    st.subheader("üí∞ Active Price Source Details")
    
    # ========================================================================
    # PRICE SOURCE STAMP - Permanent display (non-cached, always visible)
    # ========================================================================
    st.markdown("### üìç Price Source Stamp")
    st.markdown("**Current run price source information (non-cached):**")
    
    try:
        from helpers.price_loader import get_cache_info, CACHE_PATH
        
        # Get current price cache info (non-cached call)
        cache_info = get_cache_info()
        
        if cache_info['exists']:
            # Display as metrics in columns for quick visibility
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                # Extract filename from canonical path
                cache_filename = os.path.basename(CANONICAL_CACHE_PATH)
                st.metric(
                    "üìÅ Price Source File",
                    cache_filename,
                    help="Exact price source file used for this run"
                )
            
            with col2:
                dataframe_shape = f"{cache_info['num_days']} √ó {cache_info['num_tickers']}"
                st.metric(
                    "üìä DataFrame Shape",
                    dataframe_shape,
                    help="Rows (days) √ó Columns (tickers)"
                )
            
            with col3:
                max_date = cache_info['date_range'][1] if cache_info['date_range'][1] else "N/A"
                st.metric(
                    "üìÖ Maximum Date",
                    max_date,
                    help="Most recent date in the dataframe"
                )
            
            with col4:
                st.metric(
                    "üéØ Ticker Count",
                    cache_info['num_tickers'],
                    help="Number of unique tickers in dataframe"
                )
            
            # Display full path in a compact format
            st.caption(f"**Full path:** `{cache_info['path']}`")
        else:
            st.warning("‚ö†Ô∏è Price cache file does not exist. No price data available for this run.")
            st.caption(f"Expected location: `{CACHE_PATH}`")
    
    except ImportError as e:
        st.error(f"‚ùå Price loader module not available: {str(e)}")
    except Exception as e:
        st.error(f"‚ùå Error loading price source stamp: {str(e)}")
        import traceback
        st.caption("Error details:")
        st.code(traceback.format_exc(), language="python")
    
    st.markdown("---")
    
    # Detailed price information in expander (optional)
    with st.expander("üìä Price Cache Information - Click to expand", expanded=False):
        try:
            from helpers.price_loader import get_cache_info, CACHE_PATH, collect_required_tickers
            from waves_engine import get_all_waves_universe
            
            st.markdown("### Price Cache Status")
            
            # Get cache information
            cache_info = get_cache_info()
            
            if cache_info['exists']:
                # Display cache path
                st.markdown("#### Cache Details")
                col1, col2 = st.columns(2)
                
                with col1:
                    st.text(f"üìÅ Cache Path:")
                    st.code(cache_info['path'], language=None)
                
                with col2:
                    st.metric("File Size", f"{cache_info['size_mb']:.2f} MB")
                
                # Display data shape
                st.markdown("#### Data Shape")
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("Rows (Days)", cache_info['num_days'])
                
                with col2:
                    st.metric("Columns (Tickers)", cache_info['num_tickers'])
                
                with col3:
                    total_data_points = cache_info['num_days'] * cache_info['num_tickers']
                    st.metric("Total Data Points", f"{total_data_points:,}")
                
                # Display date range
                st.markdown("#### Date Range")
                if cache_info['date_range'][0] and cache_info['date_range'][1]:
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.metric("Start Date", cache_info['date_range'][0])
                    
                    with col2:
                        st.metric("End Date", cache_info['date_range'][1])
                    
                    # Calculate days span
                    try:
                        from datetime import datetime
                        start = datetime.strptime(cache_info['date_range'][0], '%Y-%m-%d')
                        end = datetime.strptime(cache_info['date_range'][1], '%Y-%m-%d')
                        days_span = (end - start).days
                        st.info(f"üìÖ Total span: {days_span} calendar days")
                    except Exception:
                        pass
                else:
                    st.warning("Date range information not available")
                
                # Display first 20 columns
                st.markdown("#### First 20 Tickers in Cache")
                if cache_info['tickers']:
                    first_20_tickers = cache_info['tickers'][:20]
                    
                    # Display in 4 columns for better layout
                    cols = st.columns(4)
                    for idx, ticker in enumerate(first_20_tickers):
                        with cols[idx % 4]:
                            st.text(f"{idx + 1}. {ticker}")
                    
                    if cache_info['num_tickers'] > 20:
                        st.caption(f"... and {cache_info['num_tickers'] - 20} more tickers")
                else:
                    st.warning("No tickers found in cache")
                
                st.markdown("---")
                
                # Ticker count comparison
                st.markdown("### Ticker Count Comparison")
                
                try:
                    # Build wave registry dict from waves engine
                    from waves_engine import WAVE_WEIGHTS
                    wave_registry = WAVE_WEIGHTS
                    
                    # Collect required tickers
                    required_tickers = collect_required_tickers(active_only=False)
                    
                    # Count tickers
                    cache_ticker_count = cache_info['num_tickers']
                    required_ticker_count = len(required_tickers)
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric("Tickers in Cache", cache_ticker_count)
                    
                    with col2:
                        st.metric("Required Tickers", required_ticker_count)
                    
                    with col3:
                        coverage_pct = (cache_ticker_count / required_ticker_count * 100) if required_ticker_count > 0 else 0
                        st.metric("Coverage", f"{coverage_pct:.1f}%")
                    
                    # Show missing or extra tickers
                    cache_tickers_set = set(cache_info['tickers'])
                    required_tickers_set = set(required_tickers)
                    
                    missing_tickers = sorted(required_tickers_set - cache_tickers_set)
                    extra_tickers = sorted(cache_tickers_set - required_tickers_set)
                    
                    if missing_tickers:
                        with st.expander(f"‚ö†Ô∏è Missing Tickers ({len(missing_tickers)})", expanded=False):
                            st.caption("These tickers are required but not in cache:")
                            cols = st.columns(4)
                            for idx, ticker in enumerate(missing_tickers[:50]):  # Show first 50
                                with cols[idx % 4]:
                                    st.text(f"‚Ä¢ {ticker}")
                            if len(missing_tickers) > 50:
                                st.caption(f"... and {len(missing_tickers) - 50} more")
                    
                    if extra_tickers:
                        with st.expander(f"‚ÑπÔ∏è Extra Tickers ({len(extra_tickers)})", expanded=False):
                            st.caption("These tickers are in cache but not currently required:")
                            cols = st.columns(4)
                            for idx, ticker in enumerate(extra_tickers[:50]):  # Show first 50
                                with cols[idx % 4]:
                                    st.text(f"‚Ä¢ {ticker}")
                            if len(extra_tickers) > 50:
                                st.caption(f"... and {len(extra_tickers) - 50} more")
                    
                    if not missing_tickers and not extra_tickers:
                        st.success("‚úÖ Perfect match: All required tickers are in cache, no extras")
                    elif not missing_tickers:
                        st.success("‚úÖ All required tickers are available in cache")
                    
                except Exception as e:
                    st.error(f"Error comparing ticker counts: {str(e)}")
                    import traceback
                    with st.expander("View Error Details", expanded=False):
                        st.code(traceback.format_exc(), language="python")
            else:
                st.warning("‚ö†Ô∏è Price cache file does not exist")
                st.info(f"Expected location: {CACHE_PATH}")
                st.caption("The price cache may need to be built. Try using the 'Force Reload Actions' below.")
        
        except ImportError as e:
            st.error(f"Required module not available: {str(e)}")
        except Exception as e:
            st.error(f"Error loading price source diagnostics: {str(e)}")
            import traceback
            with st.expander("View Error Details", expanded=False):
                st.code(traceback.format_exc(), language="python")
    
    st.markdown("---")
    
    # ========================================================================
    # SECTION 7: Force Reload Actions
    # ========================================================================
    st.subheader("üîß Maintenance Actions")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("üîÑ Force Reload Wave Universe", help="Rebuild wave universe from scratch"):
            st.session_state.wave_universe_version = st.session_state.get("wave_universe_version", 1) + 1
            st.session_state.force_reload_universe = True
            st.success("Wave universe reload triggered. Refreshing...")
            # Mark user interaction
            st.session_state.user_interaction_detected = True
            trigger_rerun("diagnostics_reload_wave_universe")
    
    with col2:
        if st.button("üóëÔ∏è Clear Cache & Restart", help="Clear all st.cache_data and st.cache_resource, then restart"):
            # Clear Streamlit caches
            st.cache_data.clear()
            st.cache_resource.clear()
            
            # Clear session state (preserve only essential items)
            for key in list(st.session_state.keys()):
                if key not in ["safe_mode_enabled", "session_start_time"]:
                    del st.session_state[key]
            
            st.success("‚úÖ All caches cleared. Restarting...")
            # Mark user interaction
            st.session_state.user_interaction_detected = True
            trigger_rerun("diagnostics_clear_cache_restart")
    
    st.markdown("---")
    
    # Footer
    st.caption("**Note:** This tab is for diagnostic purposes only. Most users won't need to interact with it.")


def render_bottom_ticker_bar():
    """
    Render the bottom ticker bar with scrolling animation.
    V3 ADD-ON: Bottom Ticker (Institutional Rail) - Uses V3 module when available.
    Displays portfolio tickers, earnings dates, and Fed information.
    """
    # V3 ADD-ON: Bottom Ticker (Institutional Rail) - Use V3 implementation if available
    if TICKER_V3_AVAILABLE:
        try:
            render_bottom_ticker_v3(
                max_tickers=60,
                top_n_per_wave=5,
                sample_size=15
            )
            return
        except Exception:
            # Fall back to legacy implementation if V3 fails
            pass
    
    # Legacy ticker implementation (fallback)
    try:
        # Get ticker data
        tickers = get_portfolio_tickers()
        fed_info = get_fed_decision_info()
        
        # Build ticker text content
        ticker_items = []
        
        # Add portfolio tickers with earnings dates
        if tickers:
            for ticker in tickers[:5]:  # Limit to top 5 for performance
                earnings_date = get_next_earnings_date(ticker)
                if earnings_date:
                    ticker_items.append(f"{ticker}: Next Earnings {earnings_date}")
                else:
                    ticker_items.append(f"{ticker}: EARNINGS N/A")
        else:
            ticker_items.append("PORTFOLIO: N/A")
        
        # Add Fed information
        if fed_info and fed_info.get('next_date'):
            ticker_items.append(f"FED MEETING: {fed_info['next_date']}")
        else:
            ticker_items.append("FED MEETING: N/A")
        
        if fed_info and fed_info.get('current_rate'):
            ticker_items.append(f"FED FUNDS RATE: {fed_info['current_rate']}")
        else:
            ticker_items.append("FED FUNDS RATE: N/A")
        
        # Create ticker text
        ticker_text = " ‚Ä¢ ".join(ticker_items)
        
        # Duplicate ticker text for seamless loop
        ticker_text_full = f"{ticker_text} ‚Ä¢ {ticker_text} ‚Ä¢ {ticker_text}"
        
        # Render HTML ticker bar
        ticker_html = f"""
        <style>
            @keyframes scroll-left {{
                0% {{
                    transform: translateX(0);
                }}
                100% {{
                    transform: translateX(-33.333%);
                }}
            }}
            
            .ticker-bar {{
                position: fixed;
                bottom: 0;
                left: 0;
                width: 100%;
                background: linear-gradient(90deg, #1e3a8a 0%, #3b82f6 100%);
                color: white;
                padding: 10px 0;
                z-index: 999;
                overflow: hidden;
                box-shadow: 0 -2px 10px rgba(0, 0, 0, 0.2);
                font-family: 'Courier New', monospace;
                font-size: 14px;
                font-weight: bold;
            }}
            
            .ticker-content {{
                display: inline-block;
                white-space: nowrap;
                animation: scroll-left 60s linear infinite;
                padding-left: 100%;
            }}
            
            .ticker-content:hover {{
                animation-play-state: paused;
            }}
            
            /* Add padding to main content to prevent overlap */
            .main .block-container {{
                padding-bottom: 60px !important;
            }}
            
            /* Mobile Safari compatibility: Extra bottom padding */
            @media only screen and (max-width: 768px) {{
                .main .block-container {{
                    padding-bottom: 100px !important;
                }}
                .ticker-bar {{
                    padding: 8px 0;
                    z-index: 998;
                }}
            }}
        </style>
        
        <div class="ticker-bar">
            <div class="ticker-content">
                {ticker_text_full}
            </div>
        </div>
        """
        
        st.markdown(ticker_html, unsafe_allow_html=True)
        
    except Exception as e:
        # Fail silently - don't disrupt the app if ticker bar fails
        pass


# ============================================================================
# SECTION 7.5: WAVE OVERVIEW (NEW) TAB - COMPREHENSIVE ALL-WAVES VIEW
# ============================================================================

# Constants for Wave Overview (New) tab
MIN_BETA_CALCULATION_POINTS = 10  # Minimum data points required for beta calculation
ERROR_MESSAGE_MAX_LENGTH = 50  # Maximum length for error messages in diagnostics

def get_comprehensive_wave_data_all_28():
    """
    Get comprehensive data for all 28 waves without filtering.
    
    Returns data for every wave in the registry regardless of data availability,
    ensuring graceful degradation with clear diagnostics.
    
    Returns:
        List of dictionaries, one per wave, with all requested fields
    """
    try:
        # Get all 28 waves from the canonical registry
        from waves_engine import get_all_waves, get_all_wave_ids, get_display_name_from_wave_id
        
        all_waves = get_all_waves()  # Get display names
        
        # Initialize result list
        wave_data_list = []
        
        for wave_name in all_waves:
            wave_info = {
                'wave_name': wave_name,
                'mode': '',  # Will populate from registry if available
                'benchmark_name': '',
                'data_status': 'Unavailable',
                'last_data_date': None,
                'data_age_days': None,
                'return_1d': None,
                'return_30d': None,
                'return_60d': None,
                'return_365d': None,
                'benchmark_return_1d': None,
                'benchmark_return_30d': None,
                'benchmark_return_60d': None,
                'benchmark_return_365d': None,
                'alpha_1d': None,
                'alpha_30d': None,
                'alpha_60d': None,
                'alpha_365d': None,
                'beta': None,
                'max_drawdown': None,
                'failed_tickers_count': 0,
                'primary_failure_reason': ''
            }
            
            try:
                # Get wave data for multiple timeframes
                timeframes = [1, 30, 60, 365]
                
                # Try to get mode from wave registry
                try:
                    from waves_engine import WAVE_WEIGHTS, get_wave_id_from_display_name
                    wave_id = get_wave_id_from_display_name(wave_name)
                    if wave_id:
                        # Mode is not stored in registry, leave blank as per requirements
                        pass
                except:
                    pass
                
                # Try to get benchmark name
                try:
                    wave_data_1d = get_wave_data_filtered(wave_name=wave_name, days=1)
                    if wave_data_1d is not None and len(wave_data_1d) > 0 and 'benchmark_ticker' in wave_data_1d.columns:
                        benchmark_ticker = wave_data_1d['benchmark_ticker'].iloc[0]
                        if pd.notna(benchmark_ticker):
                            wave_info['benchmark_name'] = str(benchmark_ticker)
                except:
                    pass
                
                # Get data readiness status
                try:
                    from analytics_pipeline import compute_data_ready_status
                    from waves_engine import get_wave_id_from_display_name
                    
                    wave_id = get_wave_id_from_display_name(wave_name)
                    if wave_id:
                        status_result = compute_data_ready_status(wave_id)
                        readiness_status = status_result.get('readiness_status', 'unavailable')
                        
                        # Map readiness status to data status
                        status_map = {
                            'full': 'Full',
                            'partial': 'Partial',
                            'operational': 'Operational',
                            'unavailable': 'Unavailable'
                        }
                        wave_info['data_status'] = status_map.get(readiness_status.lower(), 'Unavailable')
                except:
                    pass
                
                # Get last data date and calculate data age
                try:
                    wave_data_recent = get_wave_data_filtered(wave_name=wave_name, days=365)
                    if wave_data_recent is not None and len(wave_data_recent) > 0:
                        if 'date' in wave_data_recent.columns:
                            last_date = pd.to_datetime(wave_data_recent['date']).max()
                            wave_info['last_data_date'] = last_date.strftime('%Y-%m-%d')
                            
                            # Calculate data age in days
                            today = datetime.now()
                            data_age = (today - last_date).days
                            wave_info['data_age_days'] = data_age
                except:
                    pass
                
                # Get returns, benchmark returns, and alpha for each timeframe
                for days in timeframes:
                    try:
                        wave_data = get_wave_data_filtered(wave_name=wave_name, days=days)
                        
                        if wave_data is not None and len(wave_data) > 0:
                            # Calculate wave return
                            if 'portfolio_return' in wave_data.columns:
                                wave_return = wave_data['portfolio_return'].sum()
                                wave_info[f'return_{days}d'] = wave_return
                            
                            # Calculate benchmark return
                            if 'benchmark_return' in wave_data.columns:
                                benchmark_return = wave_data['benchmark_return'].sum()
                                wave_info[f'benchmark_return_{days}d'] = benchmark_return
                            
                            # Calculate alpha (wave - benchmark)
                            if wave_info[f'return_{days}d'] is not None and wave_info[f'benchmark_return_{days}d'] is not None:
                                alpha = wave_info[f'return_{days}d'] - wave_info[f'benchmark_return_{days}d']
                                wave_info[f'alpha_{days}d'] = alpha
                    except:
                        continue
                
                # Get beta if available
                try:
                    wave_data_365 = get_wave_data_filtered(wave_name=wave_name, days=365)
                    if wave_data_365 is not None and len(wave_data_365) > 0:
                        if 'portfolio_return' in wave_data_365.columns and 'benchmark_return' in wave_data_365.columns:
                            # Calculate beta using linear regression
                            wave_returns = wave_data_365['portfolio_return'].values
                            benchmark_returns = wave_data_365['benchmark_return'].values
                            
                            # Remove NaN values
                            mask = ~np.isnan(wave_returns) & ~np.isnan(benchmark_returns)
                            if mask.sum() > MIN_BETA_CALCULATION_POINTS:  # Need sufficient data points
                                wave_returns_clean = wave_returns[mask]
                                benchmark_returns_clean = benchmark_returns[mask]
                                
                                # Calculate beta using covariance
                                covariance = np.cov(wave_returns_clean, benchmark_returns_clean)[0, 1]
                                benchmark_variance = np.var(benchmark_returns_clean)
                                
                                if benchmark_variance > 0:
                                    beta = covariance / benchmark_variance
                                    wave_info['beta'] = beta
                except:
                    pass
                
                # Get max drawdown if available
                try:
                    wave_data_365 = get_wave_data_filtered(wave_name=wave_name, days=365)
                    if wave_data_365 is not None and len(wave_data_365) > 0:
                        if 'portfolio_return' in wave_data_365.columns:
                            # Calculate cumulative returns
                            cumulative_returns = (1 + wave_data_365['portfolio_return']).cumprod()
                            
                            # Calculate running maximum
                            running_max = cumulative_returns.expanding().max()
                            
                            # Calculate drawdown
                            drawdown = (cumulative_returns - running_max) / running_max
                            
                            # Get maximum drawdown (most negative value)
                            max_dd = drawdown.min()
                            wave_info['max_drawdown'] = max_dd
                except:
                    pass
                
                # Get failed tickers count and primary failure reason
                try:
                    from helpers.ticker_diagnostics import get_diagnostics_tracker
                    tracker = get_diagnostics_tracker()
                    
                    if tracker:
                        # Get wave holdings to check for failures
                        from waves_engine import WAVE_WEIGHTS, get_wave_id_from_display_name
                        wave_id = get_wave_id_from_display_name(wave_name)
                        
                        if wave_id and wave_id in WAVE_WEIGHTS:
                            holdings = WAVE_WEIGHTS[wave_id]
                            wave_tickers = [h.ticker for h in holdings]
                            
                            # Get all failures and extract unique tickers
                            all_failures = tracker.get_all_failures()
                            failed_tickers = set([f.ticker_original for f in all_failures])
                            
                            # Count failed tickers for this wave
                            wave_failed = [t for t in wave_tickers if t in failed_tickers]
                            wave_info['failed_tickers_count'] = len(wave_failed)
                            
                            # Get primary failure reason (most common) for this wave
                            if wave_failed:
                                wave_failures = [f for f in all_failures if f.ticker_original in wave_failed]
                                if wave_failures:
                                    failure_reasons = [f.failure_type.value for f in wave_failures]
                                    from collections import Counter
                                    most_common = Counter(failure_reasons).most_common(1)
                                    if most_common:
                                        wave_info['primary_failure_reason'] = most_common[0][0]
                except:
                    pass
                    
            except Exception as e:
                # Even if individual wave processing fails, keep the row with diagnostic info
                wave_info['primary_failure_reason'] = f"Error: {str(e)[:ERROR_MESSAGE_MAX_LENGTH]}"
            
            wave_data_list.append(wave_info)
        
        return wave_data_list
        
    except Exception as e:
        # Return empty list if catastrophic failure, but log it
        print(f"Error in get_comprehensive_wave_data_all_28: {e}")
        return []


def render_wave_overview_new_tab():
    """
    Render the new Wave Overview tab with comprehensive all-waves view.
    
    This tab provides:
    - All 28 waves always rendered in a table
    - Mandatory columns: Wave Name, Mode, Benchmark, Data Status, Last Data Date, Data Age
    - Performance columns: Returns (1D, 30D, 60D, 365D) for Wave, Benchmark, and Alpha
    - Risk/Diagnostics columns: Beta, Max Drawdown, Failed Tickers Count, Primary Failure Reason
    - Optional readiness filter (show all waves by default)
    - Diagnostics section below table
    """
    try:
        st.header("üåä Wave Overview (New)")
        st.caption("Comprehensive view of all 28 waves with performance, diagnostics, and graceful degradation")
        
        # ========================================================================
        # READINESS FILTER (OPTIONAL)
        # ========================================================================
        st.markdown("### ‚öôÔ∏è Filters")
        
        show_all_waves = st.checkbox(
            "‚úÖ Show all waves (including degraded)",
            value=True,
            help="When enabled, shows all 28 waves regardless of data status. When disabled, shows only Full and Partial waves."
        )
        
        st.divider()
        
        # ========================================================================
        # ALL WAVES OVERVIEW TABLE
        # ========================================================================
        st.markdown("### üìä All Waves Overview Table")
        st.caption("Complete performance and diagnostics for all waves in the registry")
        
        # Get comprehensive data for all 28 waves
        wave_data_list = get_comprehensive_wave_data_all_28()
        
        if not wave_data_list:
            st.error("Unable to load wave data. Please check system diagnostics.")
            return
        
        # Convert to DataFrame
        df = pd.DataFrame(wave_data_list)
        
        # Apply readiness filter if needed
        if not show_all_waves:
            df = df[df['data_status'].isin(['Full', 'Partial'])]
        
        # Format the display DataFrame
        display_df = pd.DataFrame()
        
        # Core Columns
        display_df['Wave Name'] = df['wave_name']
        display_df['Mode'] = df['mode']
        display_df['Benchmark'] = df['benchmark_name']
        display_df['Data Status'] = df['data_status']
        display_df['Last Data Date'] = df['last_data_date']
        display_df['Data Age (days)'] = df['data_age_days']
        
        # Performance Columns - Returns
        display_df['Return 1D'] = df['return_1d'].apply(lambda x: f"{x:.2%}" if pd.notna(x) else "‚Äî")
        display_df['Return 30D'] = df['return_30d'].apply(lambda x: f"{x:.2%}" if pd.notna(x) else "‚Äî")
        display_df['Return 60D'] = df['return_60d'].apply(lambda x: f"{x:.2%}" if pd.notna(x) else "‚Äî")
        display_df['Return 365D'] = df['return_365d'].apply(lambda x: f"{x:.2%}" if pd.notna(x) else "‚Äî")
        
        # Performance Columns - Benchmark Returns
        display_df['BM Return 1D'] = df['benchmark_return_1d'].apply(lambda x: f"{x:.2%}" if pd.notna(x) else "‚Äî")
        display_df['BM Return 30D'] = df['benchmark_return_30d'].apply(lambda x: f"{x:.2%}" if pd.notna(x) else "‚Äî")
        display_df['BM Return 60D'] = df['benchmark_return_60d'].apply(lambda x: f"{x:.2%}" if pd.notna(x) else "‚Äî")
        display_df['BM Return 365D'] = df['benchmark_return_365d'].apply(lambda x: f"{x:.2%}" if pd.notna(x) else "‚Äî")
        
        # Performance Columns - Alpha
        display_df['Alpha 1D'] = df['alpha_1d'].apply(lambda x: f"{x:+.2%}" if pd.notna(x) else "‚Äî")
        display_df['Alpha 30D'] = df['alpha_30d'].apply(lambda x: f"{x:+.2%}" if pd.notna(x) else "‚Äî")
        display_df['Alpha 60D'] = df['alpha_60d'].apply(lambda x: f"{x:+.2%}" if pd.notna(x) else "‚Äî")
        display_df['Alpha 365D'] = df['alpha_365d'].apply(lambda x: f"{x:+.2%}" if pd.notna(x) else "‚Äî")
        
        # Risk/Diagnostics Columns
        display_df['Beta'] = df['beta'].apply(lambda x: f"{x:.2f}" if pd.notna(x) else "‚Äî")
        display_df['Max Drawdown'] = df['max_drawdown'].apply(lambda x: f"{x:.2%}" if pd.notna(x) else "‚Äî")
        display_df['Failed Tickers'] = df['failed_tickers_count']
        display_df['Primary Failure Reason'] = df['primary_failure_reason']
        
        # Display the table
        st.dataframe(
            display_df,
            use_container_width=True,
            height=600,
            hide_index=True
        )
        
        # Show row count
        st.caption(f"üìä Displaying {len(display_df)} of 28 total waves")
        
        st.divider()
        
        # ========================================================================
        # DIAGNOSTICS SECTION
        # ========================================================================
        with st.expander("üîç Diagnostics (New Overview)", expanded=False):
            st.markdown("#### System Diagnostics")
            
            # Total waves count
            total_waves = len(wave_data_list)
            
            # Count by readiness status
            status_counts = df['data_status'].value_counts().to_dict()
            full_count = status_counts.get('Full', 0)
            partial_count = status_counts.get('Partial', 0)
            operational_count = status_counts.get('Operational', 0)
            unavailable_count = status_counts.get('Unavailable', 0)
            
            # Display metrics
            col1, col2, col3, col4, col5 = st.columns(5)
            
            with col1:
                st.metric(
                    label="Total Waves",
                    value=f"{total_waves}/28",
                    help="All waves from the canonical registry"
                )
            
            with col2:
                st.metric(
                    label="üü¢ Full",
                    value=full_count,
                    help="Complete data, all analytics available"
                )
            
            with col3:
                st.metric(
                    label="üîµ Partial",
                    value=partial_count,
                    help="Some history, basic analytics available"
                )
            
            with col4:
                st.metric(
                    label="üü° Operational",
                    value=operational_count,
                    help="Current pricing only, limited history"
                )
            
            with col5:
                st.metric(
                    label="üî¥ Unavailable",
                    value=unavailable_count,
                    help="No data available, diagnostics only"
                )
            
            st.divider()
            
            # Total failed tickers
            total_failed_tickers = df['failed_tickers_count'].sum()
            st.metric(
                label="Total Failed Tickers",
                value=int(total_failed_tickers),
                help="Total number of tickers that failed to download across all waves"
            )
            
            # Top 10 failed tickers globally
            try:
                from helpers.ticker_diagnostics import get_diagnostics_tracker
                tracker = get_diagnostics_tracker()
                
                if tracker:
                    all_failures = tracker.get_all_failures()
                    
                    if all_failures:
                        st.markdown("#### Top 10 Failed Tickers Globally")
                        
                        # Count failures per ticker
                        ticker_failure_counts = {}
                        for failure in all_failures:
                            ticker = failure.ticker_original
                            if ticker not in ticker_failure_counts:
                                ticker_failure_counts[ticker] = 0
                            ticker_failure_counts[ticker] += 1
                        
                        # Sort by failure count
                        sorted_tickers = sorted(ticker_failure_counts.items(), key=lambda x: x[1], reverse=True)[:10]
                        
                        if sorted_tickers:
                            failure_df = pd.DataFrame(sorted_tickers, columns=['Ticker', 'Failure Count'])
                            st.dataframe(failure_df, use_container_width=True, hide_index=True)
                        else:
                            st.info("No failed tickers to display")
                    else:
                        st.info("No failed tickers found")
            except Exception as e:
                st.warning(f"Unable to load failed tickers: {str(e)}")
            
            st.divider()
            
            # Top 10 waves with highest number of failures
            st.markdown("#### Top 10 Waves with Most Failed Tickers")
            
            waves_with_failures = df[df['failed_tickers_count'] > 0].copy()
            waves_with_failures = waves_with_failures.sort_values('failed_tickers_count', ascending=False).head(10)
            
            if len(waves_with_failures) > 0:
                failure_waves_df = pd.DataFrame({
                    'Wave Name': waves_with_failures['wave_name'],
                    'Failed Tickers Count': waves_with_failures['failed_tickers_count'],
                    'Primary Failure Reason': waves_with_failures['primary_failure_reason']
                })
                st.dataframe(failure_waves_df, use_container_width=True, hide_index=True)
            else:
                st.info("No waves with failed tickers")
            
            # Timestamp
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            st.caption(f"üìÖ Diagnostics generated at: {timestamp}")
    
    except Exception as e:
        st.error(f"Error rendering Wave Overview (New) tab: {str(e)}")
        st.exception(e)


# ============================================================================
# SECTION 7.5: OVERVIEW (CLEAN) TAB - Demo-Ready First Screen
# ============================================================================

def render_overview_clean_tab():
    """
    Portfolio Executive Dashboard - Tab 1
    
    Portfolio-level institutional readiness dashboard providing:
    1. Composite System Control Status - High-level system health (STABLE/WATCH/DEGRADED)
    2. AI Executive Brief Narrative - Portfolio-level human-judgment summary
    3. Human-Readable Signals - Strategy Confidence, Risk Regime, Alpha Quality, Data Integrity
    4. AI Recommendations - Clear next steps for decision-makers
    5. Performance Insights - Key outperformers and positioning context
    6. Market Context - External benchmark data and regime assessment
    
    All system diagnostics and technical details moved to collapsed expanders.
    Designed for executives to understand portfolio state within 10 seconds.
    Portfolio-level scope only - no single-wave analytics or partial diagnostics.
    """
    try:
        import os
        import pandas as pd
        from datetime import datetime
        
        # ========================================================================
        # EXECUTIVE HEADER
        # ========================================================================
        st.markdown("""
        <div style="
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
            border: 2px solid #00d9ff;
            border-radius: 12px;
            padding: 20px;
            margin-bottom: 20px;
        ">
            <h1 style="color: #00d9ff; margin: 0; font-size: 32px; text-align: center;">
                üèõÔ∏è Portfolio Executive Dashboard
            </h1>
            <p style="color: #a8dadc; margin: 8px 0 0 0; font-size: 16px; text-align: center;">
                Portfolio-Level Institutional Readiness
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        # Load data for analysis
        try:
            from helpers.price_book import get_price_book, get_price_book_meta
            from helpers.wave_performance import compute_all_waves_performance
            
            price_book = get_cached_price_book()
            price_meta = get_price_book_meta(price_book)
            performance_df = compute_all_waves_performance(price_book, periods=[1, 30, 60, 365], only_validated=True)
            
            # Data age assessment
            data_age_days = None
            data_current = False
            if price_meta['date_max'] is not None:
                latest_price_date = pd.Timestamp(price_meta['date_max'], tz='UTC').normalize()
                utc_today = pd.Timestamp.utcnow().normalize()
                data_age_days = (utc_today - latest_price_date).days
                data_current = data_age_days <= 1
            
        except Exception as e:
            st.error(f"Unable to load platform data. Please check system status.")
            if st.session_state.get("debug_mode", False):
                st.exception(e)
            return
        
        # ========================================================================
        # COMPOSITE SYSTEM CONTROL STATUS
        # ========================================================================
        st.markdown("### üéõÔ∏è Composite System Control Status")
        
        try:
            # Compute system status based on multiple signals
            # UI uses price_book as source of truth to prevent divergence
            status_issues = []
            
            # Check 1: Price book staleness using canonical thresholds from price_book.py
            # OK: ‚â§PRICE_CACHE_OK_DAYS (14), DEGRADED: PRICE_CACHE_OK_DAYS+1 to PRICE_CACHE_DEGRADED_DAYS (15-30), STALE: >PRICE_CACHE_DEGRADED_DAYS (>30)
            if data_age_days is not None and data_age_days > PRICE_CACHE_DEGRADED_DAYS:
                status_issues.append(f"Price data is {data_age_days} days stale (STALE)")
            elif data_age_days is not None and data_age_days > PRICE_CACHE_OK_DAYS:
                status_issues.append(f"Price data is {data_age_days} days old (DEGRADED)")
            # else: data_age_days ‚â§ PRICE_CACHE_OK_DAYS ‚Üí OK, no issue to report
            
            # Check 2: Missing tickers / data coverage
            total_waves = len(performance_df) if not performance_df.empty else 0
            if not performance_df.empty:
                if 'Failure_Reason' in performance_df.columns:
                    failed_waves = performance_df[performance_df['Failure_Reason'].notna()]
                    failed_count = len(failed_waves)
                    
                    if failed_count > 0:
                        missing_ticker_count = len(failed_waves[failed_waves['Failure_Reason'].str.contains('Missing tickers', case=False, na=False)])
                        if missing_ticker_count > 0:
                            status_issues.append(f"{missing_ticker_count} strategies with missing ticker data")
            
            # Check 3: Data integrity
            valid_data_pct = 0
            if not performance_df.empty and total_waves > 0:
                def parse_return(val):
                    if pd.isna(val) or val == "N/A":
                        return None
                    try:
                        return float(str(val).replace('%', '').replace('+', ''))
                    except:
                        return None
                
                if '1D Return' in performance_df.columns:
                    returns_1d = performance_df['1D Return'].apply(parse_return).dropna()
                    valid_data_pct = (len(returns_1d) / total_waves * 100)
                    
                    if valid_data_pct < 70:
                        status_issues.append(f"Low data coverage: {valid_data_pct:.0f}%")
            
            # Determine overall system status using canonical price_book thresholds
            # UI uses price_book as source of truth to prevent divergence
            # STABLE: No issues AND data age ‚â§ PRICE_CACHE_OK_DAYS (14 days)
            # WATCH: Minor issues (‚â§2) AND data age ‚â§ PRICE_CACHE_DEGRADED_DAYS (30 days)
            # DEGRADED: Multiple issues OR data age > PRICE_CACHE_DEGRADED_DAYS
            if len(status_issues) == 0 and (data_age_days is None or data_age_days <= PRICE_CACHE_OK_DAYS):
                system_status = "STABLE"
                status_color = "üü¢"
                status_bg = "#1b4332"
                status_text = "All systems operational. Data is current and complete."
            elif len(status_issues) <= 2 and (data_age_days is None or data_age_days <= PRICE_CACHE_DEGRADED_DAYS):
                system_status = "WATCH"
                status_color = "üü°"
                status_bg = "#664d03"
                status_text = "System operational with minor issues: " + "; ".join(status_issues[:2])
            else:
                system_status = "DEGRADED"
                status_color = "üî¥"
                status_bg = "#5a1a1a"
                status_text = "System requires attention: " + "; ".join(status_issues[:3])
            
            # Display status banner
            st.markdown(f"""
            <div style="
                background: {status_bg};
                border: 2px solid {'#52b788' if system_status == 'STABLE' else '#ffc107' if system_status == 'WATCH' else '#dc3545'};
                border-radius: 8px;
                padding: 16px;
                margin-bottom: 20px;
            ">
                <div style="display: flex; align-items: center; justify-content: space-between;">
                    <div>
                        <h3 style="color: {'#52b788' if system_status == 'STABLE' else '#ffc107' if system_status == 'WATCH' else '#dc3545'}; margin: 0; font-size: 24px;">
                            {status_color} System Status: {system_status}
                        </h3>
                        <p style="color: #e0e0e0; margin: 8px 0 0 0; font-size: 14px;">
                            {status_text}
                        </p>
                    </div>
                </div>
            </div>
            """, unsafe_allow_html=True)
            
        except Exception as e:
            st.warning("System status check temporarily unavailable")
            if st.session_state.get("debug_mode", False):
                st.caption(f"Debug: {str(e)}")
        
        st.divider()
        
        # ========================================================================
        # 1. AI EXECUTIVE BRIEF NARRATIVE
        # ========================================================================
        st.markdown("### üìã Executive Intelligence Summary")
        
        # Generate AI narrative
        try:
            # Compute system-level metrics
            total_waves = len(performance_df) if not performance_df.empty else 0
            
            # Parse returns for analysis
            def parse_return(val):
                if pd.isna(val) or val == "N/A":
                    return None
                try:
                    return float(str(val).replace('%', '').replace('+', ''))
                except:
                    return None
            
            # Calculate portfolio metrics
            if not performance_df.empty and '1D Return' in performance_df.columns:
                returns_1d = performance_df['1D Return'].apply(parse_return).dropna()
                returns_30d = performance_df['30D'].apply(parse_return).dropna() if '30D' in performance_df.columns else pd.Series()
                
                avg_1d = returns_1d.mean() if len(returns_1d) > 0 else 0
                avg_30d = returns_30d.mean() if len(returns_30d) > 0 else 0
                positive_count = (returns_1d > 0).sum() if len(returns_1d) > 0 else 0
                total_count = len(returns_1d)
            else:
                avg_1d = avg_30d = 0
                positive_count = total_count = 0
            
            # Determine system posture
            if avg_1d > POSTURE_STRONG_POSITIVE:
                posture = "strong positive momentum"
                regime_context = "favorable market conditions"
            elif avg_1d > 0:
                posture = "modest positive performance"
                regime_context = "constructive market backdrop"
            elif avg_1d > POSTURE_WEAK_NEGATIVE:
                posture = "slight underperformance"
                regime_context = "mixed market environment"
            else:
                posture = "defensive positioning warranted"
                regime_context = "challenging market conditions"
            
            # Risk assessment based on dispersion
            risk_assessment = "balanced market regime"
            if len(returns_1d) > 0:
                dispersion = returns_1d.std()
                if dispersion > DISPERSION_HIGH:
                    risk_assessment = "heightened dispersion across strategies"
                elif dispersion < DISPERSION_LOW:
                    risk_assessment = "low volatility regime"
            
            # Alpha source narrative with capital preservation emphasis
            if avg_1d > 0.3:
                alpha_narrative = "Platform strategies are capturing meaningful outperformance across multiple factor exposures."
            elif avg_1d > 0:
                alpha_narrative = "Platform strategies demonstrate selective alpha generation with disciplined risk management."
            elif avg_1d > POSTURE_WEAK_NEGATIVE:
                alpha_narrative = "Platform strategies are prioritizing capital preservation while maintaining strategic positioning."
            else:
                alpha_narrative = "Platform strategies have prioritized capital preservation during this risk regime, with positioning for subsequent opportunities."
            
            # Construct executive narrative with real metrics
            current_time = datetime.now().strftime("%B %d, %Y at %I:%M %p")
            
            # Get system health and latest price date
            latest_price_date = price_meta.get('date_max', 'Unknown')
            system_health = "OK" if data_current else "Stable" if data_age_days and data_age_days <= 7 else "Degraded"
            
            # Calculate portfolio-level returns if available
            portfolio_metrics = []
            if not performance_df.empty and '30D' in performance_df.columns:
                # Calculate average returns across all waves
                returns_30d_vals = performance_df['30D'].apply(parse_return).dropna()
                returns_60d_vals = performance_df['60D'].apply(parse_return).dropna() if '60D' in performance_df.columns else pd.Series()
                returns_365d_vals = performance_df['365D'].apply(parse_return).dropna() if '365D' in performance_df.columns else pd.Series()
                
                avg_30d = returns_30d_vals.mean() if len(returns_30d_vals) > 0 else None
                avg_60d = returns_60d_vals.mean() if len(returns_60d_vals) > 0 else None
                avg_365d = returns_365d_vals.mean() if len(returns_365d_vals) > 0 else None
                
                if avg_30d is not None:
                    portfolio_metrics.append(f"30D: {avg_30d:+.1f}%")
                if avg_60d is not None:
                    portfolio_metrics.append(f"60D: {avg_60d:+.1f}%")
                if avg_365d is not None:
                    portfolio_metrics.append(f"365D: {avg_365d:+.1f}%")
            
            # Get alpha metrics from portfolio attribution if available
            portfolio_attrib = st.session_state.get('portfolio_alpha_attribution', {})
            total_alpha = None
            overlay_alpha = None
            
            if portfolio_attrib.get('success', False):
                summary_60d = portfolio_attrib.get('period_summaries', {}).get('60D')
                if summary_60d:
                    total_alpha = summary_60d.get('total_alpha', 0) * 100  # Convert to percentage
                    overlay_alpha = summary_60d.get('overlay_alpha', 0) * 100
            
            # Build market context line
            market_context_parts = []
            market_tickers = ['SPY', 'QQQ', 'IWM', 'TLT']
            for ticker in market_tickers:
                try:
                    if ticker in price_book.columns:
                        prices = price_book[ticker].dropna()
                        if len(prices) >= 2:
                            ret_1d = ((prices.iloc[-1] / prices.iloc[-2]) - 1) * 100
                            market_context_parts.append(f"{ticker} {ret_1d:+.1f}%")
                except:
                    pass
            
            market_context = "Market: " + ", ".join(market_context_parts) if market_context_parts else ""
            
            # Strategy performance context - emphasize regime, not raw counts
            if positive_count > 0 and total_count > 0:
                win_rate = (positive_count / total_count * 100)
                if win_rate >= 70:
                    performance_context = f"Broad-based strength observed ({positive_count} of {total_count} strategies positive)."
                elif win_rate >= 50:
                    performance_context = f"Balanced performance distribution across the portfolio ({positive_count} of {total_count} strategies positive)."
                else:
                    performance_context = f"Selective opportunities in current regime ({positive_count} of {total_count} strategies positive)."
            else:
                performance_context = "Performance data is being compiled."
            
            narrative_text = f"""
**As of {current_time}**

**System Health:** {system_health} | **Last Price Date:** {latest_price_date} | **Data Age:** {data_age_days if data_age_days is not None else 'Unknown'} days

The platform is monitoring **{total_waves} institutional-grade investment strategies** exhibiting {posture} within {regime_context}. 

{performance_context} {alpha_narrative}
"""
            
            # Add portfolio metrics if available
            if portfolio_metrics:
                narrative_text += f"\n**Portfolio Returns:** {', '.join(portfolio_metrics)}"
            
            # Add alpha metrics if available
            if total_alpha is not None:
                narrative_text += f"\n**Total Alpha (60D):** {total_alpha:+.2f}%"
            if overlay_alpha is not None:
                narrative_text += f" | **Overlay Alpha:** {overlay_alpha:+.2f}%"
            
            # Add market context if available
            if market_context:
                narrative_text += f"\n\n**{market_context}**"
            
            narrative_text += f"""

**Strategic Assessment:** {'Evaluate risk reduction measures' if avg_1d < -1.0 else 'Maintain strategic positioning' if avg_1d >= 0 else 'Continue monitoring market developments'}.
            """
            
            st.markdown(narrative_text)
            
        except Exception as e:
            st.warning("Executive summary temporarily unavailable. System is operational.")
            if st.session_state.get("debug_mode", False):
                st.caption(f"Debug: {str(e)}")
        
        st.divider()
        
        # ========================================================================
        # 2. HUMAN-READABLE SIGNALS
        # ========================================================================
        st.markdown("### üéØ Platform Intelligence Signals")
        
        signal_col1, signal_col2, signal_col3, signal_col4 = st.columns(4)
        
        # Calculate signals - define these at function scope for use in recommendations
        alpha_quality = DEFAULT_ALPHA_QUALITY
        risk_regime = DEFAULT_RISK_REGIME
        data_integrity = DEFAULT_DATA_INTEGRITY
        confidence = DEFAULT_CONFIDENCE
        
        try:
            # Strategy Confidence: Based on data coverage and freshness
            # Note: Strategy Confidence requires very fresh data (data_current = age <= 1 day)
            # for "High" confidence, which is more stringent than Data Integrity's OK threshold
            # (age <= 14 days). This is intentional - confidence in strategic decisions
            # requires fresher data than general data integrity validation.
            if not performance_df.empty:
                valid_data_pct = (len(returns_1d) / total_waves * 100) if total_waves > 0 else 0
                
                if data_current and valid_data_pct >= CONFIDENCE_HIGH_COVERAGE_PCT:
                    confidence = "High"
                    confidence_color = "üü¢"
                elif valid_data_pct >= CONFIDENCE_MODERATE_COVERAGE_PCT:
                    confidence = "Moderate"
                    confidence_color = "üü°"
                else:
                    confidence = "Low"
                    confidence_color = "üî¥"
            else:
                confidence = "Low"
                confidence_color = "üî¥"
            
            # Risk Regime: Based on market context and volatility
            # Try to get VIX and market data
            try:
                if 'VIX' in price_book.columns:
                    vix_prices = price_book['VIX'].dropna()
                    current_vix = vix_prices.iloc[-1] if len(vix_prices) > 0 else None
                else:
                    current_vix = None
            except:
                current_vix = None
            
            if current_vix is not None:
                if current_vix < RISK_REGIME_VIX_LOW:
                    risk_regime = "Risk-On"
                    regime_color = "üü¢"
                elif current_vix < RISK_REGIME_VIX_HIGH:
                    risk_regime = "Neutral"
                    regime_color = "üü°"
                else:
                    risk_regime = "Risk-Off"
                    regime_color = "üî¥"
            else:
                # Fallback based on performance
                if avg_1d > RISK_REGIME_PERF_RISK_ON:
                    risk_regime = "Risk-On"
                    regime_color = "üü¢"
                elif avg_1d < RISK_REGIME_PERF_RISK_OFF:
                    risk_regime = "Risk-Off"
                    regime_color = "üî¥"
                else:
                    risk_regime = "Neutral"
                    regime_color = "üü°"
            
            # Alpha Quality: Based on performance distribution
            if total_count > 0 and avg_1d > ALPHA_QUALITY_STRONG_RETURN and positive_count / total_count > ALPHA_QUALITY_STRONG_RATIO:
                alpha_quality = "Strong"
                alpha_color = "üü¢"
            elif total_count > 0 and (avg_1d > 0 or positive_count / total_count > ALPHA_QUALITY_MIXED_RATIO):
                alpha_quality = "Mixed"
                alpha_color = "üü°"
            else:
                alpha_quality = "Weak"
                alpha_color = "üî¥"
            
            # Data Integrity: Based on coverage and age using canonical price_book thresholds
            # UI uses price_book as source of truth to prevent divergence
            # OK: age ‚â§ PRICE_CACHE_OK_DAYS (14 days) AND coverage ‚â• 95%
            # DEGRADED: (age > PRICE_CACHE_OK_DAYS but ‚â§ PRICE_CACHE_DEGRADED_DAYS) OR (coverage ‚â• 80% but < 95%)
            # COMPROMISED: age > PRICE_CACHE_DEGRADED_DAYS OR coverage < 80%
            if (data_age_days is None or data_age_days <= PRICE_CACHE_OK_DAYS) and valid_data_pct >= DATA_INTEGRITY_VERIFIED_COVERAGE:
                data_integrity = "Verified"
                integrity_color = "üü¢"
            elif (data_age_days is None or data_age_days <= PRICE_CACHE_DEGRADED_DAYS) and valid_data_pct >= DATA_INTEGRITY_DEGRADED_COVERAGE:
                data_integrity = "Degraded"
                integrity_color = "üü°"
            else:
                data_integrity = "Compromised"
                integrity_color = "üî¥"
            
            # Display signals
            with signal_col1:
                st.metric("Strategy Confidence", f"{confidence_color} {confidence}")
                st.caption(f"{valid_data_pct:.0f}% coverage validated")
            
            with signal_col2:
                st.metric("Risk Regime", f"{regime_color} {risk_regime}")
                if current_vix:
                    st.caption(f"VIX: {current_vix:.1f}")
                else:
                    st.caption("Market-derived assessment")
            
            with signal_col3:
                st.metric("Alpha Quality", f"{alpha_color} {alpha_quality}")
                st.caption(f"{positive_count}/{total_count} strategies positive")
            
            with signal_col4:
                st.metric("Data Integrity", f"{integrity_color} {data_integrity}")
                if data_age_days is not None:
                    st.caption(f"Data age: {data_age_days} day{'s' if data_age_days != 1 else ''}")
                else:
                    st.caption("Assessment complete")
                    
        except Exception as e:
            st.warning("Signal generation temporarily unavailable")
            if st.session_state.get("debug_mode", False):
                st.caption(f"Debug: {str(e)}")
        
        st.divider()
        
        # ========================================================================
        # 3. AI RECOMMENDATIONS
        # ========================================================================
        st.markdown("### üí° AI Recommendations")
        
        try:
            # Generate recommendations based on signals
            recommendations = []
            
            # Recommendation based on alpha quality
            if alpha_quality == "Strong":
                recommendations.append("‚úÖ **Maintain Current Exposure** - Platform strategies are performing well. Continue current positioning across high-conviction opportunities.")
            elif alpha_quality == "Mixed":
                recommendations.append("‚ö†Ô∏è **Selective Rebalancing** - Consider rotating toward top-performing strategies while maintaining diversification.")
            else:
                recommendations.append("üî¥ **Reduce Risk Exposure** - Platform performance suggests defensive positioning. Consider reducing allocations to underperforming strategies.")
            
            # Recommendation based on risk regime
            if risk_regime == "Risk-Off":
                recommendations.append("üõ°Ô∏è **Increase Hedging** - Elevated volatility suggests adding protective positions or increasing cash allocation.")
            elif risk_regime == "Risk-On":
                recommendations.append("üìà **Opportunistic Growth** - Favorable conditions support increasing exposure to growth-oriented strategies.")
            
            # Recommendation based on data integrity
            if data_integrity == "Degraded" or data_integrity == "Compromised":
                recommendations.append("üîß **Refresh Data Sources** - Data quality requires attention. Initiate data refresh to ensure decision accuracy.")
            
            # Data-driven tactical recommendations
            if not performance_df.empty:
                # Identify top performers
                top_perf = performance_df.nlargest(3, '1D Return') if '1D Return' in performance_df.columns else pd.DataFrame()
                
                if not top_perf.empty and len(top_perf) > 0:
                    top_name = top_perf.iloc[0]['Wave'] if 'Wave' in top_perf.columns else "leading strategy"
                    recommendations.append(f"‚≠ê **Spotlight Opportunity** - {top_name} shows notable outperformance. Review for potential position sizing increase.")
            
            # Display recommendations
            for rec in recommendations:
                st.markdown(rec)
                
        except Exception as e:
            st.info("AI recommendations will appear here based on current market conditions and platform performance.")
            if st.session_state.get("debug_mode", False):
                st.caption(f"Debug: {str(e)}")
        
        st.divider()
        
        # ========================================================================
        # 4. PERFORMANCE INSIGHTS - TOP STRATEGIES
        # ========================================================================
        st.markdown("### ‚≠ê Top Performing Strategies")
        st.caption("Ranked by alpha performance across multiple timeframes")
        
        try:
            if not performance_df.empty:
                # Get top 5 performers by 30D alpha and 60D alpha
                def parse_return_value(val):
                    if pd.isna(val) or val == "N/A":
                        return None
                    try:
                        return float(str(val).replace('%', '').replace('+', ''))
                    except:
                        return None
                
                # Create tabs for different timeframes
                perf_tab_30d, perf_tab_60d = st.tabs(["üìä Top 5 by 30D Alpha", "üìà Top 5 by 60D Alpha"])
                
                with perf_tab_30d:
                    st.caption("Ranked by 30-day alpha performance")
                    
                    if '30D' in performance_df.columns:
                        # Create numeric column for sorting
                        performance_df['30D_Numeric'] = performance_df['30D'].apply(parse_return_value)
                        
                        # Sort and get top performers (up to 5)
                        top_30d = performance_df.nlargest(min(5, len(performance_df)), '30D_Numeric')
                        
                        if not top_30d.empty and top_30d['30D_Numeric'].notna().any():
                            # Display as ranked table
                            ranked_data = []
                            for rank, (_, row) in enumerate(top_30d.iterrows(), 1):
                                wave_name = row.get('Wave', 'N/A')
                                return_30d = row.get('30D_Numeric', None)
                                return_1d = parse_return_value(row.get('1D Return', 'N/A'))
                                
                                if return_30d is not None:
                                    ranked_data.append({
                                        'Rank': f"#{rank}",
                                        'Wave': wave_name,
                                        '30D Alpha': f"{return_30d:+.2f}%",
                                        '1D Return': f"{return_1d:+.2f}%" if return_1d is not None else "N/A"
                                    })
                            
                            if ranked_data:
                                ranked_df = pd.DataFrame(ranked_data)
                                st.dataframe(ranked_df, hide_index=True, use_container_width=True)
                            else:
                                st.info("Insufficient data for 30D alpha ranking")
                        else:
                            st.info("No valid 30D performance data available")
                    else:
                        st.info("30D performance data not available")
                
                with perf_tab_60d:
                    st.caption("Ranked by 60-day alpha performance")
                    
                    if '60D' in performance_df.columns:
                        # Create numeric column for sorting
                        performance_df['60D_Numeric'] = performance_df['60D'].apply(parse_return_value)
                        
                        # Sort and get top performers (up to 5)
                        top_60d = performance_df.nlargest(min(5, len(performance_df)), '60D_Numeric')
                        
                        if not top_60d.empty and top_60d['60D_Numeric'].notna().any():
                            # Display as ranked table
                            ranked_data = []
                            for rank, (_, row) in enumerate(top_60d.iterrows(), 1):
                                wave_name = row.get('Wave', 'N/A')
                                return_60d = row.get('60D_Numeric', None)
                                return_30d = parse_return_value(row.get('30D', 'N/A'))
                                
                                if return_60d is not None:
                                    ranked_data.append({
                                        'Rank': f"#{rank}",
                                        'Wave': wave_name,
                                        '60D Alpha': f"{return_60d:+.2f}%",
                                        '30D Return': f"{return_30d:+.2f}%" if return_30d is not None else "N/A"
                                    })
                            
                            if ranked_data:
                                ranked_df = pd.DataFrame(ranked_data)
                                st.dataframe(ranked_df, hide_index=True, use_container_width=True)
                            else:
                                st.info("Insufficient data for 60D alpha ranking")
                        else:
                            st.info("No valid 60D performance data available")
                    else:
                        st.info("60D performance data not available")
                
        except Exception as e:
            st.warning("Performance insights temporarily unavailable.")
            if st.session_state.get("debug_mode", False):
                st.caption(f"Debug: {str(e)}")
        
        st.divider()
        
        # ========================================================================
        # 5. MARKET CONTEXT (External Benchmark Data)
        # ========================================================================
        st.markdown("### üåç Market Context (External Benchmark Data)")
        st.caption("External market indicators - not portfolio performance")
        
        try:
            # Define key market indicators
            market_tickers = {
                'SPY': 'S&P 500',
                'QQQ': 'Nasdaq 100',
                'IWM': 'Small Caps',
                'TLT': 'Treasuries',
                'GLD': 'Gold',
                'VIX': 'Volatility'
            }
            
            market_cols = st.columns(6)
            
            for idx, (ticker, name) in enumerate(market_tickers.items()):
                with market_cols[idx]:
                    try:
                        if ticker in price_book.columns:
                            prices = price_book[ticker].dropna()
                            
                            if len(prices) >= 2:
                                ret_1d = ((prices.iloc[-1] / prices.iloc[-2]) - 1) * 100
                                
                                # Determine trend indicator
                                if ret_1d > 0.5:
                                    trend = "üü¢"
                                elif ret_1d < -0.5:
                                    trend = "üî¥"
                                else:
                                    trend = "‚ö™"
                                
                                st.metric(name, f"{trend} {ret_1d:+.1f}%")
                            else:
                                st.metric(name, "‚Äî")
                                st.caption("Pending")
                        else:
                            st.metric(name, "‚Äî")
                            st.caption("N/A")
                    except Exception as e:
                        st.metric(name, "‚Äî")
                        st.caption("Error")
        except Exception as e:
            st.warning("Market context will appear here")
            if st.session_state.get("debug_mode", False):
                st.caption(f"Debug: {str(e)}")
        
        st.divider()
        
        # ========================================================================
        # SYSTEM DIAGNOSTICS (Collapsed Expander)
        # ========================================================================
        with st.expander("üîß System Diagnostics & Technical Details", expanded=False):
            st.markdown("#### Technical Diagnostics for Administrators")
            st.caption("Detailed system validation metrics and engineering data")
            
            # Build info
            st.markdown("**Build Information**")
            try:
                import subprocess
                branch = subprocess.check_output(['git', 'branch', '--show-current']).decode('utf-8').strip()
                utc_now = datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC")
                
                diag_col1, diag_col2 = st.columns(2)
                with diag_col1:
                    st.metric("Git Branch", branch)
                with diag_col2:
                    st.metric("UTC Timestamp", utc_now)
            except:
                st.caption("Build info unavailable")
            
            st.divider()
            
            # Price book diagnostics
            st.markdown("**Data Cache Status**")
            try:
                from helpers.wave_performance import get_price_book_diagnostics
                
                pb_diag = get_price_book_diagnostics(price_book)
                
                diag_col1, diag_col2, diag_col3 = st.columns(3)
                
                with diag_col1:
                    cache_filename = os.path.basename(CANONICAL_CACHE_PATH)
                    st.metric("Cache File", cache_filename)
                    st.caption(f"Path: {pb_diag.get('path', 'N/A')}")
                
                with diag_col2:
                    shape = pb_diag.get('shape', (0, 0))
                    st.metric("Shape", f"{shape[0]} √ó {shape[1]}")
                    st.caption(f"{pb_diag.get('total_days', 0)} days, {pb_diag.get('total_tickers', 0)} tickers")
                
                with diag_col3:
                    last_date = pb_diag.get('date_max', 'N/A')
                    st.metric("Last Date", last_date)
                    
                    if last_date != 'N/A':
                        latest_date = pd.Timestamp(last_date, tz='UTC').normalize()
                        utc_today = pd.Timestamp.utcnow().normalize()
                        days_stale = (utc_today - latest_date).days
                        
                        if days_stale > 7:
                            st.error(f"STALE: {days_stale} days")
                        elif days_stale > 1:
                            st.warning(f"CACHED: {days_stale} days")
                        else:
                            st.success("Current")
            except Exception as e:
                st.caption(f"Cache diagnostics unavailable: {str(e)[:50]}")
            
            st.divider()
            
            # Session state
            st.markdown("**Session State**")
            
            run_id = st.session_state.get("run_id", 0)
            run_seq = st.session_state.get("run_seq", 0)
            run_trigger = st.session_state.get("run_trigger", "unknown")
            safe_mode = st.session_state.get("safe_mode_no_fetch", True)
            loop_detected = st.session_state.get("loop_detected", False)
            
            session_col1, session_col2, session_col3 = st.columns(3)
            
            with session_col1:
                st.metric("Run ID", run_id)
                st.metric("Run Sequence", run_seq)
            
            with session_col2:
                st.metric("Trigger", run_trigger)
                st.metric("Safe Mode", "ON" if safe_mode else "OFF")
            
            with session_col3:
                st.metric("Loop Detected", "YES" if loop_detected else "NO")
                selected_wave = st.session_state.get("selected_wave", "N/A")
                st.metric("Selected Wave", selected_wave)
            
            st.divider()
            
            # Wave validation
            st.markdown("**Wave Validation Status**")
            
            try:
                if not performance_df.empty:
                    total_waves = len(performance_df)
                    
                    # Check for validation issues
                    if 'Failure_Reason' in performance_df.columns:
                        failed_waves = performance_df[performance_df['Failure_Reason'].notna()]
                        failed_count = len(failed_waves)
                    else:
                        failed_count = 0
                    
                    val_col1, val_col2 = st.columns(2)
                    
                    with val_col1:
                        st.metric("Total Waves", total_waves)
                    
                    with val_col2:
                        st.metric("Failed Validations", failed_count)
                        
                        if failed_count > 0:
                            st.warning(f"{failed_count} waves with validation issues")
                        else:
                            st.success("All waves passing validation")
                    
                    # Show failed waves if any
                    if failed_count > 0 and 'Failure_Reason' in performance_df.columns:
                        st.markdown("**Failed Waves:**")
                        failure_groups = failed_waves.groupby('Failure_Reason')['Wave'].apply(list).to_dict()
                        
                        for reason, waves in failure_groups.items():
                            st.markdown(f"**{reason}:** {', '.join(waves[:5])}")
                            if len(waves) > 5:
                                st.caption(f"...and {len(waves) - 5} more")
                else:
                    st.caption("No performance data available")
            except Exception as e:
                st.caption(f"Validation status unavailable: {str(e)[:50]}")
            
            st.divider()
            
            # Network status
            st.markdown("**Network & Auto-Refresh**")
            
            try:
                from helpers.price_book import ALLOW_NETWORK_FETCH
                network_status = "ENABLED" if ALLOW_NETWORK_FETCH else "DISABLED"
            except:
                network_status = "N/A"
            
            auto_refresh_status = st.session_state.get("auto_refresh_enabled", False)
            
            net_col1, net_col2 = st.columns(2)
            
            with net_col1:
                st.metric("Network Fetch", network_status)
            
            with net_col2:
                st.metric("Auto-Refresh", "ON" if auto_refresh_status else "OFF")
    
    except Exception as e:
        st.error("Unable to render executive dashboard")
        if st.session_state.get("debug_mode", False):
            st.exception(e)


# ============================================================================
# SECTION 8: MAIN APPLICATION ENTRY POINT
# ============================================================================

def main():
    """
    Main application entry point - Executive Layer v2.
    Orchestrates the entire Institutional Console UI with enhanced analytics.
    """
    # ========================================================================
    # STEP -1: Page Configuration (must be first Streamlit call)
    # ========================================================================
    st.set_page_config(page_title="Institutional Console - Executive Layer v2", layout="wide")
    
    # ========================================================================
    # STEP -0.5: Render Proof Banner and Build Stamp
    # ========================================================================
    render_proof_banner()
    render_build_stamp()
    
    # ========================================================================
    # ENTRYPOINT FINGERPRINT
    # ========================================================================
    print("[ENTRYPOINT] Running app.py")
    
    # ========================================================================
    # STEP 0: Initialize Safe Mode (Default ON)
    # ========================================================================
    
    # Initialize Safe Mode flag (default: ON for stability)
    if "safe_mode_no_fetch" not in st.session_state:
        st.session_state.safe_mode_no_fetch = True  # Default to ON
    
    # Initialize loop detection flag
    if "loop_detected" not in st.session_state:
        st.session_state.loop_detected = False
    
    # ========================================================================
    # STEP 1: Run Guard Counter (Hard Circuit Breaker) - Enhanced Loop Detection
    # ========================================================================
    
    # Initialize run_count if not present (tracks consecutive runs without user action)
    if "run_count" not in st.session_state:
        st.session_state.run_count = 0
    
    # Initialize user_interaction_detected flag
    if "user_interaction_detected" not in st.session_state:
        st.session_state.user_interaction_detected = False
    
    # Reset run_count if user interaction was detected in previous run
    if st.session_state.user_interaction_detected:
        st.session_state.run_count = 0
        st.session_state.user_interaction_detected = False
    
    # Increment run_count
    st.session_state.run_count += 1
    
    # Check run_count threshold (max 3 iterations without user action)
    if st.session_state.run_count > 3:
        st.session_state.loop_detected = True
        # Log loop detection but continue rendering UI
        logger.warning(f"Loop detection triggered: {st.session_state.run_count} consecutive runs without user interaction")
        # Display warning banner but allow UI to continue
        if st.session_state.get("debug_mode", False):
            st.warning("‚ö†Ô∏è Loop detection: Multiple consecutive runs detected. Debug mode allows continued execution.")
        # st.stop()  # REMOVED: Allow UI to render instead of halting
    
    # ========================================================================
    # STEP 1.5: Rerun Throttle Safety Fuse (Prevent Rapid Reruns)
    # ========================================================================
    
    # Initialize rerun throttle state
    if "last_rerun_time" not in st.session_state:
        st.session_state.last_rerun_time = time.time()
        st.session_state.rapid_rerun_count = 0
    else:
        current_time = time.time()
        time_since_last_rerun = current_time - st.session_state.last_rerun_time
        
        # Check if rerun is happening too quickly (configurable threshold)
        if time_since_last_rerun < RERUN_THROTTLE_THRESHOLD:
            st.session_state.rapid_rerun_count += 1
            
            # If we've exceeded max rapid reruns, log but continue
            if st.session_state.rapid_rerun_count >= MAX_RAPID_RERUNS:
                # Log rapid rerun detection but continue rendering UI
                logger.warning(f"Rapid rerun detection triggered: {st.session_state.rapid_rerun_count} reruns within {RERUN_THROTTLE_THRESHOLD}s")
                # Display warning banner but allow UI to continue
                if st.session_state.get("debug_mode", False):
                    st.warning(f"‚ö†Ô∏è Rapid rerun detected: {st.session_state.rapid_rerun_count} reruns within {RERUN_THROTTLE_THRESHOLD}s. Debug mode allows continued execution.")
                # st.stop()  # REMOVED: Allow UI to render instead of halting
        else:
            # Reset rapid rerun counter if enough time has passed
            st.session_state.rapid_rerun_count = 0
        
        # Update last rerun time
        st.session_state.last_rerun_time = current_time
    
    # ========================================================================
    # STEP 2: Run ID Counter & Trigger Diagnostics (Infinite Loop Prevention)
    # ========================================================================
    
    # Initialize run_id counter if not present
    if "run_id" not in st.session_state:
        st.session_state.run_id = 0
        st.session_state.run_trigger = "initial_load"
        st.session_state.initial_load_complete = False
    else:
        st.session_state.run_id += 1
        
        # Determine what triggered this rerun
        if st.session_state.get("_last_button_clicked"):
            st.session_state.run_trigger = f"button: {st.session_state._last_button_clicked}"
            st.session_state._last_button_clicked = None
            st.session_state.user_interaction_detected = True  # Mark user interaction
        elif st.session_state.get("_widget_interaction"):
            st.session_state.run_trigger = "widget_interaction"
            st.session_state._widget_interaction = False
            st.session_state.user_interaction_detected = True  # Mark user interaction
        elif st.session_state.get("auto_refresh_enabled") and not st.session_state.get("safe_mode_no_fetch", True):
            st.session_state.run_trigger = "auto_refresh"
            # Auto-refresh does NOT count as user interaction
        else:
            st.session_state.run_trigger = "user_interaction"
            st.session_state.user_interaction_detected = True  # Mark user interaction
    
    # ONE RUN ONLY latch: After initial load, require user interaction for any computations
    st.session_state.one_run_only_block = (
        st.session_state.initial_load_complete and 
        not st.session_state.user_interaction_detected
    )
    if st.session_state.run_trigger == "initial_load":
        st.session_state.initial_load_complete = True
    
    # Display run diagnostics at the very top (only in debug mode)
    if st.session_state.get("debug_mode", False):
        st.caption(f"üîÑ Run ID: {st.session_state.run_id} | Trigger: {st.session_state.run_trigger}")
    
    # ========================================================================
    # STEP 2.5: Run Trace Banner & Safety Latch
    # ========================================================================
    
    # Calculate run trace values (but don't display unless debug mode)
    run_seq = st.session_state.get("run_seq", 0)
    delta_seconds = st.session_state.get("delta_seconds", 0.0)
    last_trigger = st.session_state.get("last_trigger", "unknown")
    buttons_clicked = st.session_state.get("buttons_clicked", [])
    
    # Format buttons clicked
    buttons_str = ", ".join(buttons_clicked[-3:]) if buttons_clicked else "None"
    
    # Display banner (only in debug mode)
    if st.session_state.get("debug_mode", False):
        st.info(f"""
**üîÑ RUN TRACE**  
‚Ä¢ **Run #:** {run_seq}  
‚Ä¢ **Delta:** {delta_seconds:.2f}s  
‚Ä¢ **Last Trigger:** {last_trigger}  
‚Ä¢ **Recent Buttons:** {buttons_str}
""")
    
    # Safety Latch: STOP after 2 runs unless debug mode enabled
    # Initialize debug mode checkbox state
    if "allow_continuous_reruns" not in st.session_state:
        st.session_state.allow_continuous_reruns = False
    
    # Check if we should engage the loop trap
    # Note: We check this AFTER the sidebar is rendered so users can toggle the checkbox
    st.session_state.loop_trap_should_engage = (
        run_seq >= 2 and not st.session_state.allow_continuous_reruns
    )
    
    # Display warning if loop trap will engage (only in debug mode)
    if st.session_state.loop_trap_should_engage and st.session_state.get("debug_mode", False):
        st.error("‚ö†Ô∏è **Loop Trap Engaged: Blocking reruns**")
        st.warning(f"""
The application has completed {run_seq} runs. To prevent infinite loops, 
further automatic reruns are blocked.

**To continue:**
1. Enable "Allow Continuous Reruns (Debug)" in the sidebar, or
2. Refresh the page manually
""")
    
    # ========================================================================
    # STEP 3: Top Banner with Status Information (Moved to Debug Mode)
    # ========================================================================
    
    # Get snapshot timestamp if available (for debug display later)
    snapshot_timestamp = "Unknown"
    try:
        import os
        from datetime import datetime
        snapshot_path = "data/live_snapshot.csv"
        if os.path.exists(snapshot_path):
            mtime = os.path.getmtime(snapshot_path)
            snapshot_timestamp = datetime.fromtimestamp(mtime).strftime("%Y-%m-%d %H:%M:%S")
    except Exception:
        pass
    
    # Display status banner (only in debug mode)
    if st.session_state.get("debug_mode", False):
        safe_mode_status = "üî¥ ON" if st.session_state.safe_mode_no_fetch else "üü¢ OFF"
        loop_status = "‚ö†Ô∏è YES" if st.session_state.loop_detected else "‚úÖ NO"
        
        st.info(f"""
**System Status**  
‚Ä¢ **Safe Mode:** {safe_mode_status}  
‚Ä¢ **Loop Detected:** {loop_status}  
‚Ä¢ **Last Snapshot:** {snapshot_timestamp}
""")
    
    # ========================================================================
    # STALE SNAPSHOT BANNER - Display warning if snapshots are stale (debug mode only)
    # ========================================================================
    if st.session_state.get("debug_mode", False):
        try:
            from helpers.compute_gate import check_stale_snapshot
            
            # Check main snapshot
            live_snapshot_path = "data/live_snapshot.csv"
            is_stale, age_minutes = check_stale_snapshot(
                live_snapshot_path,
                st.session_state,
                build_key="engine_snapshot"
            )
            
            if is_stale and age_minutes != float('inf'):
                # Snapshot exists but is stale
                age_hours = age_minutes / 60
                st.warning(f"""
‚ö†Ô∏è **Stale Snapshot Detected**  
The live snapshot is {age_hours:.1f} hours old (threshold: 1 hour).  
Click a rebuild button in the sidebar to refresh the data.
""")
            elif not os.path.exists(live_snapshot_path):
                # Snapshot is missing
                st.warning("""
‚ö†Ô∏è **Snapshot Missing**  
No live snapshot found. Click a rebuild button in the sidebar to generate data.
""")
        except Exception as e:
            # Silently fail if stale check fails
            pass
    
    # ========================================================================
    # WALL-CLOCK WATCHDOG - Enforce 3-second timeout in Safe Mode
    # ========================================================================
    elapsed_time = time.time() - WATCHDOG_START_TIME
    if st.session_state.safe_mode_no_fetch and elapsed_time > 3.0:
        # Log watchdog timeout but continue rendering UI
        logger.warning(f"Safe Mode watchdog timeout: {elapsed_time:.2f}s elapsed (threshold: 3.0s)")
        # Display warning banner but allow UI to continue
        if st.session_state.get("debug_mode", False):
            st.warning(f"‚è±Ô∏è Safe Mode watchdog timeout: {elapsed_time:.2f}s elapsed. Debug mode allows continued execution.")
        # st.stop()  # REMOVED: Allow UI to render instead of halting
    
    # Debug trace: Mark that we passed the watchdog
    if st.session_state.get("debug_mode", False):
        st.caption(f"üîç Trace: Passed watchdog check (elapsed: {elapsed_time:.2f}s)")
    
    # ========================================================================
    # Wave Registry CSV Self-Healing - Validate and Auto-Rebuild
    # ========================================================================
    
    # Validate wave registry CSV on startup (only once per session)
    if "wave_registry_validated" not in st.session_state:
        try:
            from wave_registry_manager import auto_heal_wave_registry
            
            # Auto-heal will validate and rebuild if necessary
            healed = auto_heal_wave_registry()
            
            if healed:
                print("‚úÖ Wave registry CSV validated successfully")
            else:
                print("‚ö†Ô∏è Wave registry CSV validation/rebuild failed - some waves may not load")
            
            st.session_state.wave_registry_validated = True
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Could not validate wave registry CSV: {e}")
            st.session_state.wave_registry_validated = False
            # Store exception for debug panel
            if "data_load_exceptions" not in st.session_state:
                st.session_state.data_load_exceptions = []
            st.session_state.data_load_exceptions.append({
                "component": "Wave Registry Validation",
                "error": str(e),
                "traceback": traceback.format_exc()
            })
    
    # ========================================================================
    # Wave Universe Validation - Verify Active Waves (UPDATED for Active/Inactive Split)
    # ========================================================================
    
    # Validate that we have the expected number of active waves on startup
    if "wave_universe_validated" not in st.session_state:
        try:
            from waves_engine import get_all_waves_universe
            from wave_registry_manager import get_active_wave_registry
            
            universe = get_all_waves_universe()
            
            # Get expected count from active waves in CSV registry
            active_waves_df = get_active_wave_registry()
            expected_active_count = len(active_waves_df)
            
            actual_count = universe.get('count', 0)
            
            if expected_active_count == 0:
                # CSV not available, skip validation
                st.session_state.wave_universe_validation_failed = False
                print(f"‚ö†Ô∏è Could not load active wave registry, skipping validation")
            elif actual_count != expected_active_count:
                # Get inactive waves for informative message
                all_waves_df = active_waves_df  # We only have active waves loaded
                inactive_count = 0
                inactive_names = ''
                try:
                    import pandas as pd
                    full_registry = pd.read_csv("data/wave_registry.csv")
                    inactive_waves = full_registry[full_registry['active'] == False]
                    inactive_count = len(inactive_waves)
                    inactive_names = ', '.join(inactive_waves['wave_name'].tolist()) if len(inactive_waves) > 0 else ''
                except:
                    pass
                
                # Only mark as validation failure if the discrepancy is NOT explained by inactive waves
                # If actual + inactive = expected, then this is normal and not a failure
                total_waves = actual_count + inactive_count
                if total_waves == expected_active_count + inactive_count or actual_count == expected_active_count:
                    # This is expected - we have active waves loaded, and inactive waves are properly excluded
                    st.session_state.wave_universe_validation_failed = False
                    st.session_state.wave_universe_validated_count = f"{actual_count}/{actual_count} active waves"
                    if inactive_count > 0 and inactive_names:
                        st.session_state.wave_universe_inactive_info = f"{inactive_count} inactive wave(s): {inactive_names}"
                    print(f"‚úÖ Wave Universe Validated: {actual_count}/{actual_count} active waves loaded")
                    if inactive_count > 0:
                        print(f"‚ÑπÔ∏è {inactive_count} inactive wave(s) properly excluded: {inactive_names}")
                else:
                    # Genuine mismatch - mark as failure
                    st.session_state.wave_universe_validation_failed = True
                    st.session_state.wave_universe_discrepancy = f"Expected {expected_active_count} active waves, found {actual_count}"
                    if inactive_count > 0 and inactive_names:
                        st.session_state.wave_universe_inactive_info = f"Inactive waves: {inactive_names}"
                    print(f"‚ö†Ô∏è Wave Universe Validation Failed: {st.session_state.wave_universe_discrepancy}")
            else:
                st.session_state.wave_universe_validation_failed = False
                st.session_state.wave_universe_validated_count = f"{actual_count}/{actual_count} active waves"
                print(f"‚úÖ Wave Universe Validated: {actual_count}/{actual_count} active waves")
                
                # Check for inactive waves for informational purposes
                try:
                    import pandas as pd
                    full_registry = pd.read_csv("data/wave_registry.csv")
                    inactive_waves = full_registry[full_registry['active'] == False]
                    if len(inactive_waves) > 0:
                        inactive_names = ', '.join(inactive_waves['wave_name'].tolist())
                        st.session_state.wave_universe_inactive_info = f"{len(inactive_waves)} inactive wave(s): {inactive_names}"
                        print(f"‚ÑπÔ∏è {len(inactive_waves)} inactive wave(s) properly excluded: {inactive_names}")
                except:
                    pass
            
            st.session_state.wave_universe_validated = True
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Could not validate wave universe: {e}")
            st.session_state.wave_universe_validation_failed = False
            # Store exception for debug panel
            if "data_load_exceptions" not in st.session_state:
                st.session_state.data_load_exceptions = []
            st.session_state.data_load_exceptions.append({
                "component": "Wave Universe Validation",
                "error": str(e),
                "traceback": traceback.format_exc()
            })
    
    # ========================================================================
    # Global Compute Lock - Prevent duplicate heavy computations
    # ========================================================================
    
    # Initialize compute lock if not present
    if "compute_lock" not in st.session_state:
        st.session_state.compute_lock = False
        st.session_state.compute_lock_reason = None
    
    # Initialize compute operations tracker
    if "compute_operations_done" not in st.session_state:
        st.session_state.compute_operations_done = set()
    
    # ========================================================================
    # Snapshot Auto-Build and Staleness Management (DISABLED IN SAFE MODE)
    # ========================================================================
    
    # Debug trace marker
    if st.session_state.get("debug_mode", False):
        st.caption("üîç Trace: Entering snapshot build section")
    
    # SAFE MODE: Skip auto-build entirely when Safe Mode is ON
    # Only validate if snapshot exists, but never trigger builds
    if "snapshot_validated" not in st.session_state:
        st.session_state.snapshot_validated = True  # Mark as validated to prevent re-entry
        
        # Check if snapshot exists without building
        import os
        snapshot_path = "data/live_snapshot.csv"
        if os.path.exists(snapshot_path):
            try:
                from datetime import datetime
                mtime = os.path.getmtime(snapshot_path)
                age_minutes = (datetime.now() - datetime.fromtimestamp(mtime)).total_seconds() / 60
                
                st.session_state.snapshot_exists = True
                st.session_state.snapshot_fresh = age_minutes <= 15
                st.session_state.snapshot_age_minutes = age_minutes
                st.session_state.snapshot_rebuilt = False
                st.session_state.snapshot_error = None
                st.session_state.snapshot_stale_fallback = False
                st.session_state.snapshot_build_suppressed = True
                st.session_state.snapshot_suppression_reason = "Safe Mode enabled - auto-build disabled"
                
                print(f"‚ÑπÔ∏è Snapshot exists (age: {age_minutes:.1f} min) - Safe Mode prevents auto-rebuild")
            except Exception as e:
                print(f"‚ö†Ô∏è Warning: Could not check snapshot age: {e}")
                st.session_state.snapshot_exists = True
                st.session_state.snapshot_fresh = False
                # Store exception for debug panel
                if "data_load_exceptions" not in st.session_state:
                    st.session_state.data_load_exceptions = []
                st.session_state.data_load_exceptions.append({
                    "component": "Snapshot Age Check",
                    "error": str(e),
                    "traceback": traceback.format_exc()
                })
        else:
            st.session_state.snapshot_exists = False
            st.session_state.snapshot_fresh = False
            st.session_state.snapshot_age_minutes = None
            st.session_state.snapshot_rebuilt = False
            st.session_state.snapshot_error = "Snapshot does not exist"
            st.session_state.snapshot_stale_fallback = False
            st.session_state.snapshot_build_suppressed = True
            st.session_state.snapshot_suppression_reason = "Safe Mode enabled - auto-build disabled"
            
            print(f"‚ÑπÔ∏è Snapshot does not exist - Safe Mode prevents auto-build. Use manual rebuild button.")
    
    # ========================================================================
    # Session State Initialization
    # ========================================================================
    
    # Initialize session state guard to prevent reinitialization during reruns
    if "initialized" not in st.session_state:
        st.session_state.initialized = True
        
        # One-time operations on first session startup
        try:
            from analytics_pipeline import print_readiness_report
            print_readiness_report()
        except Exception as e:
            print(f"Warning: Could not generate readiness report: {e}")
            # Store exception for debug panel
            if "data_load_exceptions" not in st.session_state:
                st.session_state.data_load_exceptions = []
                st.session_state.data_load_exceptions.append(str(e))
            st.session_state.data_load_exceptions.append({
                "component": "Readiness Report",
                "error": str(e),
                "traceback": traceback.format_exc()
            })
        finally:
            st.session_state.readiness_report_logged = True
        
        # Create backup on successful startup
        try:
            create_last_known_good_backup()
        except Exception as e:
            print(f"Warning: Could not create backup: {e}")
            # Store exception for debug panel
            if "data_load_exceptions" not in st.session_state:
                st.session_state.data_load_exceptions = []
            st.session_state.data_load_exceptions.append({
                "component": "Backup Creation",
                "error": str(e),
                "traceback": traceback.format_exc()
            })
        finally:
            st.session_state.backup_created = True
        
        # Initialize wave_intelligence_center error flag
        st.session_state.wave_ic_has_errors = False
        
        # Initialize wave_universe_version
        st.session_state.wave_universe_version = 1
        
        # Initialize last_refresh_time
        st.session_state.last_refresh_time = datetime.now()
        
        # Initialize last_successful_refresh_time
        st.session_state.last_successful_refresh_time = datetime.now()
        
        # Initialize auto_refresh_enabled using config constant (default: OFF to prevent rerun loops)
        st.session_state.auto_refresh_enabled = DEFAULT_AUTO_REFRESH_ENABLED
        
        # Initialize auto_refresh_interval using config constant
        st.session_state.auto_refresh_interval_ms = DEFAULT_REFRESH_INTERVAL_MS
        
        # Initialize auto_refresh_error_count
        st.session_state.auto_refresh_error_count = 0
        
        # Initialize auto_refresh_paused flag
        st.session_state.auto_refresh_paused = False
        
        # Initialize auto_refresh_error_message
        st.session_state.auto_refresh_error_message = None
        
        # Initialize show_bottom_ticker (default: ON)
        st.session_state.show_bottom_ticker = True
        
        # Initialize selected_wave_id (default: None - portfolio view)
        # UPDATED: Use selected_wave_id as the authoritative state key
        st.session_state.selected_wave_id = None
        
        # Initialize mode (default: Standard)
        st.session_state.mode = "Standard"
        
        # Initialize other session state variables as needed
        # (Additional initialization code would go here)
    
    # ========================================================================
    # STEP 4: Render Main UI
    # ========================================================================
    
    # Resolve canonical app context (single source of truth)
    ctx = resolve_app_context()
    
    # Render selected wave banner at the very top (above all tabs)
    # Use enhanced banner if ENABLE_WAVE_PROFILE is True, otherwise use simple banner
    if ENABLE_WAVE_PROFILE:
        render_selected_wave_banner_enhanced(
            selected_wave=ctx["selected_wave_name"],
            mode=ctx["mode"]
        )
    else:
        render_selected_wave_banner_simple(
            selected_wave=ctx["selected_wave_name"],
            mode=ctx["mode"]
        )
    
    # Render Mission Control at the top
    render_mission_control()
    
    # Render sidebar
    render_sidebar_info()
    
    # NOW enforce the loop trap AFTER sidebar is rendered
    # This allows users to see and toggle the "Allow Continuous Reruns" checkbox
    if st.session_state.get("loop_trap_should_engage", False):
        st.stop()
    
    # Main analytics tabs
    st.title("Institutional Console - Executive Layer v2")
    
    # ========================================================================
    # ENTRYPOINT FINGERPRINT - UI Banner
    # ========================================================================
    st.caption("ENTRYPOINT: app.py")
    
    # ========================================================================
    # REALITY PANEL - Single Source of Truth for Price Data
    # ========================================================================
    # Display the Reality Panel showing actual PRICE_BOOK metadata
    render_reality_panel()
    
    st.markdown("---")
    
    # ========================================================================
    # UI RENDERING PHASE CONFIRMATION
    # ========================================================================
    # Log message confirming that we've reached the UI rendering phase
    logger.info("‚úì Entering UI rendering phase - portfolio data loaded, diagnostics complete")
    print("‚úì UI RENDERING PHASE: Beginning tab rendering with portfolio data")
    
    # ========================================================================
    # ROUND 7 Phase 1: Wave Universe Validation Banner
    # ========================================================================
    # Display warning banner if wave universe validation failed
    if st.session_state.get("wave_universe_validation_failed", False):
        discrepancy = st.session_state.get("wave_universe_discrepancy", "Unknown discrepancy")
        st.error(f"üî¥ **Wave Universe Validation Failed:** {discrepancy}")
    
    # Check if Wave Intelligence Center encountered errors (SAFE_MODE fallback)
    # Use session state to check both the error flag and if safe mode is enabled
    wave_ic_has_errors = st.session_state.get("wave_ic_has_errors", False)
    use_safe_mode = st.session_state.get("safe_mode_enabled", SAFE_MODE)
    
    # Tab structure based on SAFE_MODE status and ENABLE_WAVE_PROFILE
    if wave_ic_has_errors and use_safe_mode:
        # SAFE_MODE fallback - exclude Wave Intelligence Center tab
        st.warning("‚ö†Ô∏è Wave Intelligence Center is temporarily unavailable. Displaying core tabs only.")
        
        analytics_tabs = st.tabs([
            "Institutional Readiness",        # NEW: FIRST TAB - Clean demo-ready overview
            "Console",                 # Core functionality
            "Overview",                # Market tab equivalent
            "Details",     
            "Reports",     
            "Overlays",    
            "Attribution", 
            "Board Pack",  
            "IC Pack",
            "Alpha Capture",
            "Wave Monitor",            # NEW: ROUND 7 Phase 5 - Individual wave analytics
            "Plan B Monitor",          # NEW: Plan B canonical metrics (decoupled from live tickers)
            "Wave Intelligence (Plan B)",  # NEW: Proxy-based analytics for all 28 waves
            "Governance & Audit",      # NEW: Governance and transparency layer
            "Operator Panel",          # NEW: Operator layer for system state and diagnostics
            "Diagnostics",             # Health/Diagnostics tab
            "Wave Overview (New)"      # NEW: Comprehensive all-waves overview
        ])
        
        # Institutional Readiness tab (FIRST)
        with analytics_tabs[0]:
            safe_component("Institutional Readiness", render_overview_clean_tab)
        
        # Console tab
        with analytics_tabs[1]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Executive Console", render_executive_tab)
        
        # Overview tab
        with analytics_tabs[2]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Overview", render_overview_tab)
        
        # Details tab
        with analytics_tabs[3]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Details", render_details_tab)
        
        # Reports tab
        with analytics_tabs[4]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Reports", render_reports_tab)
        
        # Overlays tab
        with analytics_tabs[5]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Overlays", render_overlays_tab)
        
        # Attribution tab
        with analytics_tabs[6]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Attribution", render_attribution_tab)
        
        # Board Pack tab
        with analytics_tabs[7]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Board Pack", render_board_pack_tab)
        
        # IC Pack tab
        with analytics_tabs[8]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("IC Pack", render_ic_pack_tab)
        
        # Alpha Capture tab
        with analytics_tabs[9]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Alpha Capture", render_alpha_capture_tab)
        
        # Wave Monitor tab (NEW - ROUND 7 Phase 5)
        with analytics_tabs[10]:
            safe_component("Wave Monitor", render_wave_monitor_tab)
        
        # Plan B Monitor tab (NEW - Plan B canonical metrics)
        with analytics_tabs[11]:
            safe_component("Plan B Monitor", render_planb_monitor_tab)
        
        # Wave Intelligence (Plan B) tab (NEW - Proxy-based analytics)
        with analytics_tabs[12]:
            safe_component("Wave Intelligence (Plan B)", render_wave_intelligence_planb_tab)
        
        # Governance & Audit tab (NEW)
        with analytics_tabs[13]:
            safe_component("Governance & Audit", render_governance_audit_tab)
        
        # Operator Panel tab (NEW)
        with analytics_tabs[14]:
            safe_component("Operator Panel", render_operator_panel_tab)
        
        # Diagnostics tab
        with analytics_tabs[15]:
            safe_component("Diagnostics", render_diagnostics_tab)
        
        # Wave Overview (New) tab
        with analytics_tabs[16]:
            safe_component("Wave Overview (New)", render_wave_overview_new_tab)
    
    elif ENABLE_WAVE_PROFILE:
        # Normal mode with Wave Profile enabled - Institutional Readiness is FIRST
        analytics_tabs = st.tabs([
            "Institutional Readiness",            # NEW: FIRST TAB - Clean demo-ready overview
            "Overview",                    # Second tab - unified system overview with performance, alpha attribution, and market context
            "Console",                     # Third tab - Executive
            "Wave",                        # Fourth tab - Wave Profile with hero card
            "Details",                     # Factor Decomp equivalent
            "Reports",                     # Risk Lab equivalent
            "Overlays",                    # Correlation equivalent
            "Attribution",                 # Rolling Diagnostics equivalent
            "Board Pack",                  # Mode Proof equivalent
            "IC Pack",
            "Alpha Capture",
            "Wave Monitor",                # NEW: ROUND 7 Phase 5 - Individual wave analytics
            "Plan B Monitor",              # NEW: Plan B canonical metrics (decoupled from live tickers)
            "Wave Intelligence (Plan B)",  # NEW: Proxy-based analytics for all 28 waves
            "Governance & Audit",          # NEW: Governance and transparency layer
            "Operator Panel",              # NEW: Operator layer for system state and diagnostics
            "Diagnostics",                 # Health/Diagnostics tab
            "Wave Overview (New)"          # NEW: Comprehensive all-waves overview
        ])
        
        # Institutional Readiness tab (FIRST)
        with analytics_tabs[0]:
            safe_component("Institutional Readiness", render_overview_clean_tab)
        
        # Overview tab (second) - Executive Brief
        with analytics_tabs[1]:
            safe_component("Executive Brief", render_executive_brief_tab)
        
        # Console tab (third)
        with analytics_tabs[2]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Executive Console", render_executive_tab)
        
        # Wave Profile tab (fourth)
        with analytics_tabs[3]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Wave Profile", render_wave_intelligence_center_tab)
        
        # Details tab
        with analytics_tabs[4]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Details", render_details_tab)
        
        # Reports tab
        with analytics_tabs[5]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Reports", render_reports_tab)
        
        # Overlays tab
        with analytics_tabs[6]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Overlays", render_overlays_tab)
        
        # Attribution tab
        with analytics_tabs[7]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Attribution", render_attribution_tab)
        
        # Board Pack tab
        with analytics_tabs[8]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Board Pack", render_board_pack_tab)
        
        # IC Pack tab
        with analytics_tabs[9]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("IC Pack", render_ic_pack_tab)
        
        # Alpha Capture tab
        with analytics_tabs[10]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Alpha Capture", render_alpha_capture_tab)
        
        # Wave Monitor tab (NEW - ROUND 7 Phase 5)
        with analytics_tabs[11]:
            safe_component("Wave Monitor", render_wave_monitor_tab)
        
        # Plan B Monitor tab (NEW - Plan B canonical metrics)
        with analytics_tabs[12]:
            safe_component("Plan B Monitor", render_planb_monitor_tab)
        
        # Wave Intelligence (Plan B) tab (NEW - Proxy-based analytics)
        with analytics_tabs[13]:
            safe_component("Wave Intelligence (Plan B)", render_wave_intelligence_planb_tab)
        
        # Governance & Audit tab (NEW)
        with analytics_tabs[14]:
            safe_component("Governance & Audit", render_governance_audit_tab)
        
        # Operator Panel tab (NEW)
        with analytics_tabs[15]:
            safe_component("Operator Panel", render_operator_panel_tab)
        
        # Diagnostics tab
        with analytics_tabs[16]:
            safe_component("Diagnostics", render_diagnostics_tab)
        
        # Wave Overview (New) tab
        with analytics_tabs[17]:
            safe_component("Wave Overview (New)", render_wave_overview_new_tab)
    else:
        # Original tab layout (when ENABLE_WAVE_PROFILE is False)
        # Institutional Readiness is FIRST tab
        analytics_tabs = st.tabs([
            "Institutional Readiness",           # NEW: FIRST TAB - Clean demo-ready overview
            "Overview",                   # Second tab - unified system overview
            "Console",                    # Third tab - Executive
            "Details",                    # Factor Decomp equivalent
            "Reports",                    # Risk Lab equivalent
            "Overlays",                   # Correlation equivalent
            "Attribution",                # Rolling Diagnostics equivalent
            "Board Pack",                 # Mode Proof equivalent
            "IC Pack",
            "Alpha Capture",
            "Wave Monitor",               # NEW: ROUND 7 Phase 5 - Individual wave analytics
            "Plan B Monitor",             # NEW: Plan B canonical metrics (decoupled from live tickers)
            "Wave Intelligence (Plan B)", # NEW: Proxy-based analytics for all 28 waves
            "Governance & Audit",         # NEW: Governance and transparency layer
            "Operator Panel",             # NEW: Operator layer for system state and diagnostics
            "Diagnostics",                # Health/Diagnostics tab
            "Wave Overview (New)"         # NEW: Comprehensive all-waves overview
        ])
        
        # Institutional Readiness tab (FIRST)
        with analytics_tabs[0]:
            safe_component("Institutional Readiness", render_overview_clean_tab)
        
        # Overview tab (second) - Executive Brief
        with analytics_tabs[1]:
            safe_component("Executive Brief", render_executive_brief_tab)
        
        # Console tab (third)
        with analytics_tabs[2]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Executive Console", render_executive_tab)
        
        # Details tab
        with analytics_tabs[3]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Details", render_details_tab)
        
        # Reports tab
        with analytics_tabs[4]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Reports", render_reports_tab)
        
        # Overlays tab
        with analytics_tabs[5]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Overlays", render_overlays_tab)
        
        # Attribution tab
        with analytics_tabs[6]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Attribution", render_attribution_tab)
        
        # Board Pack tab
        with analytics_tabs[7]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Board Pack", render_board_pack_tab)
        
        # IC Pack tab
        with analytics_tabs[8]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("IC Pack", render_ic_pack_tab)
        
        # Alpha Capture tab
        with analytics_tabs[9]:
            render_sticky_header(ctx["selected_wave_name"], ctx["mode"])
            safe_component("Alpha Capture", render_alpha_capture_tab)
        
        # Wave Monitor tab (NEW - ROUND 7 Phase 5)
        with analytics_tabs[10]:
            safe_component("Wave Monitor", render_wave_monitor_tab)
        
        # Plan B Monitor tab (NEW - Plan B canonical metrics)
        with analytics_tabs[11]:
            safe_component("Plan B Monitor", render_planb_monitor_tab)
        
        # Wave Intelligence (Plan B) tab (NEW - Proxy-based analytics)
        with analytics_tabs[12]:
            safe_component("Wave Intelligence (Plan B)", render_wave_intelligence_planb_tab)
        
        # Governance & Audit tab (NEW)
        with analytics_tabs[13]:
            safe_component("Governance & Audit", render_governance_audit_tab)
        
        # Operator Panel tab (NEW)
        with analytics_tabs[14]:
            safe_component("Operator Panel", render_operator_panel_tab)
        
        # Diagnostics tab
        with analytics_tabs[15]:
            safe_component("Diagnostics", render_diagnostics_tab)
        
        # Wave Overview (New) tab
        with analytics_tabs[16]:
            safe_component("Wave Overview (New)", render_wave_overview_new_tab)
    
    # ========================================================================
    # Bottom Ticker Bar Rendering
    # ========================================================================
    
    # Render bottom ticker bar if enabled
    if st.session_state.get("show_bottom_ticker", True):
        safe_component("Bottom Ticker", render_bottom_ticker_bar, show_error=False)


# ============================================================================
# APPLICATION ENTRY POINT
# ============================================================================

if __name__ == "__main__":
    main()
